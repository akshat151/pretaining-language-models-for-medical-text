{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32c33332-45bb-4743-9f1f-93086c04dd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default split loaded successfully with 11550 samples.\n",
      "Original Label Mapping: {1: 'neoplasms', 2: 'digestive system diseases', 3: 'nervous system diseases', 4: 'cardiovascular diseases', 5: 'general pathological conditions'}\n",
      "Adjusted Label Mapping: {0: 'neoplasms', 1: 'digestive system diseases', 2: 'nervous system diseases', 3: 'cardiovascular diseases', 4: 'general pathological conditions'}\n",
      "After filtering, 11550 samples remain.\n",
      "Unique labels: [0 1 2 3 4]\n",
      "Tokenization complete.\n",
      "Vocabulary size: 33400\n",
      "Training samples: 9240, Validation samples: 2310\n",
      "Loaded 400000 word vectors from GloVe.\n",
      "LSTMClassifier(\n",
      "  (embedding): Embedding(33400, 100, padding_idx=0)\n",
      "  (lstm): LSTM(100, 128, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
      "  (fc): Linear(in_features=256, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      ")\n",
      "Epoch 1/10: Train Loss = 1.3824, Train Acc = 42.41% | Val Loss = 1.2126, Val Acc = 51.86%\n",
      "Epoch 2/10: Train Loss = 1.1692, Train Acc = 53.38% | Val Loss = 1.1723, Val Acc = 53.42%\n",
      "Epoch 3/10: Train Loss = 1.0649, Train Acc = 56.34% | Val Loss = 1.0746, Val Acc = 55.80%\n",
      "Epoch 4/10: Train Loss = 0.9438, Train Acc = 62.02% | Val Loss = 1.0702, Val Acc = 55.58%\n",
      "Epoch 5/10: Train Loss = 0.8482, Train Acc = 66.17% | Val Loss = 1.0814, Val Acc = 55.19%\n",
      "Epoch 6/10: Train Loss = 0.7608, Train Acc = 70.04% | Val Loss = 1.1247, Val Acc = 55.93%\n",
      "Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 7/10: Train Loss = 0.6803, Train Acc = 72.93% | Val Loss = 1.2049, Val Acc = 54.20%\n",
      "Epoch 8/10: Train Loss = 0.5742, Train Acc = 76.95% | Val Loss = 1.2968, Val Acc = 51.86%\n",
      "Epoch 9/10: Train Loss = 0.5298, Train Acc = 78.01% | Val Loss = 1.3652, Val Acc = 52.68%\n",
      "Epoch 00010: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 10/10: Train Loss = 0.4940, Train Acc = 78.72% | Val Loss = 1.4331, Val Acc = 51.00%\n",
      "\n",
      "For the sample abstract:\n",
      "\"Recent advances in medical research show that artificial intelligence can greatly improve diagnostic accuracy for various diseases. Further clinical studies are required to validate these preliminary findings.\"\n",
      "Predicted label is: cardiovascular diseases\n",
      "\n",
      "Trained model saved to lstm_model.pth\n",
      "\n",
      "Additional Test Cases:\n",
      "Test Case 1:\n",
      "Abstract: The patient was diagnosed with a tumor in the lung, suggesting the onset of neoplasms.\n",
      "Predicted label: neoplasms\n",
      "\n",
      "Test Case 2:\n",
      "Abstract: Severe abdominal pain and persistent nausea indicate potential digestive system diseases.\n",
      "Predicted label: digestive system diseases\n",
      "\n",
      "Test Case 3:\n",
      "Abstract: The patient exhibits tremors and loss of motor control consistent with nervous system diseases.\n",
      "Predicted label: general pathological conditions\n",
      "\n",
      "Test Case 4:\n",
      "Abstract: High blood pressure and chest pain were observed, pointing towards cardiovascular diseases.\n",
      "Predicted label: cardiovascular diseases\n",
      "\n",
      "Test Case 5:\n",
      "Abstract: Generalized weakness and fever may be signs of general pathological conditions.\n",
      "Predicted label: nervous system diseases\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1. Set random seeds for reproducibility\n",
    "# -----------------------------------------------------------\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2. Load the spaCy English model (disable parser & NER for speed)\n",
    "# -----------------------------------------------------------\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3. Load the Hugging Face Dataset and Label Mapping\n",
    "# -----------------------------------------------------------\n",
    "ds_default = load_dataset(\"TimSchopf/medical_abstracts\", \"default\")\n",
    "ds_labels = load_dataset(\"TimSchopf/medical_abstracts\", \"labels\")\n",
    "\n",
    "df = pd.DataFrame(ds_default[\"train\"])\n",
    "print(f\"Default split loaded successfully with {len(df)} samples.\")\n",
    "\n",
    "labels_df = pd.DataFrame(ds_labels[\"train\"])\n",
    "label_map = dict(zip(labels_df[\"condition_label\"], labels_df[\"condition_name\"]))\n",
    "print(\"Original Label Mapping:\", label_map)\n",
    "\n",
    "# Adjust labels from 1–5 to 0–4 for compatibility with CrossEntropyLoss\n",
    "label_map = {k - 1: v for k, v in label_map.items()}\n",
    "print(\"Adjusted Label Mapping:\", label_map)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4. Preprocess and Clean Data\n",
    "# -----------------------------------------------------------\n",
    "# Use the correct column names: \"medical_abstract\" for text and \"condition_label\" for labels.\n",
    "df = df.dropna(subset=[\"medical_abstract\", \"condition_label\"]).reset_index(drop=True)\n",
    "\n",
    "min_label_freq = 5\n",
    "label_counts = df[\"condition_label\"].value_counts()\n",
    "valid_labels = label_counts[label_counts >= min_label_freq].index\n",
    "df = df[df[\"condition_label\"].isin(valid_labels)].reset_index(drop=True)\n",
    "print(f\"After filtering, {len(df)} samples remain.\")\n",
    "\n",
    "texts = df[\"medical_abstract\"].tolist()\n",
    "labels = df[\"condition_label\"].tolist()  # originally 1–5\n",
    "labels = [l - 1 for l in labels]           # convert to 0-based indexing\n",
    "\n",
    "print(\"Unique labels:\", np.unique(labels))\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 5. Tokenization and Vocabulary Construction\n",
    "# -----------------------------------------------------------\n",
    "def batch_advanced_tokenize(texts, batch_size=1000):\n",
    "    tokenized_texts = []\n",
    "    for doc in nlp.pipe(texts, batch_size=batch_size):\n",
    "        tokens = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "        tokenized_texts.append(tokens)\n",
    "    return tokenized_texts\n",
    "\n",
    "tokenized_texts = batch_advanced_tokenize(texts, batch_size=1000)\n",
    "print(\"Tokenization complete.\")\n",
    "\n",
    "all_tokens = [token for tokens in tokenized_texts for token in tokens]\n",
    "vocab_counter = Counter(all_tokens)\n",
    "min_word_freq = 2\n",
    "vocab = {token for token, count in vocab_counter.items() if count >= min_word_freq}\n",
    "\n",
    "# Reserve indices: 0 for <PAD>, 1 for <UNK>\n",
    "word_to_index = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for word in sorted(vocab):\n",
    "    word_to_index[word] = len(word_to_index)\n",
    "vocab_size = len(word_to_index)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "def text_to_sequence(tokens):\n",
    "    return [word_to_index.get(token, word_to_index[\"<UNK>\"]) for token in tokens]\n",
    "\n",
    "sequences = [text_to_sequence(tokens) for tokens in tokenized_texts]\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 6. Pad Sequences to a Fixed Maximum Length\n",
    "# -----------------------------------------------------------\n",
    "max_len = 256  # Adjust as needed\n",
    "\n",
    "def pad_sequence_fn(seq, max_len):\n",
    "    if len(seq) < max_len:\n",
    "        return seq + [0] * (max_len - len(seq))\n",
    "    else:\n",
    "        return seq[:max_len]\n",
    "\n",
    "padded_sequences = [pad_sequence_fn(seq, max_len) for seq in sequences]\n",
    "X = np.array(padded_sequences)\n",
    "y = np.array(labels)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 7. Split the Data into Training and Validation Sets\n",
    "# -----------------------------------------------------------\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 8. Create PyTorch Dataset and DataLoader\n",
    "# -----------------------------------------------------------\n",
    "class MedicalAbstractDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.long), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = MedicalAbstractDataset(X_train, y_train)\n",
    "val_dataset = MedicalAbstractDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                          num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size,\n",
    "                        num_workers=4, pin_memory=True)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 9. Load Pre-trained GloVe Embeddings and Build the Embedding Matrix\n",
    "# -----------------------------------------------------------\n",
    "def load_glove_embeddings(filepath, embedding_dim):\n",
    "    embeddings_index = {}\n",
    "    with open(filepath, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "            if vector.shape[0] == embedding_dim:\n",
    "                embeddings_index[word] = vector\n",
    "    return embeddings_index\n",
    "\n",
    "embedding_dim = 100\n",
    "glove_path = \"glove.6B.100d.txt\"\n",
    "if not os.path.exists(glove_path):\n",
    "    raise FileNotFoundError(f\"{glove_path} not found. Please download it and place it in your working directory.\")\n",
    "\n",
    "glove_embeddings = load_glove_embeddings(glove_path, embedding_dim)\n",
    "print(f\"Loaded {len(glove_embeddings)} word vectors from GloVe.\")\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim), dtype=np.float32)\n",
    "for word, idx in word_to_index.items():\n",
    "    if word in glove_embeddings:\n",
    "        embedding_matrix[idx] = glove_embeddings[word]\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 10. Define the LSTM-based Model for Text Classification\n",
    "# -----------------------------------------------------------\n",
    "# Increase dropout from 0.3 to 0.4 for added regularization.\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, output_dim,\n",
    "                 dropout=0.4, pretrained_embeddings=None, freeze_embeddings=False):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(torch.tensor(pretrained_embeddings))\n",
    "            self.embedding.weight.requires_grad = not freeze_embeddings\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers,\n",
    "                            batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        # Obtain the last hidden states from both directions.\n",
    "        _, (h_n, _) = self.lstm(embedded)\n",
    "        forward_h = h_n[-2, :, :]\n",
    "        backward_h = h_n[-1, :, :]\n",
    "        hidden = torch.cat((forward_h, backward_h), dim=1)\n",
    "        hidden = self.dropout(hidden)\n",
    "        logits = self.fc(hidden)\n",
    "        return logits\n",
    "\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "output_dim = len(label_map)  # 5 classes (0 to 4)\n",
    "dropout = 0.4  # Increased dropout\n",
    "\n",
    "model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, num_layers,\n",
    "                       output_dim, dropout, pretrained_embeddings=embedding_matrix,\n",
    "                       freeze_embeddings=False)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 11. Define the Loss, Optimizer, and Training/Evaluation Functions\n",
    "# -----------------------------------------------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Add weight decay for regularization.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Use ReduceLROnPlateau to adjust learning rate when validation loss plateaus.\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss, epoch_correct = 0, 0\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * inputs.size(0)\n",
    "        epoch_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "    return epoch_loss / len(loader.dataset), epoch_correct / len(loader.dataset)\n",
    "\n",
    "def evaluate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss, epoch_correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_loss += loss.item() * inputs.size(0)\n",
    "            epoch_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "    return epoch_loss / len(loader.dataset), epoch_correct / len(loader.dataset)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = evaluate_epoch(model, val_loader, criterion, device)\n",
    "    scheduler.step(val_loss)  # Adjust learning rate based on validation loss\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Train Acc = {train_acc*100:.2f}% | \" +\n",
    "          f\"Val Loss = {val_loss:.4f}, Val Acc = {val_acc*100:.2f}%\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 12. Inference Function: Predict the Label for a New Abstract\n",
    "# -----------------------------------------------------------\n",
    "def predict_abstract(model, text, word_to_index, max_len, device, label_map):\n",
    "    tokens = [token.text for token in nlp(text) if not token.is_punct and not token.is_space]\n",
    "    seq = [word_to_index.get(token, word_to_index[\"<UNK>\"]) for token in tokens]\n",
    "    seq = pad_sequence_fn(seq, max_len)\n",
    "    input_tensor = torch.tensor(seq, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)\n",
    "    pred_class = logits.argmax(dim=1).item()\n",
    "    pred_label = label_map[pred_class]\n",
    "    return pred_label\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 13. Example Inference (Single Test Case)\n",
    "# -----------------------------------------------------------\n",
    "sample_abstract = (\n",
    "    \"Recent advances in medical research show that artificial intelligence can greatly \"\n",
    "    \"improve diagnostic accuracy for various diseases. Further clinical studies are required \"\n",
    "    \"to validate these preliminary findings.\"\n",
    ")\n",
    "predicted_label = predict_abstract(model, sample_abstract, word_to_index, max_len, device, label_map)\n",
    "print(\"\\nFor the sample abstract:\\n\\\"{}\\\"\\nPredicted label is: {}\".format(sample_abstract, predicted_label))\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 14. Save the Trained Model\n",
    "# -----------------------------------------------------------\n",
    "model_save_path = \"lstm_model.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"\\nTrained model saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad40d127-36c3-4cb7-b993-d0648da9078c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Additional Test Cases:\n",
      "Test Case 1:\n",
      "Abstract: The patient was diagnosed with a tumor in the lung.\n",
      "Predicted label: neoplasms\n",
      "\n",
      "Test Case 2:\n",
      "Abstract: Severe abdominal pain and persistent nausea.\n",
      "Predicted label: digestive system diseases\n",
      "\n",
      "Test Case 3:\n",
      "Abstract: The patient exhibits tremors and loss of motor control.\n",
      "Predicted label: general pathological conditions\n",
      "\n",
      "Test Case 4:\n",
      "Abstract: High blood pressure and chest pain were observed.\n",
      "Predicted label: cardiovascular diseases\n",
      "\n",
      "Test Case 5:\n",
      "Abstract: Generalized weakness and fever.\n",
      "Predicted label: general pathological conditions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 15. Additional Test Cases for Inference\n",
    "# -----------------------------------------------------------\n",
    "test_cases = [\n",
    "    \"The patient was diagnosed with a tumor in the lung.\",\n",
    "    \"Severe abdominal pain and persistent nausea.\",\n",
    "    \"The patient exhibits tremors and loss of motor control.\",\n",
    "    \"High blood pressure and chest pain were observed.\",\n",
    "    \"Generalized weakness and fever.\"\n",
    "]\n",
    "\n",
    "print(\"\\nAdditional Test Cases:\")\n",
    "for i, test_abstract in enumerate(test_cases, start=1):\n",
    "    pred_label = predict_abstract(model, test_abstract, word_to_index, max_len, device, label_map)\n",
    "    print(f\"Test Case {i}:\")\n",
    "    print(f\"Abstract: {test_abstract}\")\n",
    "    print(f\"Predicted label: {pred_label}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c117ec9-ed75-4f4d-b8a6-d0003b0c56ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
