{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aa61a8a-269e-47c9-97cc-987c3fe4c5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "After filtering, 3000000 samples remain with 22555 unique labels.\n",
      "Number of classes after filtering: 22555\n",
      "Tokenization complete.\n",
      "Vocabulary size: 489792\n",
      "Training samples: 2400000, Validation samples: 600000\n",
      "Loaded 400000 word vectors from GloVe.\n",
      "LSTMAttentionClassifier(\n",
      "  (embedding): Embedding(489792, 100, padding_idx=0)\n",
      "  (lstm): LSTM(100, 128, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (attention): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (attention_vector): Linear(in_features=256, out_features=1, bias=False)\n",
      "  (fc): Linear(in_features=256, out_features=22555, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "Epoch 1/10: Train Loss=2.6491, Train Acc=52.18% | Val Loss=1.0387, Val Acc=71.84%\n",
      "Epoch 2/10: Train Loss=1.0929, Train Acc=72.15% | Val Loss=0.8298, Val Acc=77.20%\n",
      "Epoch 3/10: Train Loss=0.8926, Train Acc=76.75% | Val Loss=0.7245, Val Acc=79.72%\n",
      "Epoch 4/10: Train Loss=0.7845, Train Acc=79.30% | Val Loss=0.6900, Val Acc=80.72%\n",
      "Epoch 5/10: Train Loss=0.7092, Train Acc=81.10% | Val Loss=0.6655, Val Acc=81.48%\n",
      "Epoch 6/10: Train Loss=0.6500, Train Acc=82.55% | Val Loss=0.6419, Val Acc=82.07%\n",
      "Epoch 7/10: Train Loss=0.6018, Train Acc=83.68% | Val Loss=0.6312, Val Acc=82.49%\n",
      "Epoch 8/10: Train Loss=0.5588, Train Acc=84.72% | Val Loss=0.6295, Val Acc=82.61%\n",
      "Epoch 9/10: Train Loss=0.5197, Train Acc=85.65% | Val Loss=0.6199, Val Acc=82.87%\n",
      "Epoch 10/10: Train Loss=0.4850, Train Acc=86.48% | Val Loss=0.6163, Val Acc=83.07%\n",
      "\n",
      "For the text: 'The patient was diagnosed with acute MI and was admitted to the ICU for further monitoring.' with abbreviation at position 6, predicted expansion is: meconium ileus\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "import spacy\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# -------------------------------\n",
    "# Set random seeds for reproducibility\n",
    "# -------------------------------\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# -------------------------------\n",
    "# Load spaCy English model\n",
    "# -------------------------------\n",
    "# Disable parser and NER to speed up tokenization.\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# -------------------------------\n",
    "# Unzip the dataset files if needed\n",
    "# -------------------------------\n",
    "if not os.path.exists(\"train.csv\"):\n",
    "    with zipfile.ZipFile('train.csv.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('.')\n",
    "if not os.path.exists(\"train.csv\"):\n",
    "    with zipfile.ZipFile('archive (3).zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('.')\n",
    "\n",
    "# =======================\n",
    "# Step 1: Load the Dataset\n",
    "# =======================\n",
    "try:\n",
    "    df = pd.read_csv('train.csv')\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'train.csv' not found. Please ensure the dataset is in the working directory.\")\n",
    "    exit()\n",
    "\n",
    "# -------------------------------\n",
    "# Reduce the dataset size for development\n",
    "# -------------------------------\n",
    "max_samples = 3000000  # 3M samples maximum\n",
    "if len(df) > max_samples:\n",
    "    df = df.sample(n=max_samples, random_state=42).reset_index(drop=True)\n",
    "    print(f\"Dataset reduced to {len(df)} samples.\")\n",
    "\n",
    "# ================================\n",
    "# Step 2: Preprocess and Clean Data\n",
    "# ================================\n",
    "df = df.dropna(subset=['TEXT', 'LABEL', 'LOCATION'])\n",
    "df['LOCATION'] = df['LOCATION'].astype(int)\n",
    "\n",
    "# --- Reduce Label Space by Filtering Rare Labels ---\n",
    "min_label_freq = 5\n",
    "label_counts = df['LABEL'].value_counts()\n",
    "valid_labels = label_counts[label_counts >= min_label_freq].index\n",
    "df = df[df['LABEL'].isin(valid_labels)].reset_index(drop=True)\n",
    "print(f\"After filtering, {len(df)} samples remain with {df['LABEL'].nunique()} unique labels.\")\n",
    "\n",
    "# --- Advanced Preprocessing Functions ---\n",
    "def extract_context(text, location, window_size=50):\n",
    "    \"\"\"\n",
    "    Extract a window of tokens around the abbreviation.\n",
    "    Assumes LOCATION is based on simple whitespace tokenization.\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    start = max(0, location - window_size // 2)\n",
    "    end = min(len(tokens), location + window_size // 2)\n",
    "    return \" \".join(tokens[start:end])\n",
    "\n",
    "# Use list comprehension instead of apply for faster context extraction.\n",
    "contexts = [extract_context(text, loc, window_size=50) for text, loc in zip(df['TEXT'], df['LOCATION'])]\n",
    "df['CONTEXT'] = contexts\n",
    "\n",
    "texts = df['CONTEXT'].tolist()\n",
    "labels = df['LABEL'].tolist()\n",
    "\n",
    "# ===============================\n",
    "# Step 3: Encode Labels\n",
    "# ===============================\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Number of classes after filtering: {num_classes}\")\n",
    "\n",
    "# ============================================\n",
    "# Step 4: Tokenization and Vocabulary Construction\n",
    "# ============================================\n",
    "def batch_advanced_tokenize(texts, batch_size=1000):\n",
    "    tokenized_texts = []\n",
    "    for doc in nlp.pipe(texts, batch_size=batch_size):\n",
    "        tokens = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "        tokenized_texts.append(tokens)\n",
    "    return tokenized_texts\n",
    "\n",
    "tokenized_texts = batch_advanced_tokenize(texts, batch_size=1000)\n",
    "print(\"Tokenization complete.\")\n",
    "\n",
    "# Build vocabulary from tokenized texts.\n",
    "all_tokens = [token for tokens in tokenized_texts for token in tokens]\n",
    "vocab_counter = Counter(all_tokens)\n",
    "min_word_freq = 2\n",
    "vocab = {token for token, count in vocab_counter.items() if count >= min_word_freq}\n",
    "\n",
    "# Reserve indices: 0 for padding, 1 for unknown tokens.\n",
    "word_to_index = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for word in sorted(vocab):\n",
    "    word_to_index[word] = len(word_to_index)\n",
    "vocab_size = len(word_to_index)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Convert texts to sequences of indices.\n",
    "def text_to_sequence(tokens):\n",
    "    return [word_to_index.get(token, word_to_index[\"<UNK>\"]) for token in tokens]\n",
    "\n",
    "sequences = [text_to_sequence(tokens) for tokens in tokenized_texts]\n",
    "\n",
    "# ===========================\n",
    "# Step 5: Pad Sequences\n",
    "# ===========================\n",
    "max_len = 256  # Fixed maximum length.\n",
    "def pad_sequence_fn(seq, max_len):\n",
    "    return seq + [0] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len]\n",
    "\n",
    "padded_sequences = [pad_sequence_fn(seq, max_len) for seq in sequences]\n",
    "X = np.array(padded_sequences)\n",
    "y = np.array(labels_encoded)\n",
    "\n",
    "# ==================================\n",
    "# Step 6: Train/Validation Data Split\n",
    "# ==================================\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")\n",
    "\n",
    "# ============================================\n",
    "# Step 7: Create PyTorch Dataset and DataLoader\n",
    "# ============================================\n",
    "class MedicalAbbrDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.long), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = MedicalAbbrDataset(X_train, y_train)\n",
    "val_dataset = MedicalAbbrDataset(X_val, y_val)\n",
    "# Use multiple workers and pin_memory for faster data loading on GPU.\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
    "\n",
    "# ============================================\n",
    "# Step 8: Load Pre-trained GloVe Embeddings and Build Embedding Matrix\n",
    "# ============================================\n",
    "def load_glove_embeddings(filepath, embedding_dim):\n",
    "    embeddings_index = {}\n",
    "    with open(filepath, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            if vector.shape[0] == embedding_dim:\n",
    "                embeddings_index[word] = vector\n",
    "    return embeddings_index\n",
    "\n",
    "embedding_dim = 100\n",
    "glove_path = \"glove.6B.100d.txt\"\n",
    "if not os.path.exists(glove_path):\n",
    "    raise FileNotFoundError(f\"{glove_path} not found. Please download it and place it in the working directory.\")\n",
    "\n",
    "glove_embeddings = load_glove_embeddings(glove_path, embedding_dim)\n",
    "print(f\"Loaded {len(glove_embeddings)} word vectors from GloVe.\")\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim), dtype=np.float32)\n",
    "for word, idx in word_to_index.items():\n",
    "    if word in glove_embeddings:\n",
    "        embedding_matrix[idx] = glove_embeddings[word]\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "\n",
    "# ============================================\n",
    "# Step 9: Define the LSTM Classifier with Attention\n",
    "# ============================================\n",
    "class LSTMAttentionClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, output_dim, dropout=0.3,\n",
    "                 pretrained_embeddings=None, freeze_embeddings=False):\n",
    "        super(LSTMAttentionClassifier, self).__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(torch.tensor(pretrained_embeddings))\n",
    "            self.embedding.weight.requires_grad = not freeze_embeddings\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, \n",
    "                            batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        \n",
    "        # Attention layers\n",
    "        self.attention = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.attention_vector = nn.Linear(hidden_dim * 2, 1, bias=False)\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len]\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_len, embedding_dim]\n",
    "        lstm_out, _ = self.lstm(embedded)  # [batch_size, seq_len, hidden_dim*2]\n",
    "        \n",
    "        # Compute attention scores for each time step\n",
    "        attn_energy = torch.tanh(self.attention(lstm_out))  # [batch_size, seq_len, hidden_dim*2]\n",
    "        attn_scores = self.attention_vector(attn_energy)  # [batch_size, seq_len, 1]\n",
    "        attn_weights = torch.softmax(attn_scores, dim=1)  # [batch_size, seq_len, 1]\n",
    "        \n",
    "        # Compute context vector as the weighted sum of LSTM outputs\n",
    "        context = torch.sum(attn_weights * lstm_out, dim=1)  # [batch_size, hidden_dim*2]\n",
    "        context = self.dropout(context)\n",
    "        logits = self.fc(context)\n",
    "        return logits\n",
    "\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "output_dim = num_classes\n",
    "dropout = 0.3\n",
    "\n",
    "model = LSTMAttentionClassifier(vocab_size, embedding_dim, hidden_dim, num_layers, output_dim, dropout,\n",
    "                                pretrained_embeddings=embedding_matrix, freeze_embeddings=False)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "# ============================================\n",
    "# Step 10: Define Loss, Optimizer, and Training Loop\n",
    "# ============================================\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss, epoch_correct = 0, 0\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * inputs.size(0)\n",
    "        epoch_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "    return epoch_loss / len(loader.dataset), epoch_correct / len(loader.dataset)\n",
    "\n",
    "def evaluate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss, epoch_correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_loss += loss.item() * inputs.size(0)\n",
    "            epoch_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "    return epoch_loss / len(loader.dataset), epoch_correct / len(loader.dataset)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = evaluate_epoch(model, val_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss={train_loss:.4f}, Train Acc={train_acc*100:.2f}% | Val Loss={val_loss:.4f}, Val Acc={val_acc*100:.2f}%\")\n",
    "\n",
    "# ============================================\n",
    "# Step 11: Inference Function\n",
    "# ============================================\n",
    "def predict_abbreviation(model, text, location, word_to_index, max_len, device, label_encoder):\n",
    "    context = extract_context(text, location, window_size=50)\n",
    "    tokens = [token.text for token in nlp(context) if not token.is_punct and not token.is_space]\n",
    "    seq = [word_to_index.get(token, word_to_index[\"<UNK>\"]) for token in tokens]\n",
    "    seq = pad_sequence_fn(seq, max_len)\n",
    "    input_tensor = torch.tensor(seq, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)\n",
    "    pred_class = logits.argmax(dim=1).item()\n",
    "    pred_label = label_encoder.inverse_transform([pred_class])[0]\n",
    "    return pred_label\n",
    "\n",
    "# ============================================\n",
    "# Step 12: Example Inference\n",
    "# ============================================\n",
    "sample_text = \"The patient was diagnosed with acute MI and was admitted to the ICU for further monitoring.\"\n",
    "sample_location = 6\n",
    "predicted_expansion = predict_abbreviation(model, sample_text, sample_location, word_to_index, max_len, device, label_encoder)\n",
    "print(f\"\\nFor the text: '{sample_text}' with abbreviation at position {sample_location}, predicted expansion is: {predicted_expansion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e111035-29a3-4fdc-9d0e-e9cf8501a7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to trained_model_LSTM_Attention.pth\n"
     ]
    }
   ],
   "source": [
    "# After training is complete, save the model state dictionary to a file.\n",
    "model_save_path = \"trained_model_LSTM_Attention.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a37d8235-2180-4aef-b1f2-0a6eb7d5ec2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized texts saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the tokenized texts to a file named \"tokenized_texts.pkl\"\n",
    "with open(\"tokenized_texts.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenized_texts, f)\n",
    "\n",
    "print(\"Tokenized texts saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59124fe-b970-4f05-adcd-3e2b556def8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cs5330_project)",
   "language": "python",
   "name": "cs5330_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
