{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f689a62d-7726-47d3-8a94-bbde7bc889bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discharge dataset loaded successfully.\n",
      "Columns in discharge dataset: Index(['note_id', 'subject_id', 'hadm_id', 'note_type', 'note_seq',\n",
      "       'charttime', 'storetime', 'text'],\n",
      "      dtype='object')\n",
      "Dataset reduced to 10000 samples.\n",
      "Number of target classes (note types): 1\n",
      "Tokenization complete on discharge texts.\n",
      "Vocabulary size for MIMIC discharge: 82901\n",
      "Training samples: 8000, Validation samples: 2000\n",
      "Loaded 400000 word vectors from GloVe.\n",
      "Using device: cuda\n",
      "LSTMClassifier(\n",
      "  (embedding): Embedding(82901, 100, padding_idx=0)\n",
      "  (lstm): LSTM(100, 128, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "Loading pretrained weights from Medal model...\n",
      "Pretrained weights loaded successfully (partial transfer if dimensions differ).\n",
      "Epoch 1/10: Train Loss=0.0000, Train Acc=100.00% | Val Loss=0.0000, Val Acc=100.00%\n",
      "Epoch 2/10: Train Loss=0.0000, Train Acc=100.00% | Val Loss=0.0000, Val Acc=100.00%\n",
      "Epoch 3/10: Train Loss=0.0000, Train Acc=100.00% | Val Loss=0.0000, Val Acc=100.00%\n",
      "Epoch 4/10: Train Loss=0.0000, Train Acc=100.00% | Val Loss=0.0000, Val Acc=100.00%\n",
      "Epoch 5/10: Train Loss=0.0000, Train Acc=100.00% | Val Loss=0.0000, Val Acc=100.00%\n",
      "Epoch 6/10: Train Loss=0.0000, Train Acc=100.00% | Val Loss=0.0000, Val Acc=100.00%\n",
      "Epoch 7/10: Train Loss=0.0000, Train Acc=100.00% | Val Loss=0.0000, Val Acc=100.00%\n",
      "Epoch 8/10: Train Loss=0.0000, Train Acc=100.00% | Val Loss=0.0000, Val Acc=100.00%\n",
      "Epoch 9/10: Train Loss=0.0000, Train Acc=100.00% | Val Loss=0.0000, Val Acc=100.00%\n",
      "Epoch 10/10: Train Loss=0.0000, Train Acc=100.00% | Val Loss=0.0000, Val Acc=100.00%\n",
      "\n",
      "For the sample discharge text, predicted note type is: DS\n",
      "Fine-tuned model saved to fine_tuned_model_MIMIC.pth\n"
     ]
    }
   ],
   "source": [
    "%run ../setup.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.backends.cudnn as cudnn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "import spacy\n",
    "import os\n",
    "\n",
    "# -------------------------------\n",
    "# Set random seeds for reproducibility\n",
    "# -------------------------------\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# -------------------------------\n",
    "# Enable cuDNN benchmark for potential speed-up (only effective if using GPU)\n",
    "# -------------------------------\n",
    "if torch.cuda.is_available():\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "# -------------------------------\n",
    "# Load spaCy English model (disable parser and NER for speed)\n",
    "# -------------------------------\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Load and Inspect the MIMIC Discharge Dataset\n",
    "# -------------------------------\n",
    "discharge_csv = \"discharge.csv\"\n",
    "try:\n",
    "    df_mimic = pd.read_csv(discharge_csv)\n",
    "    print(\"Discharge dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{discharge_csv}' not found. Please ensure the file is in the working directory.\")\n",
    "    exit()\n",
    "\n",
    "# Print out columns for inspection\n",
    "print(\"Columns in discharge dataset:\", df_mimic.columns)\n",
    "\n",
    "# -------------------------------\n",
    "# Decrease the number of samples for faster execution.\n",
    "# Change max_samples to the desired number, e.g., 10000.\n",
    "# -------------------------------\n",
    "max_samples = 10000\n",
    "if len(df_mimic) > max_samples:\n",
    "    df_mimic = df_mimic.sample(n=max_samples, random_state=42).reset_index(drop=True)\n",
    "    print(f\"Dataset reduced to {max_samples} samples.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Preprocess the Data\n",
    "# -------------------------------\n",
    "# We use the \"text\" column as input and \"note_type\" as label.\n",
    "df_mimic = df_mimic.dropna(subset=[\"text\", \"note_type\"])\n",
    "df_mimic['note_type'] = df_mimic['note_type'].astype(str)\n",
    "\n",
    "texts = df_mimic[\"text\"].tolist()\n",
    "labels = df_mimic[\"note_type\"].tolist()\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Encode Labels\n",
    "# -------------------------------\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Number of target classes (note types): {num_classes}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Tokenization and Vocabulary Construction\n",
    "# -------------------------------\n",
    "def batch_tokenize(texts, batch_size=1000):\n",
    "    tokenized_texts = []\n",
    "    for doc in nlp.pipe(texts, batch_size=batch_size):\n",
    "        # Remove punctuation and space tokens.\n",
    "        tokens = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "        tokenized_texts.append(tokens)\n",
    "    return tokenized_texts\n",
    "\n",
    "tokenized_texts = batch_tokenize(texts, batch_size=1000)\n",
    "print(\"Tokenization complete on discharge texts.\")\n",
    "\n",
    "# Build vocabulary from tokenized texts.\n",
    "all_tokens = [token for tokens in tokenized_texts for token in tokens]\n",
    "vocab_counter = Counter(all_tokens)\n",
    "min_word_freq = 2\n",
    "vocab = {token for token, count in vocab_counter.items() if count >= min_word_freq}\n",
    "\n",
    "# Reserve indices: 0 for padding, 1 for unknown tokens.\n",
    "word_to_index = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for word in sorted(vocab):\n",
    "    word_to_index[word] = len(word_to_index)\n",
    "vocab_size = len(word_to_index)\n",
    "print(f\"Vocabulary size for MIMIC discharge: {vocab_size}\")\n",
    "\n",
    "# Convert tokenized texts to sequences of indices.\n",
    "def text_to_sequence(tokens):\n",
    "    return [word_to_index.get(token, word_to_index[\"<UNK>\"]) for token in tokens]\n",
    "\n",
    "sequences = [text_to_sequence(tokens) for tokens in tokenized_texts]\n",
    "\n",
    "# -------------------------------\n",
    "# Step 5: Pad Sequences\n",
    "# -------------------------------\n",
    "max_len = 256  # Fixed maximum sequence length.\n",
    "\n",
    "def pad_sequence_fn(seq, max_len):\n",
    "    return seq + [0] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len]\n",
    "\n",
    "padded_sequences = [pad_sequence_fn(seq, max_len) for seq in sequences]\n",
    "X = np.array(padded_sequences)\n",
    "y = np.array(labels_encoded)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 6: Train/Validation Data Split\n",
    "# -------------------------------\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 7: Create PyTorch Dataset and DataLoader for Discharge Data\n",
    "# -------------------------------\n",
    "class MIMICDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.long), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "# Increase batch size and number of workers for faster loading.\n",
    "batch_size = 128  # Increase batch size if memory allows.\n",
    "num_workers = 8   # Adjust based on available CPU cores.\n",
    "\n",
    "train_dataset = MIMICDataset(X_train, y_train)\n",
    "val_dataset = MIMICDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 8: Load Pre-trained GloVe Embeddings and Build Embedding Matrix\n",
    "# -------------------------------\n",
    "def load_glove_embeddings(filepath, embedding_dim):\n",
    "    embeddings_index = {}\n",
    "    with open(filepath, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            if vector.shape[0] == embedding_dim:\n",
    "                embeddings_index[word] = vector\n",
    "    return embeddings_index\n",
    "\n",
    "embedding_dim = 100\n",
    "glove_path = \"glove.6B.100d.txt\"\n",
    "if not os.path.exists(glove_path):\n",
    "    raise FileNotFoundError(f\"{glove_path} not found. Please download it and place it in the working directory.\")\n",
    "\n",
    "glove_embeddings = load_glove_embeddings(glove_path, embedding_dim)\n",
    "print(f\"Loaded {len(glove_embeddings)} word vectors from GloVe.\")\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim), dtype=np.float32)\n",
    "for word, idx in word_to_index.items():\n",
    "    if word in glove_embeddings:\n",
    "        embedding_matrix[idx] = glove_embeddings[word]\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "\n",
    "# -------------------------------\n",
    "# Step 9: Define the LSTM-Only Model (No Attention)\n",
    "# -------------------------------\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, output_dim, dropout=0.3,\n",
    "                 pretrained_embeddings=None, freeze_embeddings=False):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(torch.tensor(pretrained_embeddings))\n",
    "            self.embedding.weight.requires_grad = not freeze_embeddings\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, \n",
    "                            batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (h_n, _) = self.lstm(embedded)\n",
    "        # For bidirectional LSTM, concatenate the last hidden state from both directions.\n",
    "        forward_h = h_n[-2, :, :]\n",
    "        backward_h = h_n[-1, :, :]\n",
    "        hidden = torch.cat((forward_h, backward_h), dim=1)\n",
    "        hidden = self.dropout(hidden)\n",
    "        logits = self.fc(hidden)\n",
    "        return logits\n",
    "\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "# Use the number of target classes for fine-tuning (derived from note_type)\n",
    "new_output_dim = num_classes  \n",
    "dropout = 0.3\n",
    "\n",
    "# -------------------------------\n",
    "# Step 10: Initialize Model and Load Pretrained Weights from Medal\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Instantiate the fine-tuning model.\n",
    "model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, num_layers, new_output_dim, dropout,\n",
    "                       pretrained_embeddings=embedding_matrix, freeze_embeddings=False)\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "# Load pretrained Medal LSTM weights (if available).\n",
    "pretrained_path = \"trained_models/models/medal_trained_model_LSTM.pth\"\n",
    "if os.path.exists(pretrained_path):\n",
    "    print(\"Loading pretrained weights from Medal model...\")\n",
    "    pretrained_state = torch.load(pretrained_path, map_location=device)\n",
    "    model_dict = model.state_dict()\n",
    "    # Load only matching layers (ignores final fc layer if dimensions differ).\n",
    "    pretrained_state = {k: v for k, v in pretrained_state.items() if k in model_dict and v.size() == model_dict[k].size()}\n",
    "    model_dict.update(pretrained_state)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(\"Pretrained weights loaded successfully (partial transfer if dimensions differ).\")\n",
    "else:\n",
    "    print(\"Pretrained model not found; fine-tuning will start from scratch.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 11: Define Loss, Optimizer, and Training Loop for Fine-Tuning\n",
    "# -------------------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss, epoch_correct = 0, 0\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * inputs.size(0)\n",
    "        epoch_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "    return epoch_loss / len(loader.dataset), epoch_correct / len(loader.dataset)\n",
    "\n",
    "def evaluate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss, epoch_correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_loss += loss.item() * inputs.size(0)\n",
    "            epoch_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "    return epoch_loss / len(loader.dataset), epoch_correct / len(loader.dataset)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = evaluate_epoch(model, val_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss={train_loss:.4f}, Train Acc={train_acc*100:.2f}% | Val Loss={val_loss:.4f}, Val Acc={val_acc*100:.2f}%\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 12: Define Inference Function for the Fine-Tuned MIMIC Model\n",
    "# -------------------------------\n",
    "def predict_mimic(model, text, word_to_index, max_len, device, label_encoder):\n",
    "    # Tokenize the entire text using spaCy.\n",
    "    tokens = [token.text for token in nlp(text) if not token.is_punct and not token.is_space]\n",
    "    seq = [word_to_index.get(token, word_to_index[\"<UNK>\"]) for token in tokens]\n",
    "    seq = pad_sequence_fn(seq, max_len)\n",
    "    input_tensor = torch.tensor(seq, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)\n",
    "    pred_class = logits.argmax(dim=1).item()\n",
    "    pred_label = label_encoder.inverse_transform([pred_class])[0]\n",
    "    return pred_label\n",
    "\n",
    "# Example Inference on a sample discharge text:\n",
    "sample_discharge_text = (\n",
    "    \"Patient admitted with severe chest pain. \"\n",
    "    \"Discharge summary indicates a myocardial infarction and appropriate treatment was provided. \"\n",
    "    \"Follow-up is recommended.\"\n",
    ")\n",
    "predicted_label = predict_mimic(model, sample_discharge_text, word_to_index, max_len, device, label_encoder)\n",
    "print(f\"\\nFor the sample discharge text, predicted note type is: {predicted_label}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 13: Save the Fine-Tuned Model\n",
    "# -------------------------------\n",
    "fine_tuned_model_path = \"fine_tuned_model_MIMIC.pth\"\n",
    "torch.save(model.state_dict(), fine_tuned_model_path)\n",
    "print(f\"Fine-tuned model saved to {fine_tuned_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd11b5d1-2422-4e9a-b311-0c42b0b70153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['note_id', 'subject_id', 'hadm_id', 'note_type', 'note_seq',\n",
      "       'charttime', 'storetime', 'text'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_mimic.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55b13e8f-efa9-43d0-bf26-075f302ab5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnosis.csv loaded successfully.\n",
      "Columns in Diagnosis.csv:\n",
      "Index(['subject_id', 'stay_id', 'seq_num', 'icd_code', 'icd_version',\n",
      "       'icd_title'],\n",
      "      dtype='object')\n",
      "--------------------------------------------------\n",
      "edstays.csv loaded successfully.\n",
      "Columns in edstays.csv:\n",
      "Index(['subject_id', 'hadm_id', 'stay_id', 'intime', 'outtime', 'gender',\n",
      "       'race', 'arrival_transport', 'disposition'],\n",
      "      dtype='object')\n",
      "--------------------------------------------------\n",
      "Discharge.csv loaded successfully.\n",
      "Columns in Discharge.csv:\n",
      "Index(['note_id', 'subject_id', 'hadm_id', 'note_type', 'note_seq',\n",
      "       'charttime', 'storetime', 'text'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "diagnosis_csv = \"diagnosis.csv\"\n",
    "edstays_csv = \"edstays.csv\"\n",
    "discharge_csv = \"discharge.csv\"\n",
    "\n",
    "# Load Diagnosis.csv and print its columns\n",
    "try:\n",
    "    df_diagnosis = pd.read_csv(diagnosis_csv)\n",
    "    print(\"Diagnosis.csv loaded successfully.\")\n",
    "    print(\"Columns in Diagnosis.csv:\")\n",
    "    print(df_diagnosis.columns)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading {diagnosis_csv}: {e}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Load edstays.csv and print its columns\n",
    "try:\n",
    "    df_edstays = pd.read_csv(edstays_csv)\n",
    "    print(\"edstays.csv loaded successfully.\")\n",
    "    print(\"Columns in edstays.csv:\")\n",
    "    print(df_edstays.columns)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading {edstays_csv}: {e}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Load Dichange.csv and print its columns\n",
    "try:\n",
    "    df_discharge = pd.read_csv(discharge_csv)\n",
    "    print(\"Discharge.csv loaded successfully.\")\n",
    "    print(\"Columns in Discharge.csv:\")\n",
    "    print(df_discharge.columns)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading {discharge_csv}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bd97b7a-82fe-4225-9126-f1cdbb614f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diagnosis.csv loaded. Columns:\n",
      "Index(['subject_id', 'stay_id', 'icd_code', 'icd_version', 'icd_title'], dtype='object')\n",
      "--------------------------------------------------\n",
      "edstays.csv loaded. Columns:\n",
      "Index(['subject_id', 'hadm_id', 'stay_id', 'intime', 'outtime', 'gender',\n",
      "       'race', 'arrival_transport', 'disposition'],\n",
      "      dtype='object')\n",
      "--------------------------------------------------\n",
      "discharge.csv loaded. Columns:\n",
      "Index(['note_id', 'subject_id', 'hadm_id', 'note_type', 'note_seq',\n",
      "       'charttime', 'storetime', 'text'],\n",
      "      dtype='object')\n",
      "--------------------------------------------------\n",
      "Merged Diagnosis and edstays. Time taken: 13.21 seconds\n",
      "Columns after first merge:\n",
      "Index(['subject_id', 'stay_id', 'icd_code', 'icd_version', 'icd_title',\n",
      "       'hadm_id', 'intime', 'outtime', 'gender', 'race', 'arrival_transport',\n",
      "       'disposition'],\n",
      "      dtype='object')\n",
      "--------------------------------------------------\n",
      "Merged with Discharge data. Time taken: 10.40 seconds\n",
      "Columns in final merged dataset:\n",
      "Index(['subject_id', 'hadm_id', 'stay_id', 'icd_code', 'icd_version',\n",
      "       'icd_title', 'intime', 'outtime', 'gender', 'race', 'arrival_transport',\n",
      "       'disposition', 'note_id', 'note_type', 'note_seq', 'charttime',\n",
      "       'storetime', 'text'],\n",
      "      dtype='object')\n",
      "--------------------------------------------------\n",
      "Merged dataset saved to MIMIC_diagnosis_prediction_dataset.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# -------------------------------\n",
    "# File paths (adjust if necessary)\n",
    "# -------------------------------\n",
    "diagnosis_csv = \"diagnosis.csv\"\n",
    "edstays_csv = \"edstays.csv\"\n",
    "discharge_csv = \"discharge.csv\"\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Load the Datasets, Selecting Only Needed Columns\n",
    "# -------------------------------\n",
    "# Select only columns needed for merging\n",
    "diag_usecols = ['subject_id', 'stay_id', 'icd_code', 'icd_version', 'icd_title']\n",
    "edstays_usecols = ['subject_id', 'stay_id', 'hadm_id', 'intime', 'outtime', 'gender', 'race', 'arrival_transport', 'disposition']\n",
    "discharge_usecols = ['note_id', 'subject_id', 'hadm_id', 'note_type', 'note_seq', 'charttime', 'storetime', 'text']\n",
    "\n",
    "df_diag = pd.read_csv(diagnosis_csv, usecols=diag_usecols)\n",
    "df_edstays = pd.read_csv(edstays_csv, usecols=edstays_usecols)\n",
    "df_discharge = pd.read_csv(discharge_csv, usecols=discharge_usecols)\n",
    "\n",
    "print(\"diagnosis.csv loaded. Columns:\")\n",
    "print(df_diag.columns)\n",
    "print(\"-\" * 50)\n",
    "print(\"edstays.csv loaded. Columns:\")\n",
    "print(df_edstays.columns)\n",
    "print(\"-\" * 50)\n",
    "print(\"discharge.csv loaded. Columns:\")\n",
    "print(df_discharge.columns)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Merge Diagnosis and edstays Using Indexes and join\n",
    "# -------------------------------\n",
    "\n",
    "# Set the index on the common keys ('subject_id' and 'stay_id') for both dataframes.\n",
    "start_time = time.time()\n",
    "df_diag.set_index(['subject_id', 'stay_id'], inplace=True)\n",
    "df_edstays.set_index(['subject_id', 'stay_id'], inplace=True)\n",
    "\n",
    "# Use join on the indexes – joining on the key is often faster.\n",
    "df_diag_ed = df_diag.join(df_edstays, how=\"inner\").reset_index()\n",
    "print(\"Merged Diagnosis and edstays. Time taken: {:.2f} seconds\".format(time.time() - start_time))\n",
    "print(\"Columns after first merge:\")\n",
    "print(df_diag_ed.columns)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Merge the Result with Discharge Data Using Indexes\n",
    "# -------------------------------\n",
    "\n",
    "# Both the merged dataframe and discharge data share 'subject_id' and 'hadm_id'.\n",
    "start_time = time.time()\n",
    "df_diag_ed.set_index(['subject_id', 'hadm_id'], inplace=True)\n",
    "df_discharge.set_index(['subject_id', 'hadm_id'], inplace=True)\n",
    "\n",
    "# Join on the indexes.\n",
    "df_merged = df_diag_ed.join(df_discharge, how=\"inner\").reset_index()\n",
    "print(\"Merged with Discharge data. Time taken: {:.2f} seconds\".format(time.time() - start_time))\n",
    "print(\"Columns in final merged dataset:\")\n",
    "print(df_merged.columns)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Save the Final Merged Dataset\n",
    "# -------------------------------\n",
    "output_csv = \"MIMIC_diagnosis_prediction_dataset.csv\"\n",
    "df_merged.to_csv(output_csv, index=False)\n",
    "print(f\"Merged dataset saved to {output_csv}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94544c8a-e756-4b94-b1bf-dc0f6115ce68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset loaded successfully.\n",
      "Columns in merged dataset:\n",
      "Index(['subject_id', 'hadm_id', 'stay_id', 'icd_code', 'icd_version',\n",
      "       'icd_title', 'intime', 'outtime', 'gender', 'race', 'arrival_transport',\n",
      "       'disposition', 'note_id', 'note_type', 'note_seq', 'charttime',\n",
      "       'storetime', 'text'],\n",
      "      dtype='object')\n",
      "--------------------------------------------------\n",
      "Dataset reduced to 10000 samples.\n",
      "Number of target diagnosis classes: 1763\n",
      "Tokenization complete on note texts. Time taken: 1728.91 seconds\n",
      "Vocabulary size for note texts: 85054\n",
      "Original class counts:\n",
      "188     333\n",
      "59      158\n",
      "1312    130\n",
      "1321    128\n",
      "515     126\n",
      "       ... \n",
      "1051      1\n",
      "1035      1\n",
      "1019      1\n",
      "1003      1\n",
      "1759      1\n",
      "Length: 1763, dtype: int64\n",
      "The following classes have less than 2 samples and will be removed: [1183, 1071, 1381, 1087, 975, 1047, 1175, 1119, 1031, 1429, 1421, 1023, 1309, 1317, 1135, 1293, 1063, 1007, 1405, 1143, 1277, 1167, 1103, 1389, 1151, 981, 1245, 709, 1599, 717, 725, 1591, 733, 1583, 749, 757, 1575, 1567, 765, 773, 781, 1559, 789, 797, 821, 1551, 837, 1607, 701, 1221, 693, 1743, 1727, 1719, 1703, 541, 1695, 1687, 565, 1663, 589, 1639, 605, 1631, 629, 1615, 653, 669, 677, 685, 1543, 861, 869, 1535, 1303, 1093, 1295, 1287, 1271, 1263, 1133, 1255, 1247, 1231, 1223, 1215, 1165, 1181, 1207, 1197, 1199, 1205, 1213, 1077, 1029, 1021, 1495, 909, 1527, 925, 1519, 941, 949, 1511, 1503, 1487, 1407, 1479, 1471, 1463, 1455, 997, 1431, 1005, 1415, 959, 110, 951, 1702, 31, 782, 15, 798, 814, 822, 7, 1750, 1742, 1734, 1718, 862, 55, 886, 894, 1670, 918, 1662, 934, 942, 950, 958, 982, 1646, 774, 766, 263, 159, 247, 582, 590, 598, 231, 215, 191, 183, 175, 606, 167, 151, 742, 630, 143, 654, 135, 670, 111, 103, 702, 95, 710, 71, 1630, 1622, 1598, 1254, 1182, 1470, 1190, 1198, 1206, 1214, 1222, 1462, 1230, 1454, 1246, 1446, 1006, 1278, 1286, 1294, 1302, 1438, 1342, 1422, 1358, 1414, 1366, 1398, 1150, 1478, 525, 1486, 1014, 1022, 1030, 1038, 1590, 1574, 1566, 1054, 1062, 1558, 1550, 1542, 1534, 1070, 1078, 1526, 1110, 1118, 1518, 1134, 1510, 1502, 1494, 255, 566, 1453, 1757, 735, 727, 719, 1693, 1709, 703, 695, 687, 1725, 1741, 1749, 14, 743, 22, 38, 46, 54, 78, 679, 94, 102, 1390, 671, 655, 1677, 759, 279, 1517, 927, 1461, 1469, 919, 911, 1485, 1493, 903, 1501, 1509, 895, 1533, 775, 1541, 1565, 1573, 879, 847, 1589, 1597, 1605, 1613, 839, 1629, 126, 134, 150, 391, 447, 439, 398, 431, 414, 422, 430, 446, 462, 423, 399, 383, 647, 343, 518, 335, 526, 327, 319, 311, 303, 295, 550, 558, 390, 382, 463, 471, 166, 182, 631, 623, 615, 607, 599, 583, 230, 575, 559, 262, 551, 270, 278, 294, 302, 310, 318, 326, 543, 511, 479, 0, 1747, 469, 1457, 1433, 1425, 1409, 1353, 1345, 1313, 1305, 1289, 1281, 1273, 1257, 1233, 1217, 1201, 1177, 1441, 1465, 633, 1481, 1705, 1665, 1617, 1609, 1601, 1593, 1585, 1577, 1569, 1537, 1529, 1521, 1513, 1505, 1497, 1153, 1145, 1137, 1105, 809, 801, 777, 769, 761, 745, 737, 721, 713, 705, 697, 689, 681, 665, 657, 833, 841, 897, 1025, 1097, 1089, 1081, 1073, 1049, 1041, 1001, 905, 993, 977, 969, 961, 929, 913, 1721, 1737, 1745, 602, 834, 826, 802, 794, 778, 770, 762, 738, 706, 698, 682, 674, 666, 658, 650, 842, 858, 882, 1010, 1090, 1082, 1074, 1050, 1026, 1018, 994, 890, 954, 938, 922, 914, 906, 898, 618, 530, 1753, 506, 218, 210, 202, 186, 178, 170, 146, 138, 98, 66, 34, 26, 10, 2, 1761, 226, 234, 242, 354, 466, 418, 402, 394, 386, 370, 346, 250, 330, 298, 290, 282, 266, 258, 641, 617, 453, 888, 872, 856, 848, 840, 832, 824, 816, 800, 776, 768, 760, 752, 744, 736, 728, 880, 896, 609, 912, 1096, 1072, 1048, 1040, 1032, 1024, 1008, 1000, 992, 984, 960, 952, 936, 928, 920, 720, 712, 704, 696, 240, 224, 200, 184, 176, 144, 120, 96, 88, 72, 64, 56, 40, 24, 16, 280, 304, 312, 616, 688, 680, 672, 656, 640, 624, 608, 392, 528, 464, 456, 448, 432, 416, 1144, 1176, 1184, 1704, 185, 137, 129, 121, 113, 81, 65, 57, 33, 17, 9, 1, 1744, 1736, 1728, 225, 233, 241, 457, 601, 593, 585, 561, 489, 465, 417, 249, 401, 385, 353, 297, 289, 281, 1720, 1696, 1192, 1664, 1464, 1456, 1440, 1416, 1408, 1360, 1344, 1304, 1288, 1280, 1264, 1256, 1232, 1208, 1200, 1472, 1480, 1488, 1568, 1640, 1616, 1608, 1600, 1592, 1576, 1560, 1496, 1544, 1536, 1528, 1520, 1512, 1504, 1098, 1114, 1146, 820, 796, 788, 780, 772, 756, 732, 724, 716, 708, 700, 692, 684, 668, 660, 652, 812, 844, 1178, 876, 1068, 1060, 1044, 1036, 1028, 1020, 1012, 1004, 980, 972, 956, 924, 916, 900, 892, 636, 628, 596, 572, 164, 156, 148, 140, 132, 116, 84, 76, 68, 52, 28, 4, 1723, 1715, 1699, 172, 220, 236, 372, 564, 468, 428, 420, 396, 380, 348, 252, 340, 308, 292, 284, 268, 260, 1092, 1108, 1124, 1636, 141, 133, 117, 93, 85, 53, 45, 37, 29, 1748, 1724, 1716, 1692, 1668, 1660, 149, 157, 165, 341, 437, 429, 413, 397, 365, 349, 309, 173, 301, 293, 269, 237, 197, 181, 1652, 1628, 1132, 1620, 1364, 1348, 1332, 1300, 1292, 1284, 1268, 1260, 1228, 1220, 1212, 1204, 1196, 1172, 1140, 1436, 1452, 1460, 1556, 1612, 1604, 1596, 1588, 1572, 1564, 1540, 1468, 1532, 1524, 1508, 1500, 1484, 1476, 1683, 1675, 1643, 1722, 163, 155, 147, 139, 123, 115, 107, 75, 51, 43, 35, 19, 1762, 1746, 1738, 179, 203, 235, 419, 627, 603, 595, 563, 467, 427, 395, 243, 387, 379, 371, 315, 283, 251, 1730, 1658, 675, 1650, 1418, 1410, 1402, 1394, 1386, 1354, 1346, 1298, 1290, 1282, 1274, 1234, 1218, 1210, 1194, 1426, 1458, 1466, 1562, 1634, 1626, 1610, 1594, 1586, 1570, 1546, 1482, 1538, 1530, 1522, 1506, 1498, 1490, 659, 699, 1635, 1203, 1443, 1427, 1419, 1411, 1387, 1379, 1363, 1355, 1339, 1299, 1283, 1275, 1259, 1243, 1235, 1451, 1483, 1491, 1563, 1627, 1619, 1611, 1603, 1587, 1579, 1555, 1499, 1547, 1539, 1531, 1523, 1515, 1507, 1219, 1195, 723, 1179, 923, 915, 899, 891, 883, 867, 859, 843, 811, 803, 787, 779, 771, 755, 747, 931, 939, 955, 1091, 1171, 1163, 1155, 1131, 1123, 1107, 1083, 995, 1067, 1059, 1051, 1035, 1019, 1003, 1759]\n",
      "Updated class counts:\n",
      "188     333\n",
      "59      158\n",
      "1312    130\n",
      "1321    128\n",
      "515     126\n",
      "       ... \n",
      "485       2\n",
      "445       2\n",
      "405       2\n",
      "389       2\n",
      "8         2\n",
      "Length: 830, dtype: int64\n",
      "Training samples: 7253, Validation samples: 1814\n",
      "Loaded 400000 word vectors from GloVe.\n",
      "Using device: cuda\n",
      "LSTMClassifier(\n",
      "  (embedding): Embedding(85054, 100, padding_idx=0)\n",
      "  (lstm): LSTM(100, 128, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (fc): Linear(in_features=256, out_features=1763, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "Loading pretrained weights from Medal model...\n",
      "Pretrained weights loaded successfully (partial transfer if dimensions differ).\n",
      "Epoch 1/10: Train Loss=6.5683, Train Acc=3.05% | Val Loss=5.8680, Val Acc=3.69%\n",
      "Epoch 2/10: Train Loss=5.9476, Train Acc=3.60% | Val Loss=5.8012, Val Acc=3.69%\n",
      "Epoch 3/10: Train Loss=5.8144, Train Acc=3.75% | Val Loss=5.7471, Val Acc=3.75%\n",
      "Epoch 4/10: Train Loss=5.6451, Train Acc=4.26% | Val Loss=5.6583, Val Acc=4.52%\n",
      "Epoch 5/10: Train Loss=5.4676, Train Acc=4.98% | Val Loss=5.5918, Val Acc=4.69%\n",
      "Epoch 6/10: Train Loss=5.2714, Train Acc=6.09% | Val Loss=5.5299, Val Acc=5.13%\n",
      "Epoch 7/10: Train Loss=5.0926, Train Acc=6.45% | Val Loss=5.4819, Val Acc=5.18%\n",
      "Epoch 8/10: Train Loss=4.9190, Train Acc=7.32% | Val Loss=5.4447, Val Acc=5.51%\n",
      "Epoch 9/10: Train Loss=4.7389, Train Acc=8.53% | Val Loss=5.4411, Val Acc=5.07%\n",
      "Epoch 10/10: Train Loss=4.5737, Train Acc=9.91% | Val Loss=5.4251, Val Acc=5.07%\n",
      "\n",
      "For the sample note, predicted diagnosis (icd_code) is: R0600\n",
      "Fine-tuned diagnosis prediction model saved to fine_tuned_model_diagnosis.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.backends.cudnn as cudnn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "import spacy\n",
    "import os\n",
    "import time\n",
    "\n",
    "# -------------------------------\n",
    "# Set random seeds for reproducibility\n",
    "# -------------------------------\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# -------------------------------\n",
    "# Enable cuDNN benchmark for potential speed-up (effective if using GPU)\n",
    "# -------------------------------\n",
    "if torch.cuda.is_available():\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "# -------------------------------\n",
    "# Load spaCy English model (disable parser and NER for speed)\n",
    "# -------------------------------\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Load and Inspect the Merged Dataset for Diagnosis Prediction\n",
    "# -------------------------------\n",
    "merged_csv = \"MIMIC_diagnosis_prediction_dataset.csv\"\n",
    "try:\n",
    "    df_mimic = pd.read_csv(merged_csv)\n",
    "    print(\"Merged dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{merged_csv}' not found. Please ensure the file is in the working directory.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Columns in merged dataset:\")\n",
    "print(df_mimic.columns)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# -------------------------------\n",
    "# Optionally reduce the number of samples for faster execution.\n",
    "# -------------------------------\n",
    "max_samples = 10000  # Adjust as needed\n",
    "if len(df_mimic) > max_samples:\n",
    "    df_mimic = df_mimic.sample(n=max_samples, random_state=42).reset_index(drop=True)\n",
    "    print(f\"Dataset reduced to {max_samples} samples.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Preprocess the Data\n",
    "# -------------------------------\n",
    "# For diagnosis prediction, we use the note text as input and diagnosis code (icd_code) as the label.\n",
    "df_mimic = df_mimic.dropna(subset=[\"text\", \"icd_code\"])\n",
    "df_mimic['icd_code'] = df_mimic['icd_code'].astype(str)\n",
    "\n",
    "texts = df_mimic[\"text\"].tolist()\n",
    "labels = df_mimic[\"icd_code\"].tolist()\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Encode Diagnosis Labels\n",
    "# -------------------------------\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Number of target diagnosis classes: {num_classes}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Tokenization and Vocabulary Construction\n",
    "# -------------------------------\n",
    "def batch_tokenize(texts, batch_size=1000):\n",
    "    tokenized_texts = []\n",
    "    for doc in nlp.pipe(texts, batch_size=batch_size):\n",
    "        tokens = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "        tokenized_texts.append(tokens)\n",
    "    return tokenized_texts\n",
    "\n",
    "start_time = time.time()\n",
    "tokenized_texts = batch_tokenize(texts, batch_size=1000)\n",
    "print(\"Tokenization complete on note texts. Time taken: {:.2f} seconds\".format(time.time()-start_time))\n",
    "\n",
    "all_tokens = [token for tokens in tokenized_texts for token in tokens]\n",
    "vocab_counter = Counter(all_tokens)\n",
    "min_word_freq = 2\n",
    "vocab = {token for token, count in vocab_counter.items() if count >= min_word_freq}\n",
    "\n",
    "# Reserve special tokens\n",
    "word_to_index = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for word in sorted(vocab):\n",
    "    word_to_index[word] = len(word_to_index)\n",
    "vocab_size = len(word_to_index)\n",
    "print(f\"Vocabulary size for note texts: {vocab_size}\")\n",
    "\n",
    "def text_to_sequence(tokens):\n",
    "    return [word_to_index.get(token, word_to_index[\"<UNK>\"]) for token in tokens]\n",
    "\n",
    "sequences = [text_to_sequence(tokens) for tokens in tokenized_texts]\n",
    "\n",
    "# -------------------------------\n",
    "# Step 5: Pad Sequences\n",
    "# -------------------------------\n",
    "max_len = 256  # Fixed maximum sequence length\n",
    "def pad_sequence_fn(seq, max_len):\n",
    "    return seq + [0]*(max_len - len(seq)) if len(seq) < max_len else seq[:max_len]\n",
    "\n",
    "padded_sequences = [pad_sequence_fn(seq, max_len) for seq in sequences]\n",
    "X = np.array(padded_sequences)\n",
    "y = np.array(labels_encoded)\n",
    "\n",
    "# -------------------------------\n",
    "# Filter Out Underrepresented Classes (with <2 samples)\n",
    "# -------------------------------\n",
    "class_counts = pd.Series(y).value_counts()\n",
    "print(\"Original class counts:\")\n",
    "print(class_counts)\n",
    "classes_to_remove = class_counts[class_counts < 2].index.tolist()\n",
    "if classes_to_remove:\n",
    "    print(\"The following classes have less than 2 samples and will be removed:\", classes_to_remove)\n",
    "    mask = ~np.isin(y, classes_to_remove)\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "    print(\"Updated class counts:\")\n",
    "    print(pd.Series(y).value_counts())\n",
    "else:\n",
    "    print(\"All classes have at least 2 samples.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 6: Train/Validation Data Split\n",
    "# -------------------------------\n",
    "test_ratio = 0.2\n",
    "test_count = int(len(X)*test_ratio)\n",
    "if test_count < num_classes:\n",
    "    print(f\"Warning: test_count ({test_count}) is less than number of classes ({num_classes}).\")\n",
    "    print(\"Using non-stratified split.\")\n",
    "    stratify_param = None\n",
    "else:\n",
    "    stratify_param = y\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=test_ratio, random_state=42, stratify=stratify_param\n",
    ")\n",
    "print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 7: Create PyTorch Dataset and DataLoader\n",
    "# -------------------------------\n",
    "class MIMICDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.long), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "batch_size = 128\n",
    "num_workers = 8\n",
    "\n",
    "train_dataset = MIMICDataset(X_train, y_train)\n",
    "val_dataset = MIMICDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 8: Load Pre-trained GloVe Embeddings and Build Embedding Matrix\n",
    "# -------------------------------\n",
    "def load_glove_embeddings(filepath, embedding_dim):\n",
    "    embeddings_index = {}\n",
    "    with open(filepath, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "            if vector.shape[0] == embedding_dim:\n",
    "                embeddings_index[word] = vector\n",
    "    return embeddings_index\n",
    "\n",
    "embedding_dim = 100\n",
    "glove_path = \"glove.6B.100d.txt\"\n",
    "if not os.path.exists(glove_path):\n",
    "    raise FileNotFoundError(f\"{glove_path} not found. Please download it and place it in the working directory.\")\n",
    "\n",
    "glove_embeddings = load_glove_embeddings(glove_path, embedding_dim)\n",
    "print(f\"Loaded {len(glove_embeddings)} word vectors from GloVe.\")\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim), dtype=np.float32)\n",
    "for word, idx in word_to_index.items():\n",
    "    if word in glove_embeddings:\n",
    "        embedding_matrix[idx] = glove_embeddings[word]\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "\n",
    "# -------------------------------\n",
    "# Step 9: Define the LSTM-Only Model (No Attention)\n",
    "# -------------------------------\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, output_dim, dropout=0.3,\n",
    "                 pretrained_embeddings=None, freeze_embeddings=False):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(torch.tensor(pretrained_embeddings))\n",
    "            self.embedding.weight.requires_grad = not freeze_embeddings\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True,\n",
    "                            dropout=dropout, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (h_n, _) = self.lstm(embedded)\n",
    "        forward_h = h_n[-2, :, :]\n",
    "        backward_h = h_n[-1, :, :]\n",
    "        hidden = torch.cat((forward_h, backward_h), dim=1)\n",
    "        hidden = self.dropout(hidden)\n",
    "        logits = self.fc(hidden)\n",
    "        return logits\n",
    "\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "new_output_dim = num_classes  # Diagnosis classes\n",
    "dropout = 0.3\n",
    "\n",
    "# -------------------------------\n",
    "# Step 10: Initialize Model and Load Pretrained Weights from Medal (if available)\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, num_layers, new_output_dim, dropout,\n",
    "                       pretrained_embeddings=embedding_matrix, freeze_embeddings=False)\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "pretrained_path = \"trained_model_LSTM.pth\"\n",
    "if os.path.exists(pretrained_path):\n",
    "    print(\"Loading pretrained weights from Medal model...\")\n",
    "    pretrained_state = torch.load(pretrained_path, map_location=device)\n",
    "    model_dict = model.state_dict()\n",
    "    pretrained_state = {k: v for k, v in pretrained_state.items() if k in model_dict and v.size() == model_dict[k].size()}\n",
    "    model_dict.update(pretrained_state)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(\"Pretrained weights loaded successfully (partial transfer if dimensions differ).\")\n",
    "else:\n",
    "    print(\"Pretrained model not found; fine-tuning will start from scratch.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 11: Define Loss, Optimizer, and Training Loop for Diagnosis Prediction\n",
    "# -------------------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss, epoch_correct = 0, 0\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * inputs.size(0)\n",
    "        epoch_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "    return epoch_loss/len(loader.dataset), epoch_correct/len(loader.dataset)\n",
    "\n",
    "def evaluate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss, epoch_correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_loss += loss.item() * inputs.size(0)\n",
    "            epoch_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "    return epoch_loss/len(loader.dataset), epoch_correct/len(loader.dataset)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = evaluate_epoch(model, val_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss={train_loss:.4f}, Train Acc={train_acc*100:.2f}% | Val Loss={val_loss:.4f}, Val Acc={val_acc*100:.2f}%\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 12: Define Inference Function for Diagnosis Prediction\n",
    "# -------------------------------\n",
    "def predict_diagnosis(model, text, word_to_index, max_len, device, label_encoder):\n",
    "    tokens = [token.text for token in nlp(text) if not token.is_punct and not token.is_space]\n",
    "    seq = [word_to_index.get(token, word_to_index[\"<UNK>\"]) for token in tokens]\n",
    "    seq = pad_sequence_fn(seq, max_len)\n",
    "    input_tensor = torch.tensor(seq, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)\n",
    "    pred_class = logits.argmax(dim=1).item()\n",
    "    pred_label = label_encoder.inverse_transform([pred_class])[0]\n",
    "    return pred_label\n",
    "\n",
    "# Example Inference on a sample note:\n",
    "sample_text = (\n",
    "    \"Patient presented with fever, cough, and shortness of breath. \"\n",
    "    \"Chest X-ray revealed infiltrates consistent with pneumonia; \"\n",
    "    \"appropriate treatment was initiated.\"\n",
    ")\n",
    "predicted_diagnosis = predict_diagnosis(model, sample_text, word_to_index, max_len, device, label_encoder)\n",
    "print(f\"\\nFor the sample note, predicted diagnosis (icd_code) is: {predicted_diagnosis}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 13: Save the Fine-Tuned Diagnosis Prediction Model\n",
    "# -------------------------------\n",
    "fine_tuned_model_path = \"trained_models/models/lstm_finetuned_for_diagnosis_pred.pth\"\n",
    "torch.save(model.state_dict(), fine_tuned_model_path)\n",
    "print(f\"Fine-tuned diagnosis prediction model saved to {fine_tuned_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6cc7e1-f817-4a0c-bacd-4277c182de01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset loaded successfully.\n",
      "Columns in merged dataset:\n",
      "Index(['subject_id', 'hadm_id', 'stay_id', 'icd_code', 'icd_version',\n",
      "       'icd_title', 'intime', 'outtime', 'gender', 'race', 'arrival_transport',\n",
      "       'disposition', 'note_id', 'note_type', 'note_seq', 'charttime',\n",
      "       'storetime', 'text'],\n",
      "      dtype='object')\n",
      "--------------------------------------------------\n",
      "Dataset reduced to 10000 samples.\n",
      "Number of unique ICD groups: 782\n",
      "Number of target diagnosis classes after grouping: 782\n",
      "Tokenization complete on note texts. Time taken: 1603.81 seconds\n",
      "Vocabulary size for note texts: 85054\n",
      "Original class counts:\n",
      "252    408\n",
      "112    333\n",
      "612    255\n",
      "44     230\n",
      "258    227\n",
      "      ... \n",
      "373      1\n",
      "349      1\n",
      "341      1\n",
      "333      1\n",
      "0        1\n",
      "Length: 782, dtype: int64\n",
      "The following classes (encoded as integers) have less than 2 samples and will be removed: [385, 302, 326, 294, 1, 33, 760, 305, 574, 65, 121, 313, 273, 414, 249, 209, 438, 454, 366, 470, 478, 145, 105, 598, 358, 743, 350, 89, 550, 558, 566, 297, 582, 767, 81, 153, 72, 670, 352, 671, 439, 328, 415, 336, 679, 359, 304, 351, 335, 319, 311, 303, 295, 312, 264, 687, 575, 104, 623, 120, 128, 144, 591, 559, 503, 56, 168, 543, 224, 232, 511, 287, 408, 678, 15, 552, 87, 80, 584, 39, 727, 656, 512, 774, 750, 680, 718, 694, 688, 544, 40, 263, 215, 695, 424, 239, 703, 231, 223, 48, 151, 432, 440, 456, 175, 167, 159, 71, 709, 417, 77, 580, 770, 778, 556, 548, 11, 516, 508, 59, 83, 484, 476, 468, 99, 436, 420, 147, 588, 596, 604, 666, 13, 780, 578, 764, 756, 740, 594, 674, 754, 700, 684, 690, 698, 738, 636, 746, 396, 380, 372, 771, 451, 483, 491, 28, 20, 12, 4, 763, 84, 507, 747, 547, 707, 699, 691, 675, 68, 92, 364, 315, 356, 211, 316, 219, 284, 276, 251, 228, 427, 339, 347, 172, 355, 403, 411, 108, 45, 490, 433, 93, 757, 641, 665, 697, 705, 713, 651, 701, 693, 721, 677, 737, 761, 769, 2, 10, 605, 625, 601, 781, 102, 441, 230, 473, 497, 174, 166, 537, 94, 593, 78, 62, 54, 46, 38, 585, 22, 597, 18, 26, 426, 277, 362, 378, 229, 221, 386, 189, 165, 309, 157, 141, 474, 482, 117, 109, 101, 301, 330, 549, 146, 50, 66, 74, 509, 501, 82, 114, 154, 322, 453, 218, 298, 373, 349, 341, 333, 0]\n",
      "Updated class counts:\n",
      "252    408\n",
      "112    333\n",
      "612    255\n",
      "44     230\n",
      "258    227\n",
      "      ... \n",
      "212      2\n",
      "220      2\n",
      "560      2\n",
      "486      2\n",
      "8        2\n",
      "Length: 512, dtype: int64\n",
      "Training samples: 7784, Validation samples: 1946\n",
      "Loaded 400000 word vectors from GloVe.\n",
      "Using device: cuda\n",
      "LSTMClassifier(\n",
      "  (embedding): Embedding(85054, 100, padding_idx=0)\n",
      "  (lstm): LSTM(100, 128, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (fc): Linear(in_features=256, out_features=782, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "Loading pretrained weights from Medal model...\n",
      "Pretrained weights loaded successfully (partial transfer if dimensions differ).\n",
      "Epoch 1/20: Train Loss=5.7373, Train Acc=3.11% | Val Loss=5.2831, Val Acc=3.49%\n",
      "Epoch 2/20: Train Loss=5.3163, Train Acc=4.39% | Val Loss=5.2240, Val Acc=4.37%\n",
      "Epoch 3/20: Train Loss=5.1930, Train Acc=4.82% | Val Loss=5.1211, Val Acc=4.78%\n",
      "Epoch 4/20: Train Loss=5.0189, Train Acc=6.28% | Val Loss=5.0237, Val Acc=5.91%\n",
      "Epoch 5/20: Train Loss=4.8621, Train Acc=7.31% | Val Loss=4.9690, Val Acc=6.22%\n",
      "Epoch 6/20: Train Loss=4.7080, Train Acc=8.12% | Val Loss=4.9005, Val Acc=8.12%\n",
      "Epoch 7/20: Train Loss=4.5372, Train Acc=9.71% | Val Loss=4.8530, Val Acc=7.71%\n",
      "Epoch 8/20: Train Loss=4.3859, Train Acc=11.41% | Val Loss=4.8240, Val Acc=7.76%\n",
      "Epoch 9/20: Train Loss=4.2503, Train Acc=12.74% | Val Loss=4.8168, Val Acc=8.58%\n",
      "Epoch 10/20: Train Loss=4.1190, Train Acc=13.62% | Val Loss=4.8086, Val Acc=8.27%\n",
      "Epoch 11/20: Train Loss=4.0572, Train Acc=14.23% | Val Loss=4.8005, Val Acc=8.63%\n",
      "Epoch 12/20: Train Loss=3.9665, Train Acc=14.88% | Val Loss=4.8183, Val Acc=8.58%\n",
      "Epoch 13/20: Train Loss=3.8286, Train Acc=16.38% | Val Loss=4.8360, Val Acc=8.07%\n",
      "Epoch 14/20: Train Loss=3.7426, Train Acc=17.77% | Val Loss=4.8583, Val Acc=8.74%\n",
      "Epoch 15/20: Train Loss=3.6983, Train Acc=18.47% | Val Loss=4.8825, Val Acc=7.50%\n",
      "Epoch 16/20: Train Loss=3.6745, Train Acc=18.58% | Val Loss=4.8699, Val Acc=7.45%\n",
      "Epoch 17/20: Train Loss=3.5841, Train Acc=20.38% | Val Loss=4.9079, Val Acc=8.12%\n",
      "Epoch 18/20: Train Loss=3.5091, Train Acc=21.18% | Val Loss=4.9227, Val Acc=8.48%\n",
      "Epoch 19/20: Train Loss=3.4386, Train Acc=22.80% | Val Loss=4.9434, Val Acc=7.76%\n",
      "Epoch 20/20: Train Loss=3.3930, Train Acc=23.41% | Val Loss=4.9581, Val Acc=7.14%\n",
      "\n",
      "For the sample note, predicted diagnosis group (first 3 characters of icd_code) is: R06\n",
      "Fine-tuned diagnosis prediction model saved to fine_tuned_model_diagnosis.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.backends.cudnn as cudnn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "import spacy\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --------------------------------------\n",
    "# 1. Set random seeds for reproducibility\n",
    "# --------------------------------------\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# --------------------------------------\n",
    "# 2. Enable cuDNN benchmark for speed-up (if using GPU)\n",
    "# --------------------------------------\n",
    "if torch.cuda.is_available():\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "# --------------------------------------\n",
    "# 3. Load spaCy English model (with parser and NER disabled for speed)\n",
    "# --------------------------------------\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# --------------------------------------\n",
    "# 4. Load the merged dataset (created from Diagnosis.csv, edstays.csv, and Discharge.csv)\n",
    "# --------------------------------------\n",
    "merged_csv = \"MIMIC_diagnosis_prediction_dataset.csv\"\n",
    "try:\n",
    "    df_mimic = pd.read_csv(merged_csv)\n",
    "    print(\"Merged dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{merged_csv}' not found. Please ensure the file is in the working directory.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Columns in merged dataset:\")\n",
    "print(df_mimic.columns)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --------------------------------------\n",
    "# 5. Optionally reduce the number of samples for faster experimentation.\n",
    "# --------------------------------------\n",
    "max_samples = 10000  # change this value as needed\n",
    "if len(df_mimic) > max_samples:\n",
    "    df_mimic = df_mimic.sample(n=max_samples, random_state=42).reset_index(drop=True)\n",
    "    print(f\"Dataset reduced to {max_samples} samples.\")\n",
    "\n",
    "# --------------------------------------\n",
    "# 6. Group the ICD codes by using only the first three characters.\n",
    "# --------------------------------------\n",
    "# Ensure that the icd_code column is a string.\n",
    "df_mimic['icd_code'] = df_mimic['icd_code'].astype(str)\n",
    "\n",
    "# Create a new column for the grouped ICD codes.\n",
    "df_mimic['icd_group'] = df_mimic['icd_code'].str[:3]\n",
    "print(\"Number of unique ICD groups:\", df_mimic['icd_group'].nunique())\n",
    "\n",
    "# --------------------------------------\n",
    "# 7. Preprocess the data: use note text as input and the icd_group as target label.\n",
    "# --------------------------------------\n",
    "df_mimic = df_mimic.dropna(subset=[\"text\", \"icd_group\"])\n",
    "texts = df_mimic[\"text\"].tolist()\n",
    "labels = df_mimic[\"icd_group\"].tolist()\n",
    "\n",
    "# --------------------------------------\n",
    "# 8. Encode the grouped ICD codes using LabelEncoder.\n",
    "# --------------------------------------\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Number of target diagnosis classes after grouping: {num_classes}\")\n",
    "\n",
    "# --------------------------------------\n",
    "# 9. Tokenization and vocabulary construction.\n",
    "# --------------------------------------\n",
    "def batch_tokenize(texts, batch_size=1000):\n",
    "    tokenized_texts = []\n",
    "    for doc in nlp.pipe(texts, batch_size=batch_size):\n",
    "        # Remove punctuation and whitespace tokens.\n",
    "        tokens = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "        tokenized_texts.append(tokens)\n",
    "    return tokenized_texts\n",
    "\n",
    "start_time = time.time()\n",
    "tokenized_texts = batch_tokenize(texts, batch_size=1000)\n",
    "print(\"Tokenization complete on note texts. Time taken: {:.2f} seconds\".format(time.time()-start_time))\n",
    "\n",
    "# Build a vocabulary using a minimum word frequency threshold.\n",
    "all_tokens = [token for tokens in tokenized_texts for token in tokens]\n",
    "vocab_counter = Counter(all_tokens)\n",
    "min_word_freq = 2\n",
    "vocab = {token for token, count in vocab_counter.items() if count >= min_word_freq}\n",
    "\n",
    "# Reserve special tokens: 0 for <PAD> and 1 for <UNK>.\n",
    "word_to_index = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for word in sorted(vocab):\n",
    "    word_to_index[word] = len(word_to_index)\n",
    "vocab_size = len(word_to_index)\n",
    "print(f\"Vocabulary size for note texts: {vocab_size}\")\n",
    "\n",
    "def text_to_sequence(tokens):\n",
    "    return [word_to_index.get(token, word_to_index[\"<UNK>\"]) for token in tokens]\n",
    "\n",
    "sequences = [text_to_sequence(tokens) for tokens in tokenized_texts]\n",
    "\n",
    "# --------------------------------------\n",
    "# 10. Pad sequences to a fixed length.\n",
    "# --------------------------------------\n",
    "max_len = 256  # Fixed maximum sequence length.\n",
    "def pad_sequence_fn(seq, max_len):\n",
    "    return seq + [0] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len]\n",
    "\n",
    "padded_sequences = [pad_sequence_fn(seq, max_len) for seq in sequences]\n",
    "X = np.array(padded_sequences)\n",
    "y = np.array(labels_encoded)\n",
    "\n",
    "# --------------------------------------\n",
    "# 11. Filter out classes with fewer than 2 samples to support stratified splitting.\n",
    "# --------------------------------------\n",
    "class_counts = pd.Series(y).value_counts()\n",
    "print(\"Original class counts:\")\n",
    "print(class_counts)\n",
    "classes_to_remove = class_counts[class_counts < 2].index.tolist()\n",
    "if classes_to_remove:\n",
    "    print(\"The following classes (encoded as integers) have less than 2 samples and will be removed:\", classes_to_remove)\n",
    "    mask = ~np.isin(y, classes_to_remove)\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "    print(\"Updated class counts:\")\n",
    "    print(pd.Series(y).value_counts())\n",
    "else:\n",
    "    print(\"All classes have at least 2 samples.\")\n",
    "\n",
    "# --------------------------------------\n",
    "# 12. Split the data into training and validation sets.\n",
    "# --------------------------------------\n",
    "test_ratio = 0.2\n",
    "test_count = int(len(X)*test_ratio)\n",
    "if test_count < num_classes:\n",
    "    print(f\"Warning: test_count ({test_count}) is less than the number of classes ({num_classes}).\")\n",
    "    print(\"Using non-stratified split.\")\n",
    "    stratify_param = None\n",
    "else:\n",
    "    stratify_param = y\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=test_ratio, random_state=42, stratify=stratify_param\n",
    ")\n",
    "print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")\n",
    "\n",
    "# --------------------------------------\n",
    "# 13. Create PyTorch Dataset and DataLoaders.\n",
    "# --------------------------------------\n",
    "class MIMICDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.long), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "batch_size = 128\n",
    "num_workers = 8\n",
    "\n",
    "train_dataset = MIMICDataset(X_train, y_train)\n",
    "val_dataset = MIMICDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                          num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size,\n",
    "                        num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "# --------------------------------------\n",
    "# 14. Load pre-trained GloVe embeddings and build the embedding matrix.\n",
    "# --------------------------------------\n",
    "def load_glove_embeddings(filepath, embedding_dim):\n",
    "    embeddings_index = {}\n",
    "    with open(filepath, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "            if vector.shape[0] == embedding_dim:\n",
    "                embeddings_index[word] = vector\n",
    "    return embeddings_index\n",
    "\n",
    "embedding_dim = 100\n",
    "glove_path = \"glove.6B.100d.txt\"\n",
    "if not os.path.exists(glove_path):\n",
    "    raise FileNotFoundError(f\"{glove_path} not found. Please download it and place it in the working directory.\")\n",
    "\n",
    "glove_embeddings = load_glove_embeddings(glove_path, embedding_dim)\n",
    "print(f\"Loaded {len(glove_embeddings)} word vectors from GloVe.\")\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim), dtype=np.float32)\n",
    "for word, idx in word_to_index.items():\n",
    "    if word in glove_embeddings:\n",
    "        embedding_matrix[idx] = glove_embeddings[word]\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "\n",
    "# --------------------------------------\n",
    "# 15. Define the LSTM-based classification model.\n",
    "# --------------------------------------\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, output_dim, dropout=0.3,\n",
    "                 pretrained_embeddings=None, freeze_embeddings=False):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(torch.tensor(pretrained_embeddings))\n",
    "            self.embedding.weight.requires_grad = not freeze_embeddings\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True,\n",
    "                            dropout=dropout, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (h_n, _) = self.lstm(embedded)\n",
    "        forward_h = h_n[-2, :, :]\n",
    "        backward_h = h_n[-1, :, :]\n",
    "        hidden = torch.cat((forward_h, backward_h), dim=1)\n",
    "        hidden = self.dropout(hidden)\n",
    "        logits = self.fc(hidden)\n",
    "        return logits\n",
    "\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "new_output_dim = num_classes  # Number of ICD groups after grouping.\n",
    "dropout = 0.3\n",
    "\n",
    "# --------------------------------------\n",
    "# 16. Initialize the model and load optional pretrained weights from the Medal model.\n",
    "# --------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, num_layers, new_output_dim, dropout,\n",
    "                       pretrained_embeddings=embedding_matrix, freeze_embeddings=False)\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "pretrained_path = \"trained_model_LSTM.pth\"\n",
    "if os.path.exists(pretrained_path):\n",
    "    print(\"Loading pretrained weights from Medal model...\")\n",
    "    pretrained_state = torch.load(pretrained_path, map_location=device)\n",
    "    model_dict = model.state_dict()\n",
    "    # Update only matching layers; skip final FC layer if dimensions differ.\n",
    "    pretrained_state = {k: v for k, v in pretrained_state.items() if k in model_dict and v.size() == model_dict[k].size()}\n",
    "    model_dict.update(pretrained_state)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(\"Pretrained weights loaded successfully (partial transfer if dimensions differ).\")\n",
    "else:\n",
    "    print(\"Pretrained model not found; fine-tuning will start from scratch.\")\n",
    "\n",
    "# --------------------------------------\n",
    "# 17. Define the loss, optimizer, and training loop.\n",
    "# --------------------------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss, epoch_correct = 0, 0\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * inputs.size(0)\n",
    "        epoch_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "    return epoch_loss / len(loader.dataset), epoch_correct / len(loader.dataset)\n",
    "\n",
    "def evaluate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss, epoch_correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_loss += loss.item() * inputs.size(0)\n",
    "            epoch_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "    return epoch_loss / len(loader.dataset), epoch_correct / len(loader.dataset)\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = evaluate_epoch(model, val_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss={train_loss:.4f}, Train Acc={train_acc*100:.2f}% | Val Loss={val_loss:.4f}, Val Acc={val_acc*100:.2f}%\")\n",
    "\n",
    "# --------------------------------------\n",
    "# 18. Define an inference function.\n",
    "# --------------------------------------\n",
    "def predict_diagnosis(model, text, word_to_index, max_len, device, label_encoder):\n",
    "    tokens = [token.text for token in nlp(text) if not token.is_punct and not token.is_space]\n",
    "    seq = [word_to_index.get(token, word_to_index[\"<UNK>\"]) for token in tokens]\n",
    "    seq = pad_sequence_fn(seq, max_len)\n",
    "    input_tensor = torch.tensor(seq, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)\n",
    "    pred_class = logits.argmax(dim=1).item()\n",
    "    pred_label = label_encoder.inverse_transform([pred_class])[0]\n",
    "    return pred_label\n",
    "\n",
    "# Example Inference on a sample note.\n",
    "sample_text = (\n",
    "    \"Patient presented with fever, cough, and shortness of breath. \"\n",
    "    \"Chest X-ray revealed infiltrates consistent with pneumonia; \"\n",
    "    \"appropriate treatment was initiated.\"\n",
    ")\n",
    "predicted_diagnosis = predict_diagnosis(model, sample_text, word_to_index, max_len, device, label_encoder)\n",
    "print(f\"\\nFor the sample note, predicted diagnosis group (first 3 characters of icd_code) is: {predicted_diagnosis}\")\n",
    "\n",
    "# --------------------------------------\n",
    "# 19. Save the fine-tuned model.\n",
    "# --------------------------------------\n",
    "fine_tuned_model_path = \"trained_models/models/lstm_finetuned_for_diagnosis_pred.pth\"\n",
    "torch.save(model.state_dict(), fine_tuned_model_path)\n",
    "print(f\"Fine-tuned diagnosis prediction model saved to {fine_tuned_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f0f487-8fad-4f1f-9ed9-b14bffe9bbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset loaded successfully.\n",
      "Columns in merged dataset:\n",
      "Index(['subject_id', 'hadm_id', 'stay_id', 'icd_code', 'icd_version',\n",
      "       'icd_title', 'intime', 'outtime', 'gender', 'race', 'arrival_transport',\n",
      "       'disposition', 'note_id', 'note_type', 'note_seq', 'charttime',\n",
      "       'storetime', 'text'],\n",
      "      dtype='object')\n",
      "--------------------------------------------------\n",
      "Initial number of unique ICD groups (first 2 chars): 316\n",
      "Number of unique ICD groups after mapping infrequent ones to 'Other': 245\n",
      "Number of target diagnosis classes after grouping and mapping: 245\n",
      "Cache length does not match current data length. Re-tokenizing.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.backends.cudnn as cudnn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "import spacy\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# --------------------------------------\n",
    "# 1. Set random seeds for reproducibility\n",
    "# --------------------------------------\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# --------------------------------------\n",
    "# 2. Enable cuDNN benchmark for faster training (if using GPU)\n",
    "# --------------------------------------\n",
    "if torch.cuda.is_available():\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "# --------------------------------------\n",
    "# 3. Load spaCy English model (disable parser/NER for speed)\n",
    "# --------------------------------------\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# --------------------------------------\n",
    "# 4. Load the merged dataset for Diagnosis Prediction\n",
    "# --------------------------------------\n",
    "merged_csv = \"MIMIC_diagnosis_prediction_dataset.csv\"\n",
    "try:\n",
    "    df_mimic = pd.read_csv(merged_csv)\n",
    "    print(\"Merged dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{merged_csv}' not found. Please ensure the file is in the working directory.\")\n",
    "    exit()\n",
    "print(\"Columns in merged dataset:\")\n",
    "print(df_mimic.columns)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --------------------------------------\n",
    "# 5. Optionally reduce the number of samples for faster experimentation.\n",
    "# --------------------------------------\n",
    "max_samples = 3000000  # Adjust this value as needed\n",
    "if len(df_mimic) > max_samples:\n",
    "    df_mimic = df_mimic.sample(n=max_samples, random_state=42).reset_index(drop=True)\n",
    "    print(f\"Dataset reduced to {max_samples} samples.\")\n",
    "\n",
    "# --------------------------------------\n",
    "# 6. Group ICD codes into broader classes.\n",
    "# --------------------------------------\n",
    "# Ensure icd_code is a string, then group by taking the first 2 characters.\n",
    "df_mimic['icd_code'] = df_mimic['icd_code'].astype(str)\n",
    "group_length = 2\n",
    "df_mimic['icd_group'] = df_mimic['icd_code'].str[:group_length]\n",
    "print(\"Initial number of unique ICD groups (first {} chars): {}\".format(\n",
    "    group_length, df_mimic['icd_group'].nunique()))\n",
    "\n",
    "# Map groups with very low frequency (threshold) to \"Other\".\n",
    "threshold = 20  # minimum number of samples per group to keep\n",
    "group_counts = df_mimic['icd_group'].value_counts()\n",
    "df_mimic['icd_group'] = df_mimic['icd_group'].apply(lambda x: x if group_counts[x] >= threshold else \"Other\")\n",
    "print(\"Number of unique ICD groups after mapping infrequent ones to 'Other':\",\n",
    "      df_mimic['icd_group'].nunique())\n",
    "\n",
    "# --------------------------------------\n",
    "# 7. Preprocess data: use note text as input and \"icd_group\" as target.\n",
    "# --------------------------------------\n",
    "df_mimic = df_mimic.dropna(subset=[\"text\", \"icd_group\"])\n",
    "texts = df_mimic[\"text\"].tolist()\n",
    "labels = df_mimic[\"icd_group\"].tolist()\n",
    "\n",
    "# --------------------------------------\n",
    "# 8. Encode the grouped ICD codes.\n",
    "# --------------------------------------\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Number of target diagnosis classes after grouping and mapping: {num_classes}\")\n",
    "\n",
    "# --------------------------------------\n",
    "# 9. Tokenization with caching.\n",
    "# --------------------------------------\n",
    "def batch_tokenize(texts, batch_size=1000):\n",
    "    tokenized_texts = []\n",
    "    for doc in nlp.pipe(texts, batch_size=batch_size):\n",
    "        tokens = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "        tokenized_texts.append(tokens)\n",
    "    return tokenized_texts\n",
    "\n",
    "tokenized_cache_file = \"trained_models/tokenizers/tokenized_texts.pkl\"\n",
    "# Check if cache exists and if its length matches the current texts.\n",
    "if os.path.exists(tokenized_cache_file):\n",
    "    with open(tokenized_cache_file, \"rb\") as f:\n",
    "        cached_tokenized_texts = pickle.load(f)\n",
    "    if len(cached_tokenized_texts) != len(texts):\n",
    "        print(\"Cache length does not match current data length. Re-tokenizing.\")\n",
    "        tokenized_texts = batch_tokenize(texts, batch_size=1000)\n",
    "        with open(tokenized_cache_file, \"wb\") as f:\n",
    "            pickle.dump(tokenized_texts, f)\n",
    "        print(\"Tokenized texts cached for future use.\")\n",
    "    else:\n",
    "        tokenized_texts = cached_tokenized_texts\n",
    "        print(\"Loaded cached tokenized texts.\")\n",
    "else:\n",
    "    start_time = time.time()\n",
    "    tokenized_texts = batch_tokenize(texts, batch_size=1000)\n",
    "    print(\"Tokenization complete on note texts. Time taken: {:.2f} seconds\".format(time.time()-start_time))\n",
    "    with open(tokenized_cache_file, \"wb\") as f:\n",
    "        pickle.dump(tokenized_texts, f)\n",
    "    print(\"Tokenized texts cached for future use.\")\n",
    "\n",
    "# --------------------------------------\n",
    "# 10. Build vocabulary and convert texts to sequences.\n",
    "# --------------------------------------\n",
    "all_tokens = [token for tokens in tokenized_texts for token in tokens]\n",
    "vocab_counter = Counter(all_tokens)\n",
    "min_word_freq = 2\n",
    "vocab = {token for token, count in vocab_counter.items() if count >= min_word_freq}\n",
    "\n",
    "# Reserve special tokens.\n",
    "word_to_index = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for word in sorted(vocab):\n",
    "    word_to_index[word] = len(word_to_index)\n",
    "vocab_size = len(word_to_index)\n",
    "print(f\"Vocabulary size for note texts: {vocab_size}\")\n",
    "\n",
    "def text_to_sequence(tokens):\n",
    "    return [word_to_index.get(token, word_to_index[\"<UNK>\"]) for token in tokens]\n",
    "\n",
    "sequences = [text_to_sequence(tokens) for tokens in tokenized_texts]\n",
    "\n",
    "# --------------------------------------\n",
    "# 11. Pad sequences.\n",
    "# --------------------------------------\n",
    "max_len = 256  # fixed length\n",
    "def pad_sequence_fn(seq, max_len):\n",
    "    return seq + [0] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len]\n",
    "\n",
    "padded_sequences = [pad_sequence_fn(seq, max_len) for seq in sequences]\n",
    "X = np.array(padded_sequences)\n",
    "y = np.array(labels_encoded)\n",
    "\n",
    "# --------------------------------------\n",
    "# 12. (Optional) Filter out classes with fewer than 2 samples.\n",
    "# --------------------------------------\n",
    "class_counts = pd.Series(y).value_counts()\n",
    "print(\"Original class counts:\")\n",
    "print(class_counts)\n",
    "classes_to_remove = class_counts[class_counts < 2].index.tolist()\n",
    "if classes_to_remove:\n",
    "    print(\"The following classes (encoded as integers) have less than 2 samples and will be removed:\", classes_to_remove)\n",
    "    mask = ~np.isin(y, classes_to_remove)\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "    print(\"Updated class counts:\")\n",
    "    print(pd.Series(y).value_counts())\n",
    "else:\n",
    "    print(\"All classes have at least 2 samples.\")\n",
    "\n",
    "# --------------------------------------\n",
    "# 13. Train/Validation split.\n",
    "# --------------------------------------\n",
    "test_ratio = 0.2\n",
    "test_count = int(len(X) * test_ratio)\n",
    "if test_count < num_classes:\n",
    "    print(f\"Warning: test_count ({test_count}) is less than number of classes ({num_classes}). Using non-stratified split.\")\n",
    "    stratify_param = None\n",
    "else:\n",
    "    stratify_param = y\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_ratio, random_state=42, stratify=stratify_param)\n",
    "print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")\n",
    "\n",
    "# --------------------------------------\n",
    "# 14. Create PyTorch Datasets and DataLoaders.\n",
    "# --------------------------------------\n",
    "class MIMICDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.long), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "batch_size = 128\n",
    "num_workers = 8\n",
    "\n",
    "train_dataset = MIMICDataset(X_train, y_train)\n",
    "val_dataset = MIMICDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "# --------------------------------------\n",
    "# 15. Load pre-trained GloVe embeddings and build the embedding matrix.\n",
    "# --------------------------------------\n",
    "def load_glove_embeddings(filepath, embedding_dim):\n",
    "    embeddings_index = {}\n",
    "    with open(filepath, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "            if vector.shape[0] == embedding_dim:\n",
    "                embeddings_index[word] = vector\n",
    "    return embeddings_index\n",
    "\n",
    "embedding_dim = 100\n",
    "glove_path = \"glove.6B.100d.txt\"\n",
    "if not os.path.exists(glove_path):\n",
    "    raise FileNotFoundError(f\"{glove_path} not found. Please download it and place it in the working directory.\")\n",
    "\n",
    "glove_embeddings = load_glove_embeddings(glove_path, embedding_dim)\n",
    "print(f\"Loaded {len(glove_embeddings)} word vectors from GloVe.\")\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim), dtype=np.float32)\n",
    "for word, idx in word_to_index.items():\n",
    "    if word in glove_embeddings:\n",
    "        embedding_matrix[idx] = glove_embeddings[word]\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "\n",
    "# --------------------------------------\n",
    "# 16. Define an Attention module.\n",
    "# --------------------------------------\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2, 1)\n",
    "    def forward(self, lstm_outputs):\n",
    "        # lstm_outputs: (batch, seq_len, hidden_dim*2)\n",
    "        weights = self.attn(lstm_outputs)  # (batch, seq_len, 1)\n",
    "        weights = torch.softmax(weights, dim=1)  # (batch, seq_len, 1)\n",
    "        context = torch.sum(weights * lstm_outputs, dim=1)  # (batch, hidden_dim*2)\n",
    "        return context\n",
    "\n",
    "# --------------------------------------\n",
    "# 17. Define the LSTM model with attention.\n",
    "# --------------------------------------\n",
    "class LSTMClassifierWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, output_dim, dropout=0.3,\n",
    "                 pretrained_embeddings=None, freeze_embeddings=False):\n",
    "        super(LSTMClassifierWithAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(torch.tensor(pretrained_embeddings))\n",
    "            self.embedding.weight.requires_grad = not freeze_embeddings\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True,\n",
    "                            dropout=dropout, bidirectional=True)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # (batch, seq_len, emb_dim)\n",
    "        lstm_out, _ = self.lstm(embedded)  # (batch, seq_len, hidden_dim*2)\n",
    "        context = self.attention(lstm_out)  # (batch, hidden_dim*2)\n",
    "        context = self.dropout(context)\n",
    "        logits = self.fc(context)\n",
    "        return logits\n",
    "\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "new_output_dim = num_classes  # Number of ICD groups (after grouping)\n",
    "dropout = 0.3\n",
    "\n",
    "# --------------------------------------\n",
    "# 18. Initialize the model and load optional pretrained weights.\n",
    "# --------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = LSTMClassifierWithAttention(vocab_size, embedding_dim, hidden_dim, num_layers, new_output_dim, dropout,\n",
    "                                    pretrained_embeddings=embedding_matrix, freeze_embeddings=False)\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "pretrained_path = \"trained_model_LSTM_Attention.pth\"\n",
    "if os.path.exists(pretrained_path):\n",
    "    print(\"Loading pretrained weights from Medal model...\")\n",
    "    pretrained_state = torch.load(pretrained_path, map_location=device)\n",
    "    model_dict = model.state_dict()\n",
    "    pretrained_state = {k: v for k, v in pretrained_state.items() if k in model_dict and v.size() == model_dict[k].size()}\n",
    "    model_dict.update(pretrained_state)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(\"Pretrained weights loaded successfully (partial transfer if dimensions differ).\")\n",
    "else:\n",
    "    print(\"Pretrained model not found; fine-tuning will start from scratch.\")\n",
    "\n",
    "# --------------------------------------\n",
    "# 19. Define loss, optimizer, and a learning rate scheduler.\n",
    "# --------------------------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "# --------------------------------------\n",
    "# 20. Define the training and evaluation loops.\n",
    "# --------------------------------------\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss, epoch_correct = 0, 0\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * inputs.size(0)\n",
    "        epoch_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "    return epoch_loss / len(loader.dataset), epoch_correct / len(loader.dataset)\n",
    "\n",
    "def evaluate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss, epoch_correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_loss += loss.item() * inputs.size(0)\n",
    "            epoch_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "    return epoch_loss / len(loader.dataset), epoch_correct / len(loader.dataset)\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = evaluate_epoch(model, val_loader, criterion, device)\n",
    "    scheduler.step(val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss={train_loss:.4f}, Train Acc={train_acc*100:.2f}% | Val Loss={val_loss:.4f}, Val Acc={val_acc*100:.2f}%\")\n",
    "\n",
    "# --------------------------------------\n",
    "# 21. Define an inference function.\n",
    "# --------------------------------------\n",
    "def predict_diagnosis(model, text, word_to_index, max_len, device, label_encoder):\n",
    "    tokens = [token.text for token in nlp(text) if not token.is_punct and not token.is_space]\n",
    "    seq = [word_to_index.get(token, word_to_index[\"<UNK>\"]) for token in tokens]\n",
    "    seq = pad_sequence_fn(seq, max_len)\n",
    "    input_tensor = torch.tensor(seq, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)\n",
    "    pred_class = logits.argmax(dim=1).item()\n",
    "    pred_label = label_encoder.inverse_transform([pred_class])[0]\n",
    "    return pred_label\n",
    "\n",
    "# --------------------------------------\n",
    "# 22. Example inference on a sample note.\n",
    "# --------------------------------------\n",
    "sample_text = (\n",
    "    \"Patient presented with fever, cough, and shortness of breath. \"\n",
    "    \"Chest X-ray revealed infiltrates consistent with pneumonia; \"\n",
    "    \"appropriate treatment was initiated.\"\n",
    ")\n",
    "predicted_diagnosis = predict_diagnosis(model, sample_text, word_to_index, max_len, device, label_encoder)\n",
    "print(f\"\\nFor the sample note, predicted diagnosis group is: {predicted_diagnosis}\")\n",
    "\n",
    "# --------------------------------------\n",
    "# 23. Save the fine-tuned model.\n",
    "# --------------------------------------\n",
    "fine_tuned_model_path = \"trained_models/models/lstm_finetuned_for_diagnosis_pred.pth\"\n",
    "torch.save(model.state_dict(), fine_tuned_model_path)\n",
    "print(f\"Fine-tuned diagnosis prediction model saved to {fine_tuned_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de37e0f-a4e1-41dd-bc48-5e061323e064",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
