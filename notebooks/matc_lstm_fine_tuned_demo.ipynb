{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0492ae1",
   "metadata": {},
   "source": [
    "# Finetuning MeDAL LSTM on Medical Abstract dataset\n",
    "\n",
    "This notebook fine-tunes a pretrained LSTM model that was previously trained on the MeDal dataset to perform a medical abbreviation disambiguation task. The notebook demonstrates how to load the pretrained model, update its weights, and fine-tune it on a new task with a medical dataset. The LSTM model is trained using cross-entropy loss and evaluated on both training and validation data. Additionally, the notebook includes learning rate scheduling to adjust the learning rate based on validation loss.\n",
    "\n",
    "Key Steps:\n",
    "1. Load pretrained MeDal model weights and filter for matching layers.\n",
    "2. Fine-tune the LSTM model by updating its weights on the new task.\n",
    "3. Use Adam optimizer with weight decay and a learning rate scheduler for better convergence.\n",
    "4. Train and evaluate the model for a set number of epochs.\n",
    "5. Track training and validation performance, including loss and accuracy.\n",
    "\n",
    "This notebook ensures the model can effectively transfer learned representations from the MeDal dataset to improve performance on the medical abstract classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "809a62f7-becc-4d81-8928-2a5ab06f7327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default split loaded successfully with 11550 samples.\n",
      "Original Label Mapping: {1: 'neoplasms', 2: 'digestive system diseases', 3: 'nervous system diseases', 4: 'cardiovascular diseases', 5: 'general pathological conditions'}\n",
      "Adjusted Label Mapping: {0: 'neoplasms', 1: 'digestive system diseases', 2: 'nervous system diseases', 3: 'cardiovascular diseases', 4: 'general pathological conditions'}\n",
      "After filtering, 11550 samples remain.\n",
      "Unique labels in dataset: [0 1 2 3 4]\n",
      "Tokenization complete.\n",
      "Vocabulary size: 33400\n",
      "Training samples: 9240, Validation samples: 2310\n",
      "Loaded 400000 word vectors from GloVe.\n",
      "LSTMClassifier(\n",
      "  (embedding): Embedding(33400, 100, padding_idx=0)\n",
      "  (lstm): LSTM(100, 128, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (fc): Linear(in_features=256, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "Loading the following keys from pretrained model:\n",
      "lstm.weight_ih_l0\n",
      "lstm.weight_hh_l0\n",
      "lstm.bias_ih_l0\n",
      "lstm.bias_hh_l0\n",
      "lstm.weight_ih_l0_reverse\n",
      "lstm.weight_hh_l0_reverse\n",
      "lstm.bias_ih_l0_reverse\n",
      "lstm.bias_hh_l0_reverse\n",
      "lstm.weight_ih_l1\n",
      "lstm.weight_hh_l1\n",
      "lstm.bias_ih_l1\n",
      "lstm.bias_hh_l1\n",
      "lstm.weight_ih_l1_reverse\n",
      "lstm.weight_hh_l1_reverse\n",
      "lstm.bias_ih_l1_reverse\n",
      "lstm.bias_hh_l1_reverse\n",
      "Pretrained model weights loaded (only matching keys).\n",
      "Epoch 1/50: Train Loss=1.5243, Train Acc=32.33% | Val Loss=1.4395, Val Acc=39.48%\n",
      "Epoch 2/50: Train Loss=1.3955, Train Acc=42.09% | Val Loss=1.3260, Val Acc=46.97%\n",
      "Epoch 3/50: Train Loss=1.2912, Train Acc=46.97% | Val Loss=1.2503, Val Acc=48.70%\n",
      "Epoch 4/50: Train Loss=1.2377, Train Acc=48.94% | Val Loss=1.2085, Val Acc=49.26%\n",
      "Epoch 5/50: Train Loss=1.2059, Train Acc=49.53% | Val Loss=1.1849, Val Acc=50.26%\n",
      "Epoch 6/50: Train Loss=1.1826, Train Acc=50.23% | Val Loss=1.1670, Val Acc=50.78%\n",
      "Epoch 7/50: Train Loss=1.1580, Train Acc=52.08% | Val Loss=1.1539, Val Acc=51.69%\n",
      "Epoch 8/50: Train Loss=1.1479, Train Acc=51.97% | Val Loss=1.1463, Val Acc=52.38%\n",
      "Epoch 9/50: Train Loss=1.1308, Train Acc=53.28% | Val Loss=1.1334, Val Acc=52.99%\n",
      "Epoch 10/50: Train Loss=1.1221, Train Acc=53.33% | Val Loss=1.1281, Val Acc=52.86%\n",
      "Epoch 11/50: Train Loss=1.1135, Train Acc=53.80% | Val Loss=1.1196, Val Acc=53.55%\n",
      "Epoch 12/50: Train Loss=1.0954, Train Acc=54.30% | Val Loss=1.1161, Val Acc=52.94%\n",
      "Epoch 13/50: Train Loss=1.0889, Train Acc=54.50% | Val Loss=1.1101, Val Acc=53.33%\n",
      "Epoch 14/50: Train Loss=1.0833, Train Acc=54.88% | Val Loss=1.1027, Val Acc=53.20%\n",
      "Epoch 15/50: Train Loss=1.0722, Train Acc=55.27% | Val Loss=1.1000, Val Acc=53.59%\n",
      "Epoch 16/50: Train Loss=1.0641, Train Acc=55.87% | Val Loss=1.0961, Val Acc=53.68%\n",
      "Epoch 17/50: Train Loss=1.0587, Train Acc=56.44% | Val Loss=1.0926, Val Acc=53.38%\n",
      "Epoch 18/50: Train Loss=1.0527, Train Acc=56.14% | Val Loss=1.0890, Val Acc=53.46%\n",
      "Epoch 19/50: Train Loss=1.0448, Train Acc=56.65% | Val Loss=1.0861, Val Acc=53.38%\n",
      "Epoch 20/50: Train Loss=1.0406, Train Acc=56.86% | Val Loss=1.0833, Val Acc=53.59%\n",
      "Epoch 21/50: Train Loss=1.0317, Train Acc=57.14% | Val Loss=1.0833, Val Acc=53.03%\n",
      "Epoch 22/50: Train Loss=1.0228, Train Acc=57.52% | Val Loss=1.0809, Val Acc=53.90%\n",
      "Epoch 23/50: Train Loss=1.0182, Train Acc=58.16% | Val Loss=1.0806, Val Acc=53.55%\n",
      "Epoch 24/50: Train Loss=1.0192, Train Acc=57.93% | Val Loss=1.0809, Val Acc=53.81%\n",
      "Epoch 25/50: Train Loss=1.0185, Train Acc=57.78% | Val Loss=1.0789, Val Acc=54.07%\n",
      "Epoch 26/50: Train Loss=1.0209, Train Acc=57.41% | Val Loss=1.0798, Val Acc=53.64%\n",
      "Epoch 27/50: Train Loss=1.0213, Train Acc=58.15% | Val Loss=1.0772, Val Acc=54.03%\n",
      "Epoch 28/50: Train Loss=1.0179, Train Acc=57.93% | Val Loss=1.0730, Val Acc=54.81%\n",
      "Epoch 29/50: Train Loss=1.0159, Train Acc=57.95% | Val Loss=1.0661, Val Acc=54.94%\n",
      "Epoch 30/50: Train Loss=1.0205, Train Acc=57.75% | Val Loss=1.0666, Val Acc=54.81%\n",
      "Epoch 31/50: Train Loss=1.0173, Train Acc=57.48% | Val Loss=1.0648, Val Acc=54.85%\n",
      "Epoch 32/50: Train Loss=1.0151, Train Acc=58.14% | Val Loss=1.0658, Val Acc=54.89%\n",
      "Epoch 33/50: Train Loss=1.0145, Train Acc=57.69% | Val Loss=1.0655, Val Acc=55.54%\n",
      "Epoch 34/50: Train Loss=1.0135, Train Acc=58.04% | Val Loss=1.0640, Val Acc=55.54%\n",
      "Epoch 35/50: Train Loss=1.0115, Train Acc=58.07% | Val Loss=1.0619, Val Acc=55.37%\n",
      "Epoch 36/50: Train Loss=1.0107, Train Acc=58.15% | Val Loss=1.0619, Val Acc=54.98%\n",
      "Epoch 37/50: Train Loss=1.0064, Train Acc=58.25% | Val Loss=1.0650, Val Acc=54.85%\n",
      "Epoch 00038: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch 38/50: Train Loss=1.0065, Train Acc=58.51% | Val Loss=1.0622, Val Acc=55.02%\n",
      "Epoch 39/50: Train Loss=1.0049, Train Acc=58.28% | Val Loss=1.0639, Val Acc=54.72%\n",
      "Epoch 40/50: Train Loss=1.0034, Train Acc=58.32% | Val Loss=1.0642, Val Acc=54.81%\n",
      "Epoch 00041: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch 41/50: Train Loss=1.0017, Train Acc=58.33% | Val Loss=1.0627, Val Acc=55.28%\n",
      "Epoch 42/50: Train Loss=1.0048, Train Acc=58.63% | Val Loss=1.0627, Val Acc=55.37%\n",
      "Epoch 43/50: Train Loss=1.0046, Train Acc=58.29% | Val Loss=1.0633, Val Acc=54.85%\n",
      "Epoch 00044: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch 44/50: Train Loss=1.0024, Train Acc=58.87% | Val Loss=1.0625, Val Acc=55.11%\n",
      "Epoch 45/50: Train Loss=1.0045, Train Acc=58.91% | Val Loss=1.0623, Val Acc=54.89%\n",
      "Epoch 46/50: Train Loss=1.0008, Train Acc=58.54% | Val Loss=1.0627, Val Acc=55.11%\n",
      "Epoch 00047: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch 47/50: Train Loss=1.0026, Train Acc=58.47% | Val Loss=1.0663, Val Acc=55.37%\n",
      "Epoch 48/50: Train Loss=1.0005, Train Acc=58.58% | Val Loss=1.0643, Val Acc=55.67%\n",
      "Epoch 49/50: Train Loss=1.0052, Train Acc=58.17% | Val Loss=1.0634, Val Acc=54.59%\n",
      "Epoch 00050: reducing learning rate of group 0 to 3.1250e-06.\n",
      "Epoch 50/50: Train Loss=1.0020, Train Acc=58.34% | Val Loss=1.0636, Val Acc=54.85%\n",
      "\n",
      "For the sample text:\n",
      "\"Recent advances in medical research show that artificial intelligence can greatly improve diagnostic accuracy for various diseases. Further clinical studies are required to validate these preliminary findings.\"\n",
      "Predicted label is: neoplasms\n",
      "\n",
      "Fine-tuned model saved to finetuned_model_LSTM.pth\n",
      "\n",
      "Additional Test Cases:\n",
      "Test Case 1:\n",
      "Text: The patient was diagnosed with a tumor in the lung, suggesting the onset of neoplasms.\n",
      "Predicted label: neoplasms\n",
      "\n",
      "Test Case 2:\n",
      "Text: Severe abdominal pain and persistent nausea indicate potential digestive system diseases.\n",
      "Predicted label: general pathological conditions\n",
      "\n",
      "Test Case 3:\n",
      "Text: The patient exhibits tremors and loss of motor control consistent with nervous system diseases.\n",
      "Predicted label: general pathological conditions\n",
      "\n",
      "Test Case 4:\n",
      "Text: High blood pressure and chest pain were observed, pointing towards cardiovascular diseases.\n",
      "Predicted label: cardiovascular diseases\n",
      "\n",
      "Test Case 5:\n",
      "Text: Generalized weakness and fever may be signs of general pathological conditions.\n",
      "Predicted label: general pathological conditions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Set random seeds for reproducibility\n",
    "# -------------------------------\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Load spaCy English model (disable parser & NER for speed)\n",
    "# -------------------------------\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Load the New Dataset from Hugging Face and Build Label Mapping\n",
    "# -------------------------------\n",
    "ds_default = load_dataset(\"TimSchopf/medical_abstracts\", \"default\")\n",
    "ds_labels = load_dataset(\"TimSchopf/medical_abstracts\", \"labels\")\n",
    "\n",
    "df = pd.DataFrame(ds_default[\"train\"])\n",
    "print(f\"Default split loaded successfully with {len(df)} samples.\")\n",
    "\n",
    "labels_df = pd.DataFrame(ds_labels[\"train\"])\n",
    "orig_label_map = dict(zip(labels_df[\"condition_label\"], labels_df[\"condition_name\"]))\n",
    "print(\"Original Label Mapping:\", orig_label_map)\n",
    "\n",
    "# Adjust labels from 1–5 to 0–4 for CrossEntropyLoss.\n",
    "label_map = {k - 1: v for k, v in orig_label_map.items()}\n",
    "print(\"Adjusted Label Mapping:\", label_map)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Preprocess and Clean Data\n",
    "# -------------------------------\n",
    "df = df.dropna(subset=[\"medical_abstract\", \"condition_label\"]).reset_index(drop=True)\n",
    "min_label_freq = 5\n",
    "label_counts = df[\"condition_label\"].value_counts()\n",
    "valid_labels = label_counts[label_counts >= min_label_freq].index\n",
    "df = df[df[\"condition_label\"].isin(valid_labels)].reset_index(drop=True)\n",
    "print(f\"After filtering, {len(df)} samples remain.\")\n",
    "\n",
    "texts = df[\"medical_abstract\"].tolist()\n",
    "labels = df[\"condition_label\"].tolist()  # originally 1–5\n",
    "labels = [l - 1 for l in labels]  # convert to 0-based\n",
    "print(\"Unique labels in dataset:\", np.unique(labels))\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Tokenization and Vocabulary Construction\n",
    "# -------------------------------\n",
    "def batch_advanced_tokenize(texts, batch_size=1000):\n",
    "    tokenized_texts = []\n",
    "    for doc in nlp.pipe(texts, batch_size=batch_size):\n",
    "        tokens = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "        tokenized_texts.append(tokens)\n",
    "    return tokenized_texts\n",
    "\n",
    "tokenized_texts = batch_advanced_tokenize(texts, batch_size=1000)\n",
    "print(\"Tokenization complete.\")\n",
    "\n",
    "all_tokens = [token for tokens in tokenized_texts for token in tokens]\n",
    "vocab_counter = Counter(all_tokens)\n",
    "min_word_freq = 2\n",
    "vocab = {token for token, count in vocab_counter.items() if count >= min_word_freq}\n",
    "\n",
    "# Reserve indices: 0 for PAD, 1 for UNK.\n",
    "word_to_index = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for word in sorted(vocab):\n",
    "    word_to_index[word] = len(word_to_index)\n",
    "vocab_size = len(word_to_index)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "def text_to_sequence(tokens):\n",
    "    return [word_to_index.get(token, word_to_index[\"<UNK>\"]) for token in tokens]\n",
    "\n",
    "sequences = [text_to_sequence(tokens) for tokens in tokenized_texts]\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Pad Sequences to a Fixed Maximum Length\n",
    "# -------------------------------\n",
    "max_len = 256\n",
    "def pad_sequence_fn(seq, max_len):\n",
    "    return seq + [0] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len]\n",
    "\n",
    "padded_sequences = [pad_sequence_fn(seq, max_len) for seq in sequences]\n",
    "X = np.array(padded_sequences)\n",
    "y = np.array(labels)\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Split Data into Training and Validation Sets\n",
    "# -------------------------------\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Create PyTorch Dataset and DataLoader\n",
    "# -------------------------------\n",
    "class MedicalAbstractDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.long), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = MedicalAbstractDataset(X_train, y_train)\n",
    "val_dataset = MedicalAbstractDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 9. Load Pre-trained GloVe Embeddings and Build Embedding Matrix\n",
    "# -------------------------------\n",
    "def load_glove_embeddings(filepath, embedding_dim):\n",
    "    embeddings_index = {}\n",
    "    with open(filepath, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "            if vector.shape[0] == embedding_dim:\n",
    "                embeddings_index[word] = vector\n",
    "    return embeddings_index\n",
    "\n",
    "embedding_dim = 100\n",
    "glove_path = \"glove.6B.100d.txt\"\n",
    "if not os.path.exists(glove_path):\n",
    "    raise FileNotFoundError(f\"{glove_path} not found. Please download it and place it in your working directory.\")\n",
    "\n",
    "glove_embeddings = load_glove_embeddings(glove_path, embedding_dim)\n",
    "print(f\"Loaded {len(glove_embeddings)} word vectors from GloVe.\")\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim), dtype=np.float32)\n",
    "for word, idx in word_to_index.items():\n",
    "    if word in glove_embeddings:\n",
    "        embedding_matrix[idx] = glove_embeddings[word]\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "\n",
    "# -------------------------------\n",
    "# 10. Define the LSTM-based Model (Pretrained Architecture)\n",
    "# -------------------------------\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, output_dim,\n",
    "                 dropout=0.3, pretrained_embeddings=None, freeze_embeddings=False):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(torch.tensor(pretrained_embeddings))\n",
    "            self.embedding.weight.requires_grad = not freeze_embeddings\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers,\n",
    "                            batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (h_n, _) = self.lstm(embedded)\n",
    "        forward_h = h_n[-2, :, :]\n",
    "        backward_h = h_n[-1, :, :]\n",
    "        hidden = torch.cat((forward_h, backward_h), dim=1)\n",
    "        hidden = self.dropout(hidden)\n",
    "        logits = self.fc(hidden)\n",
    "        return logits\n",
    "\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "# For the new dataset, we have 5 classes (0 to 4).\n",
    "output_dim = 5\n",
    "dropout = 0.3\n",
    "\n",
    "model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, num_layers,\n",
    "                       output_dim, dropout, pretrained_embeddings=embedding_matrix,\n",
    "                       freeze_embeddings=False)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "# -------------------------------\n",
    "# 11. Load Pretrained Weights and Fine-Tune\n",
    "# -------------------------------\n",
    "# Load your pretrained MeDal model weights.\n",
    "pretrained_model_path = \"trained_model_LSTM.pth\"  # Path to the MeDal-trained model\n",
    "pretrained_state_dict = torch.load(pretrained_model_path, map_location=device)\n",
    "\n",
    "# Get current model state dictionary.\n",
    "model_dict = model.state_dict()\n",
    "\n",
    "# Filter out keys that do not match in shape (i.e., embedding and fc layers).\n",
    "pretrained_dict = {k: v for k, v in pretrained_state_dict.items() if k in model_dict and model_dict[k].shape == v.shape}\n",
    "\n",
    "# Optionally, you can print which keys are loaded:\n",
    "print(\"Loading the following keys from pretrained model:\")\n",
    "for k in pretrained_dict.keys():\n",
    "    print(k)\n",
    "\n",
    "# Update current state with pretrained values.\n",
    "model_dict.update(pretrained_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "print(\"Pretrained model weights loaded (only matching keys).\")\n",
    "\n",
    "# Optionally, freeze layers except the classifier if desired:\n",
    "# for name, param in model.named_parameters():\n",
    "#     if \"fc\" not in name:\n",
    "#         param.requires_grad = False\n",
    "# print(\"Non-classifier layers frozen.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 12. Define Optimizer, Loss, and Scheduler for Fine-Tuning\n",
    "# -------------------------------\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss, epoch_correct = 0, 0\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * inputs.size(0)\n",
    "        epoch_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "    return epoch_loss / len(loader.dataset), epoch_correct / len(loader.dataset)\n",
    "\n",
    "def evaluate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss, epoch_correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_loss += loss.item() * inputs.size(0)\n",
    "            epoch_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "    return epoch_loss / len(loader.dataset), epoch_correct / len(loader.dataset)\n",
    "\n",
    "num_finetune_epochs = 50\n",
    "for epoch in range(num_finetune_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = evaluate_epoch(model, val_loader, criterion, device)\n",
    "    scheduler.step(val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_finetune_epochs}: Train Loss={train_loss:.4f}, Train Acc={train_acc*100:.2f}% | Val Loss={val_loss:.4f}, Val Acc={val_acc*100:.2f}%\")\n",
    "\n",
    "# -------------------------------\n",
    "# 13. Inference Function for Fine-Tuned Model\n",
    "# -------------------------------\n",
    "# For the new dataset, we define a prediction function that uses the full text.\n",
    "def predict_label(model, text, word_to_index, max_len, device, label_map):\n",
    "    tokens = [token.text for token in nlp(text) if not token.is_punct and not token.is_space]\n",
    "    seq = [word_to_index.get(token, word_to_index[\"<UNK>\"]) for token in tokens]\n",
    "    seq = pad_sequence_fn(seq, max_len)\n",
    "    input_tensor = torch.tensor(seq, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)\n",
    "    pred_class = logits.argmax(dim=1).item()\n",
    "    pred_label = label_map[pred_class]\n",
    "    return pred_label\n",
    "\n",
    "# -------------------------------\n",
    "# 14. Example Inference (Single Test Case)\n",
    "# -------------------------------\n",
    "sample_text = (\n",
    "    \"Recent advances in medical research show that artificial intelligence can greatly \"\n",
    "    \"improve diagnostic accuracy for various diseases. Further clinical studies are required \"\n",
    "    \"to validate these preliminary findings.\"\n",
    ")\n",
    "predicted_label = predict_label(model, sample_text, word_to_index, max_len, device, label_map)\n",
    "print(\"\\nFor the sample text:\\n\\\"{}\\\"\\nPredicted label is: {}\".format(sample_text, predicted_label))\n",
    "\n",
    "# -------------------------------\n",
    "# 15. Save the Fine-Tuned Model\n",
    "# -------------------------------\n",
    "finetuned_model_path = \"finetuned_model_LSTM.pth\"\n",
    "torch.save(model.state_dict(), finetuned_model_path)\n",
    "print(f\"\\nFine-tuned model saved to {finetuned_model_path}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 16. Additional Test Cases for Inference\n",
    "# -------------------------------\n",
    "test_cases = [\n",
    "    \"The patient was diagnosed with a tumor in the lung, suggesting the onset of neoplasms.\",\n",
    "    \"Severe abdominal pain and persistent nausea indicate potential digestive system diseases.\",\n",
    "    \"The patient exhibits tremors and loss of motor control consistent with nervous system diseases.\",\n",
    "    \"High blood pressure and chest pain were observed, pointing towards cardiovascular diseases.\",\n",
    "    \"Generalized weakness and fever may be signs of general pathological conditions.\"\n",
    "]\n",
    "\n",
    "print(\"\\nAdditional Test Cases:\")\n",
    "for i, test_text in enumerate(test_cases, start=1):\n",
    "    pred_label = predict_label(model, test_text, word_to_index, max_len, device, label_map)\n",
    "    print(f\"Test Case {i}:\")\n",
    "    print(f\"Text: {test_text}\")\n",
    "    print(f\"Predicted label: {pred_label}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41b80ca6-0b9c-429c-8f5e-82482ae7bd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Additional Test Cases:\n",
      "Test Case 1:\n",
      "Abstract: The patient was diagnosed with a tumor in the lung.\n",
      "Predicted label: neoplasms\n",
      "\n",
      "Test Case 2:\n",
      "Abstract: Severe abdominal pain and persistent nausea.\n",
      "Predicted label: general pathological conditions\n",
      "\n",
      "Test Case 3:\n",
      "Abstract: The patient exhibits tremors and loss of motor control.\n",
      "Predicted label: nervous system diseases\n",
      "\n",
      "Test Case 4:\n",
      "Abstract: High blood pressure and chest pain were observed.\n",
      "Predicted label: general pathological conditions\n",
      "\n",
      "Test Case 5:\n",
      "Abstract: Generalized weakness and fever.\n",
      "Predicted label: general pathological conditions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "    \"The patient was diagnosed with a tumor in the lung.\",\n",
    "    \"Severe abdominal pain and persistent nausea.\",\n",
    "    \"The patient exhibits tremors and loss of motor control.\",\n",
    "    \"High blood pressure and chest pain were observed.\",\n",
    "    \"Generalized weakness and fever.\"\n",
    "]\n",
    "\n",
    "print(\"\\nAdditional Test Cases:\")\n",
    "for i, test_abstract in enumerate(test_cases, start=1):\n",
    "    pred_label = predict_label(model, test_abstract, word_to_index, max_len, device, label_map)\n",
    "    print(f\"Test Case {i}:\")\n",
    "    print(f\"Abstract: {test_abstract}\")\n",
    "    print(f\"Predicted label: {pred_label}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b76319-4e30-43d6-b010-d869f08c5db7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
