{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning LSTM + Self-Attention Model on MIMIC-IV Notes for Diagnosis Prediction\n",
    "\n",
    "In this section, we fine-tune a previously trained LSTM + Self-Attention model (originally trained on the MeDAL dataset) on the MIMIC-IV clinical notes dataset for the task of **diagnosis prediction**.\n",
    "\n",
    "We use discharge summaries and other clinical notes from the MIMIC-IV dataset and aim to predict one or more ICD diagnosis codes for each note. This is a **multi-label classification** problem, as each clinical note can be associated with multiple diagnoses.\n",
    "\n",
    "Here’s a summary of the pipeline we built:\n",
    "- Loaded and preprocessed the MIMIC-IV dataset, extracting tokenized summaries of up to 200 words.\n",
    "- Used GloVe embeddings trained on an external vocabulary derived from the dataset.\n",
    "- Created lazy loading dataloaders to efficiently process and batch data during training.\n",
    "- Fine-tuned the LSTM + Self-Attention model by training on this new dataset, using the pretrained model weights from MeDAL as a starting point.\n",
    "\n",
    "This approach helps us evaluate how well a model trained on biomedical abbreviation disambiguation (MeDAL) can transfer to a real-world clinical task like diagnosis prediction from notes.\n",
    "\n",
    "MIMIC-IV dataset can be found [here](https://physionet.org/content/mimiciii-demo/1.4/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment set up: sys.path updated, working dir set to project root.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/prashanthjaganathan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/prashanthjaganathan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/prashanthjaganathan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%run ../setup.py\n",
    "\n",
    "from src.data.mimic import MIMIC_IV\n",
    "from env import ProjectPaths\n",
    "import torch\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from src.models.trainer import ModelTrainer\n",
    "from src.vectorizer.glove_embeddings import GloVeEmbedding\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIMIC-IV dataset initialized with name: mimic-iv\n",
      "Saved sklearn-computed class weights for 3370 classes to /home/jaganathan.p/pretaining-language-models-for-medical-text/dataset/MIMIC-IV/class_weights.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>text</th>\n",
       "      <th>icd_code</th>\n",
       "      <th>icd_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10733714</td>\n",
       "      <td>20907974</td>\n",
       "      <td>33005972</td>\n",
       "      <td>\\nName:  ___                Unit No:   ___\\n ...</td>\n",
       "      <td>[I269, I959]</td>\n",
       "      <td>[Hypotension, unspecified, Other pulmonary emb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18171767</td>\n",
       "      <td>22031497</td>\n",
       "      <td>38870680</td>\n",
       "      <td>\\nName:  ___                  Unit No:   ___\\...</td>\n",
       "      <td>[E106]</td>\n",
       "      <td>[Type 1 diabetes mellitus with hyperglycemia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13084154</td>\n",
       "      <td>22214693</td>\n",
       "      <td>30334417</td>\n",
       "      <td>\\nName:  ___                       Unit No:  ...</td>\n",
       "      <td>[R079, R109]</td>\n",
       "      <td>[Chest pain, unspecified, Unspecified abdomina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12872503</td>\n",
       "      <td>24929324</td>\n",
       "      <td>38402167</td>\n",
       "      <td>\\nName:  ___                  Unit No:   ___\\...</td>\n",
       "      <td>[276, 812, 401, E888]</td>\n",
       "      <td>[HYPOKALEMIA, FX UPPER HUMERUS NEC-CL, HYPERTE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12795168</td>\n",
       "      <td>22548131</td>\n",
       "      <td>33831755</td>\n",
       "      <td>\\nName:  ___                      Unit No:   ...</td>\n",
       "      <td>[434]</td>\n",
       "      <td>[CEREBRAL ART OCCLUS W/INFARCT]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   hadm_id   stay_id  \\\n",
       "0    10733714  20907974  33005972   \n",
       "1    18171767  22031497  38870680   \n",
       "2    13084154  22214693  30334417   \n",
       "3    12872503  24929324  38402167   \n",
       "4    12795168  22548131  33831755   \n",
       "\n",
       "                                                text               icd_code  \\\n",
       "0   \\nName:  ___                Unit No:   ___\\n ...           [I269, I959]   \n",
       "1   \\nName:  ___                  Unit No:   ___\\...                 [E106]   \n",
       "2   \\nName:  ___                       Unit No:  ...           [R079, R109]   \n",
       "3   \\nName:  ___                  Unit No:   ___\\...  [276, 812, 401, E888]   \n",
       "4   \\nName:  ___                      Unit No:   ...                  [434]   \n",
       "\n",
       "                                           icd_title  \n",
       "0  [Hypotension, unspecified, Other pulmonary emb...  \n",
       "1      [Type 1 diabetes mellitus with hyperglycemia]  \n",
       "2  [Chest pain, unspecified, Unspecified abdomina...  \n",
       "3  [HYPOKALEMIA, FX UPPER HUMERUS NEC-CL, HYPERTE...  \n",
       "4                    [CEREBRAL ART OCCLUS W/INFARCT]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_dataset = MIMIC_IV('mimic-iv')\n",
    "data = mimic_dataset.load_dataset()\n",
    "class_to_idx, icd_to_class, icd_to_classes = mimic_dataset.convert_class_to_idx()\n",
    "mimic_dataset.group_multilabel_data()\n",
    "train_data, val_data, test_data = mimic_dataset.split_dataset()\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total classes: 3370\n",
      "Total samples: 154704\n",
      "Mean samples/class: 88.93\n",
      "Median samples/class: 4.00\n",
      "Min samples/class: 1\n",
      "Max samples/class: 11440\n",
      "\n",
      "Top 10 most frequent classes:\n",
      "icd_code  count\n",
      "     780  11440\n",
      "     401  10440\n",
      "     250   7267\n",
      "     789   6979\n",
      "     786   6668\n",
      "    R060   4759\n",
      "    R109   4261\n",
      "     486   3974\n",
      "     599   3965\n",
      "    R079   3745\n",
      "\n",
      "Bottom 10 least frequent classes:\n",
      "icd_code  count\n",
      "    K297      1\n",
      "    T348      1\n",
      "    W501      1\n",
      "    Y793      1\n",
      "    M799      1\n",
      "    B690      1\n",
      "    V224      1\n",
      "    G248      1\n",
      "    H819      1\n",
      "    G032      1\n"
     ]
    }
   ],
   "source": [
    "# Flatten the list of lists into a single list of all ICD codes\n",
    "all_codes = [code for codes in mimic_dataset.data['icd_code'] for code in codes]\n",
    "\n",
    "# Count occurrences of each ICD code\n",
    "code_counts = Counter(all_codes)\n",
    "\n",
    "# Convert to DataFrame\n",
    "grouped = pd.DataFrame(code_counts.items(), columns=['icd_code', 'count'])\n",
    "grouped = grouped.sort_values(by='count', ascending=False)\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Total classes: {len(grouped)}\")\n",
    "print(f\"Total samples: {len(mimic_dataset.data)}\")\n",
    "print(f\"Mean samples/class: {grouped['count'].mean():.2f}\")\n",
    "print(f\"Median samples/class: {grouped['count'].median():.2f}\")\n",
    "print(f\"Min samples/class: {grouped['count'].min()}\")\n",
    "print(f\"Max samples/class: {grouped['count'].max()}\")\n",
    "\n",
    "# Print top and bottom 10 classes\n",
    "print(\"\\nTop 10 most frequent classes:\")\n",
    "print(grouped.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nBottom 10 least frequent classes:\")\n",
    "print(grouped.tail(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def load_config(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "config = load_config('config/config.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize train and valid sets using whitespace tokenizer and store in `.parquet` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3b7d4ed07fe4b24835f0fac136f8ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=2211), Label(value='0 / 2211'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>text</th>\n",
       "      <th>icd_code</th>\n",
       "      <th>icd_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15642508</td>\n",
       "      <td>25781750</td>\n",
       "      <td>39789671</td>\n",
       "      <td>past medical history significant aortic valve ...</td>\n",
       "      <td>[R509]</td>\n",
       "      <td>[Fever, unspecified]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10149765</td>\n",
       "      <td>26535625</td>\n",
       "      <td>36381439</td>\n",
       "      <td>name unit admission date discharge date date b...</td>\n",
       "      <td>[N631]</td>\n",
       "      <td>[Unspecified lump in the right breast, unspeci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12166185</td>\n",
       "      <td>26271007</td>\n",
       "      <td>36468972</td>\n",
       "      <td>medicine nightfloat admission note admission d...</td>\n",
       "      <td>[K838]</td>\n",
       "      <td>[Other specified diseases of biliary tract]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19979469</td>\n",
       "      <td>22114071</td>\n",
       "      <td>36419793</td>\n",
       "      <td>hepatitis c develop obstructive jaundice sever...</td>\n",
       "      <td>[157, 780, 070]</td>\n",
       "      <td>[UNSPECIFIED VIRAL HEPATITIS C WITHOUT HEPATIC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17691221</td>\n",
       "      <td>29996653</td>\n",
       "      <td>35619578</td>\n",
       "      <td>old female significant pmh right mca stroke sp...</td>\n",
       "      <td>[N179]</td>\n",
       "      <td>[Acute kidney failure, unspecified]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   hadm_id   stay_id  \\\n",
       "0    15642508  25781750  39789671   \n",
       "1    10149765  26535625  36381439   \n",
       "2    12166185  26271007  36468972   \n",
       "3    19979469  22114071  36419793   \n",
       "4    17691221  29996653  35619578   \n",
       "\n",
       "                                                text         icd_code  \\\n",
       "0  past medical history significant aortic valve ...           [R509]   \n",
       "1  name unit admission date discharge date date b...           [N631]   \n",
       "2  medicine nightfloat admission note admission d...           [K838]   \n",
       "3  hepatitis c develop obstructive jaundice sever...  [157, 780, 070]   \n",
       "4  old female significant pmh right mca stroke sp...           [N179]   \n",
       "\n",
       "                                           icd_title  \n",
       "0                               [Fever, unspecified]  \n",
       "1  [Unspecified lump in the right breast, unspeci...  \n",
       "2        [Other specified diseases of biliary tract]  \n",
       "3  [UNSPECIFIED VIRAL HEPATITIS C WITHOUT HEPATIC...  \n",
       "4                [Acute kidney failure, unspecified]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_train = mimic_dataset.preprocess(splits=['train'], summary_len=200)\n",
    "preprocessed_train = preprocessed_train[preprocessed_train['text'].str.strip().astype(bool)]\n",
    "preprocessed_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "mimic_dir = ProjectPaths.DATASET_DIR.value / 'MIMIC-IV' / 'preprocessed_subset'\n",
    "mimic_dir.mkdir(parents=True, exist_ok=True)\n",
    "preprocessed_train.to_csv(mimic_dir / 'train.csv', index=False)\n",
    "preprocessed_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c801d6c74f4e4cc39307868b01256fb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=277), Label(value='0 / 277'))), HB…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>text</th>\n",
       "      <th>icd_code</th>\n",
       "      <th>icd_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15401792</td>\n",
       "      <td>27111495</td>\n",
       "      <td>35301659</td>\n",
       "      <td>yo f history l elbow surgery nstemi sp pci 2 s...</td>\n",
       "      <td>[I269]</td>\n",
       "      <td>[Other pulmonary embolism without acute cor pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12429688</td>\n",
       "      <td>22424939</td>\n",
       "      <td>31874067</td>\n",
       "      <td>right handed woman significant past medical hi...</td>\n",
       "      <td>[787, 780]</td>\n",
       "      <td>[ALTERED MENTAL STATUS , NAUSEA WITH VOMITING,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18280004</td>\n",
       "      <td>20198748</td>\n",
       "      <td>34991331</td>\n",
       "      <td>parkinsonism recently admit neurology worsen l...</td>\n",
       "      <td>[780]</td>\n",
       "      <td>[ALTERED MENTAL STATUS ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14913511</td>\n",
       "      <td>20239006</td>\n",
       "      <td>32165170</td>\n",
       "      <td>ms female past medical history notable afib wa...</td>\n",
       "      <td>[S121, W183]</td>\n",
       "      <td>[Posterior displaced Type II dens fracture, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13878740</td>\n",
       "      <td>20989835</td>\n",
       "      <td>39134102</td>\n",
       "      <td>mr year old man nash cirrhosis cb hepatic ence...</td>\n",
       "      <td>[R418]</td>\n",
       "      <td>[Altered mental status, unspecified]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   hadm_id   stay_id  \\\n",
       "0    15401792  27111495  35301659   \n",
       "1    12429688  22424939  31874067   \n",
       "2    18280004  20198748  34991331   \n",
       "3    14913511  20239006  32165170   \n",
       "4    13878740  20989835  39134102   \n",
       "\n",
       "                                                text      icd_code  \\\n",
       "0  yo f history l elbow surgery nstemi sp pci 2 s...        [I269]   \n",
       "1  right handed woman significant past medical hi...    [787, 780]   \n",
       "2  parkinsonism recently admit neurology worsen l...         [780]   \n",
       "3  ms female past medical history notable afib wa...  [S121, W183]   \n",
       "4  mr year old man nash cirrhosis cb hepatic ence...        [R418]   \n",
       "\n",
       "                                           icd_title  \n",
       "0  [Other pulmonary embolism without acute cor pu...  \n",
       "1  [ALTERED MENTAL STATUS , NAUSEA WITH VOMITING,...  \n",
       "2                           [ALTERED MENTAL STATUS ]  \n",
       "3  [Posterior displaced Type II dens fracture, in...  \n",
       "4               [Altered mental status, unspecified]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_val = mimic_dataset.preprocess(splits=['valid'], summary_len=200)\n",
    "preprocessed_val = preprocessed_val[preprocessed_val['text'].str.strip().astype(bool)]\n",
    "preprocessed_val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "mimic_dir = ProjectPaths.DATASET_DIR.value / 'MIMIC-IV' / 'preprocessed_subset'\n",
    "mimic_dir.mkdir(parents=True, exist_ok=True)\n",
    "preprocessed_val.to_csv(mimic_dir / 'valid.csv', index=False)\n",
    "preprocessed_val.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b9beb12fe0418883224db141f680f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=2211), Label(value='0 / 2211'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully!\n"
     ]
    }
   ],
   "source": [
    "train_tokens = mimic_dataset.tokenize(tokenizer_type='whitespace', splits=['train'])\n",
    "\n",
    "if isinstance(train_tokens, pd.Series):\n",
    "    df = pd.DataFrame(train_tokens)\n",
    "    df.columns = ['CONTEXT']\n",
    "else:\n",
    "    df = pd.DataFrame(train_tokens, columns=['CONTEXT'])\n",
    "\n",
    "# Save as a Parquet file\n",
    "mimic_dir = ProjectPaths.DATASET_DIR.value / 'MIMIC-IV' / 'whitespace_tokenized_200ctx_subset'\n",
    "mimic_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "file_name = \"dataset/MIMIC-IV/whitespace_tokenized_200ctx_subset/train.parquet\"\n",
    "df.to_parquet(file_name)\n",
    "print('Parquet file saved successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18e193e4f1a4cacbc51ca6f41cc3488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=277), Label(value='0 / 277'))), HB…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved successfully!\n"
     ]
    }
   ],
   "source": [
    "val_tokens = mimic_dataset.tokenize(tokenizer_type='whitespace', splits=['valid'])\n",
    "\n",
    "if isinstance(val_tokens, pd.Series):\n",
    "    df = pd.DataFrame(val_tokens)\n",
    "    df.columns = ['CONTEXT']\n",
    "else:\n",
    "    df = pd.DataFrame(val_tokens, columns=['CONTEXT'])\n",
    "\n",
    "file_name = \"dataset/MIMIC-IV/whitespace_tokenized_200ctx_subset/valid.parquet\"\n",
    "df.to_parquet(file_name)\n",
    "print('Parquet file saved successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Docs: 100%|██████████| 123763/123763 [00:00<00:00, 153429.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in train corpus: 123763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_tokens = pd.read_parquet(\"dataset/MIMIC-IV/whitespace_tokenized_200ctx_subset/train.parquet\", engine=\"pyarrow\")\n",
    "# to make it as a list[list[str]]\n",
    "tokenized_train_corpus = [context.tolist() for context in tqdm(train_tokens['CONTEXT'], 'Docs', len(train_tokens['CONTEXT']))] \n",
    "print(f'Number of documents in train corpus: {len(tokenized_train_corpus)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create vocabulary from the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External vocab size: 52445\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "min_word_freq = 2\n",
    "counter = Counter()\n",
    "\n",
    "for tokens in tokenized_train_corpus:\n",
    "    counter.update(tokens)\n",
    "\n",
    "external_vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for token, freq in counter.items():\n",
    "    if freq >= min_word_freq:\n",
    "        external_vocab[token] = len(external_vocab)\n",
    "\n",
    "print(f\"External vocab size: {len(external_vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_glove = config['embedding_models']['glove']\n",
    "config_glove['external_vocab'] = external_vocab  # Pass the external vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "First, let's create the dataloader with embeddings as features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyEmbeddingDataset(Dataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            file_path, \n",
    "            embedding_model, \n",
    "            labels,  # list of list of labels\n",
    "            class_to_idx, \n",
    "            return_tokens=True,\n",
    "            max_seq_len=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            file_path (str): Path to the Parquet file containing the tokenized text.\n",
    "            embedding_model: The custom embedding model (e.g., GloVeEmbedding).\n",
    "            labels (List[List[str]]): Multi-label list for each document.\n",
    "            class_to_idx (dict): Mapping from class label to integer index.\n",
    "            max_seq_len (int, optional): Max sequence length for padding/truncating.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        table = pq.read_table(self.file_path)\n",
    "        self.tokenized_corpus = table['CONTEXT']\n",
    "        self.embedding_model = embedding_model\n",
    "        self.labels = labels\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.return_tokens = return_tokens\n",
    "        self.num_classes = len(class_to_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokenized_corpus[idx].as_py()\n",
    "\n",
    "        # Get token indices or embeddings\n",
    "        if self.return_tokens:\n",
    "            embeddings = self.embedding_model.token_indices(tokens)\n",
    "        else:\n",
    "            embeddings = self.embedding_model.embed(tokens)\n",
    "\n",
    "        seq_len = len(embeddings)\n",
    "        embeddings = embeddings + [0] * (self.max_seq_len - seq_len) if seq_len < self.max_seq_len else embeddings[:self.max_seq_len]\n",
    "        embeddings_np = np.array(embeddings, dtype=np.float32)\n",
    "\n",
    "        # Mask to indicate valid tokens\n",
    "        mask = np.ones(seq_len, dtype=np.float32)\n",
    "        if self.max_seq_len is not None and seq_len < self.max_seq_len:\n",
    "            mask = np.concatenate([mask, np.zeros(self.max_seq_len - seq_len, dtype=np.float32)])\n",
    "\n",
    "        # Convert label list into multi-hot vector\n",
    "        label_list = self.labels[idx]\n",
    "        label_vector = torch.zeros(self.num_classes, dtype=torch.float32)\n",
    "        for label in label_list:\n",
    "            if label in self.class_to_idx:\n",
    "                label_vector[self.class_to_idx[label]] = 1.0\n",
    "\n",
    "        return (\n",
    "            torch.tensor(embeddings_np, dtype=torch.float32), \n",
    "            torch.tensor(mask, dtype=torch.bool),\n",
    "            label_vector\n",
    "        )\n",
    "\n",
    "\n",
    "def create_lazy_dataloader(file_path, embedding_model, labels, class_to_idx, batch_size, max_seq_len=None, shuffle=False):\n",
    "    dataset = LazyEmbeddingDataset(file_path, embedding_model, labels, class_to_idx, max_seq_len=max_seq_len, return_tokens=True)\n",
    "    # return dataset\n",
    "    return DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=shuffle, \n",
    "        num_workers=0, \n",
    "        pin_memory=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding_model = GloVeEmbedding(**config['embedding_models']['glove'])\n",
    "\n",
    "max_seq_len = config['datasets']['mimic-iv']['max_sequence_length']\n",
    "batch_size = config['training']['hyperparameters']['batch_size']\n",
    "\n",
    "trainloader = create_lazy_dataloader(\n",
    "    'dataset/MIMIC-IV/whitespace_tokenized_200ctx_subset/train.parquet', \n",
    "    glove_embedding_model, \n",
    "    mimic_dataset.train_data['icd_code'],\n",
    "    class_to_idx, \n",
    "    batch_size=batch_size,\n",
    "    max_seq_len=max_seq_len,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "valloader = create_lazy_dataloader(\n",
    "    'dataset/MIMIC-IV/whitespace_tokenized_200ctx_subset/valid.parquet', \n",
    "    glove_embedding_model, \n",
    "    mimic_dataset.val_data['icd_code'],\n",
    "    class_to_idx, \n",
    "    batch_size=batch_size,\n",
    "    max_seq_len=max_seq_len,\n",
    "    shuffle = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pretrained model for training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaganathan.p/pretaining-language-models-for-medical-text/src/models/trainer.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.pretrained_model = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- lstm_and_self_attention --------\n",
      "{'lstm_units': 2, 'lstm_hidden_dim': 128, 'num_attention_heads': 16, 'dropout': 0.3, 'num_classes': 3370, 'embedding_dim': 100, 'create_embedding_layer': True, 'embedding_model': <src.vectorizer.glove_embeddings.GloVeEmbedding object at 0x2ac4cdc7fbb0>}\n",
      "Loaded 24 parameters from pretrained model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 32/1934 [00:25<25:05,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 200, 100])\n",
      "[NaN DETECTED] NaN found in input tensor x_residual. Shape: torch.Size([64, 200, 100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/jaganathan.p/pretaining-language-models-for-medical-text/mimic_demo.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ood.discovery.neu.edu/home/jaganathan.p/pretaining-language-models-for-medical-text/mimic_demo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model_trainer \u001b[39m=\u001b[39m ModelTrainer(\n\u001b[1;32m      <a href='vscode-notebook-cell://ood.discovery.neu.edu/home/jaganathan.p/pretaining-language-models-for-medical-text/mimic_demo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     config_file\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mconfig.yaml\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ood.discovery.neu.edu/home/jaganathan.p/pretaining-language-models-for-medical-text/mimic_demo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     pretrained_model_path\u001b[39m=\u001b[39mProjectPaths\u001b[39m.\u001b[39mPROJECT_DIR\u001b[39m.\u001b[39mvalue \u001b[39m/\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrained_models/models/medal_glove_200ctx_lstm_and_self_attention_model_model.pth\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ood.discovery.neu.edu/home/jaganathan.p/pretaining-language-models-for-medical-text/mimic_demo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     )\n\u001b[0;32m----> <a href='vscode-notebook-cell://ood.discovery.neu.edu/home/jaganathan.p/pretaining-language-models-for-medical-text/mimic_demo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m train_results \u001b[39m=\u001b[39m model_trainer\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m      <a href='vscode-notebook-cell://ood.discovery.neu.edu/home/jaganathan.p/pretaining-language-models-for-medical-text/mimic_demo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     trainloader,\n\u001b[1;32m      <a href='vscode-notebook-cell://ood.discovery.neu.edu/home/jaganathan.p/pretaining-language-models-for-medical-text/mimic_demo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     valloader,\n\u001b[1;32m      <a href='vscode-notebook-cell://ood.discovery.neu.edu/home/jaganathan.p/pretaining-language-models-for-medical-text/mimic_demo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     dataset\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmimic-iv\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ood.discovery.neu.edu/home/jaganathan.p/pretaining-language-models-for-medical-text/mimic_demo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     embedding_dim\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ood.discovery.neu.edu/home/jaganathan.p/pretaining-language-models-for-medical-text/mimic_demo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     embedding_model \u001b[39m=\u001b[39;49m glove_embedding_model\n\u001b[1;32m     <a href='vscode-notebook-cell://ood.discovery.neu.edu/home/jaganathan.p/pretaining-language-models-for-medical-text/mimic_demo.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m )\n",
      "File \u001b[0;32m~/pretaining-language-models-for-medical-text/src/models/trainer.py:163\u001b[0m, in \u001b[0;36mModelTrainer.train\u001b[0;34m(self, trainloader, valloader, dataset, embedding_dim, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_name \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmimic-iv\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    162\u001b[0m     targets \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 163\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcriterion(outputs, targets\u001b[39m.\u001b[39;49mfloat())  \u001b[39m# BCEWithLogitsLoss expects target to be float\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     targets \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/.conda/envs/medal/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/medal/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/medal/lib/python3.10/site-packages/torch/nn/modules/loss.py:734\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 734\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39;49m, target,\n\u001b[1;32m    735\u001b[0m                                               \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    736\u001b[0m                                               pos_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_weight,\n\u001b[1;32m    737\u001b[0m                                               reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m~/.conda/envs/medal/lib/python3.10/site-packages/torch/nn/functional.py:3241\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3238\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3239\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[0;32m-> 3241\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (target\u001b[39m.\u001b[39msize() \u001b[39m==\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msize()):\n\u001b[1;32m   3242\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTarget size (\u001b[39m\u001b[39m{\u001b[39;00mtarget\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m) must be the same as input size (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   3244\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "model_trainer = ModelTrainer(\n",
    "    config_file='config.yaml',\n",
    "    pretrained_model_path=ProjectPaths.PROJECT_DIR.value / 'trained_models/models/medal_glove_200ctx_lstm_and_self_attention_model_model.pth'\n",
    "    )\n",
    "\n",
    "train_results = model_trainer.train(\n",
    "    trainloader,\n",
    "    valloader,\n",
    "    dataset='mimic-iv',\n",
    "    embedding_dim=100,\n",
    "    embedding_model = glove_embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚠️ Current Issue: NaNs During Fine-Tuning on MIMIC-IV Notes\n",
    "\n",
    "While fine-tuning the pretrained LSTM + Self-Attention model (originally trained on the MeDAL dataset) for diagnosis prediction using MIMIC-IV Notes, training fails due to NaNs appearing during the forward pass — specifically during computation of `x_residual` in the feedforward layer.\n",
    "\n",
    "### What I've Tried:\n",
    "- Gradient clipping to avoid exploding gradients.\n",
    "- Verified input tensors are normalized and contain no NaNs.\n",
    "- Applied class weights in the loss function (`BCEWithLogitsLoss`) for class imbalance.\n",
    "- Checked model output and target sizes — both match as expected.\n",
    "- Inspected weights — noticed values shrinking (~0.01), indicating possible vanishing gradients.\n",
    "- Tried setting a minimum/maximum threshold for gradients — no effect.\n",
    "- Tried monitoring the weights and see when the value explodes/vanishes. This seems to happen suddenly in just a few steps.\n",
    "\n",
    "### Status\n",
    "Still debugging. Root cause is unclear — suspecting instability in the residual connection or attention layer when initialized with pretrained weights.\n",
    "\n",
    "If you have any ideas or suggestions for fixing this, I'd really appreciate your help!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.plot_results(train_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
