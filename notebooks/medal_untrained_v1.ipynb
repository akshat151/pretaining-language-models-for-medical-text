{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Code Repository and Structure\n",
    "\n",
    "This notebook shows the early work we did for the project. \n",
    "We built features that let us easily switch between different tokenizers \n",
    "and embedding methods just by changing function arguments. \n",
    "The goal was to make the project more flexible and scalable for NLP tasks.\n",
    "\n",
    "NOTE: This code doesn't show any final results — it's just our starting point \n",
    "to test out different ideas like embedding models and tokenizers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment set up: sys.path updated, working dir set to project root.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/prashanthjaganathan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/prashanthjaganathan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%run ../setup.py\n",
    "\n",
    "from src.data.medal import MeDALSubset\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import yaml\n",
    "from src.models.trainer import ModelTrainer\n",
    "from src.vectorizer.trainable import TrainableEmbedding\n",
    "from src.vectorizer.glove_embeddings import GloVeEmbedding\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from tqdm import tqdm\n",
    "from src.utils import save_embeddings_to_file\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "from src.vectorizer.bio_bert import BioBERTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MeDAL dataset initialized with name: MeDAL\n",
      "Dataset downloaded to: /home/jaganathan.p/.cache/kagglehub/datasets/xhlulu/medal-emnlp/versions/4\n",
      "Dataset moved to: /home/jaganathan.p/pretaining-language-models-for-medical-text/dataset\n",
      "Total number of classes: 22555\n"
     ]
    }
   ],
   "source": [
    "medal_dataset = MeDALSubset('MeDAL')\n",
    "data, train_data, val_data, test_data = medal_dataset.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def load_config(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "config = load_config('config/config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train = medal_dataset.preprocess(['train', 'valid'])\n",
    "# NOTE: Pre-processed for 503 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = processed_train\n",
    "\n",
    "# Save to CSV\n",
    "# NOTE: commented out v sensitive code, files contain huge corpus of preprocessed data\n",
    "# DO NOT OVERWRITE THE FILES\n",
    "train_df.to_csv(\"dataset/medal/preprocessed_subset/train.csv\", index=False)\n",
    "val_df.to_csv(\"dataset/medal/preprocessed_subset/valid.csv\", index=False)\n",
    "\n",
    "print(\"CSV files saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and load pre-processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_train = pd.read_csv('dataset/medal/preprocessed_subset/train.csv')\n",
    "preprocessed_val = pd.read_csv('dataset/medal/preprocessed_subset/valid.csv')\n",
    "# medal_dataset.train_data = preprocessed_train\n",
    "medal_dataset.val_data = preprocessed_val\n",
    "preprocessed_val.head(1)['TEXT']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_abbr_tokens = medal_dataset.tokenize('nltk', splits=['train'])\n",
    "df = pd.DataFrame(train_abbr_tokens, columns=['tokenized_text', 'abbreviation'])\n",
    "\n",
    "# Save as a Parquet file\n",
    "file_name = \"dataset/medal/nltk_tokenized_preprocessed_subset/train.parquet\"\n",
    "df.to_parquet(file_name)\n",
    "print('Parquet file saved successfully!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_abbr_tokens = medal_dataset.tokenize('nltk', splits=['valid'])\n",
    "df = pd.DataFrame(val_abbr_tokens, columns=['tokenized_text', 'abbreviation'])\n",
    "\n",
    "# Save as a Parquet file\n",
    "file_name = \"dataset/medal/nltk_tokenized_preprocessed_subset/valid.parquet\"\n",
    "df.to_parquet(file_name)\n",
    "print('Parquet file saved successfully!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = pd.read_parquet(\n",
    "    \"dataset/medal/nltk_tokenized_preprocessed_subset/train.parquet\", \n",
    "    engine=\"pyarrow\"\n",
    "    ).squeeze()\n",
    "\n",
    "# to make it as a list[list[str]]\n",
    "tokenized_train_corpus = [(doc.tolist(), abbr) for doc, abbr in tqdm(zip(train_tokens['tokenized_text'], train_tokens['abbreviation']), 'Docs', len(train_tokens))] \n",
    "tokenized_train_corpus[:3]\n",
    "# print(f'Number of documents in train corpus: {len(tokenized_train_corpus)}')\n",
    "# train_tokens = [doc for doc, _ in tokenized_train_corpus]\n",
    "# train_abbr = [abbv for _, abbv in tokenized_train_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tokens = pd.read_parquet(\"dataset/medal/nltk_tokenized_preprocessed_subset/valid.parquet\", engine=\"pyarrow\").squeeze()\n",
    "\n",
    "# to make it as a list[list[str]]\n",
    "tokenized_val_corpus = [doc.tolist() for doc in tqdm(val_tokens, 'Docs', len(val_tokens))] \n",
    "print(f'Number of documents in val corpus: {len(tokenized_val_corpus)}')\n",
    "val_tokens = [doc for doc, _ in tokenized_val_corpus]\n",
    "val_abbr = [abbv for _, abbv in tokenized_val_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Word2Vec model on the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = TrainableEmbedding(\n",
    "        tokenized_corpus=tokenized_train_corpus,\n",
    "        algorithm=\"word2vec\",\n",
    "        vector_size=100,\n",
    "        window=5,\n",
    "        min_count=2\n",
    "    )\n",
    "embeddings = embedding_model.embed(tokenized_train_corpus)\n",
    "print(f'Embedding dimensions: {len(embeddings[0][0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training FastText model on the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = TrainableEmbedding(\n",
    "        tokenized_corpus=tokenized_train_corpus,\n",
    "        algorithm=\"fasttext\",\n",
    "        vector_size=100,\n",
    "        window=7,\n",
    "        min_count=2\n",
    "    )\n",
    "embeddings = embedding_model.embed(tokenized_train_corpus)\n",
    "print(f'Embeddings Dimensions: {len(embeddings[0][0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing the embedding to Truncated Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = TrainableEmbedding(\n",
    "        tokenized_corpus=train_tokens.tolist(),\n",
    "        algorithm=\"tfidf\",\n",
    "        vector_size=100,\n",
    "        window=5,\n",
    "        min_count=2\n",
    "    )\n",
    "embeddings = embedding_model.embed(train_tokens.tolist())\n",
    "print(f'Embedding dimensions: {embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: bio wordvec model is like 12GB and unable to load it in the memeory and build embeddings\n",
    "\n",
    "train_embeddings = medal_dataset.embed(\n",
    "    'bio_wordvec',\n",
    "    splits=['train'],\n",
    "    tokenized_data = train_tokens,\n",
    "    model_path = 'trained_models/embeddings/pretrained/bio_wordvec.bin'\n",
    ")\n",
    "len(train_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying not to use bio bert as it involves trasformer models and our architecture is limited to using \n",
    "LSTM + Self Attention, therefore, looking for static embedding models only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Model Training\n",
    "\n",
    "First, let's create the dataloader with embeddings as features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyEmbeddingDataset(Dataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            file_path, \n",
    "            embedding_model,\n",
    "            class_to_idx, \n",
    "            max_seq_len=None,\n",
    "            return_tokens=True,):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            file_path (str): Path to the Parquet file containing the tokenized text.\n",
    "            embedding_model: The custom embedding model (e.g., GloVeEmbedding).\n",
    "            labels (list): Labels corresponding to each document.\n",
    "            class_to_idx (dict): Mapping from class label to integer index.\n",
    "            max_seq_len (int, optional): Max sequence length for padding/truncating.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.preprocessed_corpus = pd.read_csv(file_path)\n",
    "        self.contexts = self.preprocessed_corpus['TEXT']\n",
    "        self.corresponding_abbreviations = self.preprocessed_corpus['ABBREVIATION']\n",
    "        self.labels = self.preprocessed_corpus['LABEL']\n",
    "        self.embedding_model = embedding_model\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.return_tokens = return_tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context = self.contexts[idx]\n",
    "        abbreviation = self.corresponding_abbreviations[idx]\n",
    "\n",
    "        # Compute the embeddings for this document on the fly using the embedding model\n",
    "        embedding, attention_mask = self.embedding_model.embed(context, self.max_seq_len)  # shape: (seq_len, embedding_dim)\n",
    "\n",
    "        if embedding.ndim == 3 and embedding.shape[0] == 1:\n",
    "            embedding = embedding.squeeze(0)\n",
    "        if attention_mask.ndim == 2 and attention_mask.shape[0] == 1:\n",
    "            attention_mask = attention_mask.squeeze(0)\n",
    "            \n",
    "        # Convert label to index\n",
    "        label = self.labels[idx]\n",
    "        label_idx = self.class_to_idx[label]\n",
    "\n",
    "        return (torch.tensor(embedding, dtype=torch.float32),\n",
    "                torch.tensor(attention_mask, dtype=torch.float32),  # Return the mask\n",
    "                torch.tensor(label_idx, dtype=torch.long))\n",
    "\n",
    "\n",
    "def create_lazy_dataloader(file_path, embedding_model, class_to_idx, batch_size, max_seq_len=None):\n",
    "    dataset = LazyEmbeddingDataset(file_path, embedding_model, class_to_idx, max_seq_len=max_seq_len, return_tokens=True)\n",
    "    return DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embedding_model = BioBERTModel(**config['embedding_models']['bio_bert'])\n",
    "\n",
    "max_seq_len = config['datasets']['medal']['max_sequence_length']\n",
    "batch_size = config['training']['hyperparameters']['batch_size']\n",
    "\n",
    "trainloader = create_lazy_dataloader(\n",
    "    'dataset/medal/preprocessed_subset/train.csv', \n",
    "    bert_embedding_model, \n",
    "    medal_dataset.class_to_idx, \n",
    "    batch_size=batch_size,\n",
    "    max_seq_len=max_seq_len\n",
    "    )\n",
    "\n",
    "\n",
    "valloader = create_lazy_dataloader(\n",
    "    'dataset/medal/preprocessed_subset/valid.csv', \n",
    "    bert_embedding_model, \n",
    "    medal_dataset.class_to_idx, \n",
    "    batch_size=batch_size,\n",
    "    max_seq_len=max_seq_len\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- lstm_and_self_attention --------\n",
      "{'lstm_units': 2, 'lstm_hidden_dim': 128, 'num_attention_heads': 4, 'dropout': 0.3, 'num_classes': 22555, 'embedding_dim': 100, 'create_embedding_layer': False, 'embedding_model': BioBERTModel(\n",
      "  (model): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (projection): Linear(in_features=768, out_features=100, bias=True)\n",
      ")}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/46875 [00:00<?, ?it/s]/tmp/ipykernel_96846/1978743710.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return (torch.tensor(embedding, dtype=torch.float32),\n",
      "/tmp/ipykernel_96846/1978743710.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(attention_mask, dtype=torch.float32),  # Return the mask\n",
      "Training:   2%|▏         | 1018/46875 [1:46:52<78:39:25,  6.17s/it]"
     ]
    }
   ],
   "source": [
    "# Use the new dataloaders\n",
    "model_trainer = ModelTrainer(config_file='config.yaml')\n",
    "train_results = model_trainer.train(\n",
    "    trainloader, \n",
    "    valloader, \n",
    "    dataset='medal', \n",
    "    embedding_dim=100,\n",
    "    embedding_model = bert_embedding_model\n",
    ")\n",
    "\n",
    "# Stopped because training was too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.plot_results(train_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
