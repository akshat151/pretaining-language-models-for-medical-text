{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35f7e8fd-88d0-4bd4-8c58-3dfa93406241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train.csv: 3000000 rows\n",
      "After filtering: 3000000 rows, 22555 classes\n",
      "Classes: 22555\n",
      "Loaded tokenization from cache\n",
      "Vocab size: 489792\n",
      "Split: 2400000 train / 300000 val / 300000 test\n",
      "Loaded 400000 GloVe vectors\n",
      "LSTMClassifier(\n",
      "  (embedding): Embedding(489792, 100, padding_idx=0)\n",
      "  (lstm): LSTM(100, 128, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (fc): Linear(in_features=256, out_features=22555, bias=True)\n",
      "  (drop): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "Epoch 1/10 | Train Loss=1.2137, Acc=0.6421 | Val Loss=1.2630, Acc=0.6300\n",
      "Epoch 2/10 | Train Loss=0.7678, Acc=0.7546 | Val Loss=0.8700, Acc=0.7295\n",
      "Epoch 3/10 | Train Loss=0.5668, Acc=0.8083 | Val Loss=0.7042, Acc=0.7734\n",
      "Epoch 4/10 | Train Loss=0.4664, Acc=0.8401 | Val Loss=0.6401, Acc=0.7957\n",
      "Epoch 5/10 | Train Loss=0.3980, Acc=0.8622 | Val Loss=0.6109, Acc=0.8069\n",
      "Epoch 6/10 | Train Loss=0.3414, Acc=0.8809 | Val Loss=0.5892, Acc=0.8157\n",
      "Epoch 7/10 | Train Loss=0.2971, Acc=0.8962 | Val Loss=0.5977, Acc=0.8201\n",
      "Epoch 8/10 | Train Loss=0.2571, Acc=0.9100 | Val Loss=0.5889, Acc=0.8239\n",
      "Epoch 9/10 | Train Loss=0.2290, Acc=0.9204 | Val Loss=0.6006, Acc=0.8243\n",
      "Epoch 10/10 | Train Loss=0.2073, Acc=0.9280 | Val Loss=0.6193, Acc=0.8239\n",
      "Test Loss=0.6163, Test Acc=0.8235\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (test):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import spacy\n",
    "\n",
    "# -------------------------------\n",
    "# 0. Reproducibility & cuDNN tuner\n",
    "# -------------------------------\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load spaCy tokenizer\n",
    "# -------------------------------\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Unzip dataset if needed\n",
    "# -------------------------------\n",
    "if not os.path.exists(\"train.csv\"):\n",
    "    for z in (\"train.csv.zip\", \"archive (3).zip\"):\n",
    "        if os.path.exists(z):\n",
    "            with zipfile.ZipFile(z, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(\".\")\n",
    "\n",
    "# =======================\n",
    "# 3. Load the CSV\n",
    "# =======================\n",
    "try:\n",
    "    df = pd.read_csv(\"train.csv\")\n",
    "    print(f\"Loaded train.csv: {len(df)} rows\")\n",
    "except FileNotFoundError:\n",
    "    raise RuntimeError(\"train.csv not found – please place it in working dir\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. (Optional) Sample down\n",
    "# -------------------------------\n",
    "max_samples = 3_000_000\n",
    "if len(df) > max_samples:\n",
    "    df = df.sample(max_samples, random_state=42).reset_index(drop=True)\n",
    "    print(f\"Sampled down to {len(df)} rows\")\n",
    "\n",
    "# ================================\n",
    "# 5. Clean & filter rare labels\n",
    "# ================================\n",
    "df = df.dropna(subset=[\"TEXT\", \"LABEL\", \"LOCATION\"])\n",
    "df[\"LOCATION\"] = df[\"LOCATION\"].astype(int)\n",
    "label_counts = df[\"LABEL\"].value_counts()\n",
    "valid_labels = label_counts[label_counts >= 5].index\n",
    "df = df[df[\"LABEL\"].isin(valid_labels)].reset_index(drop=True)\n",
    "print(f\"After filtering: {len(df)} rows, {df['LABEL'].nunique()} classes\")\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Context extraction fn\n",
    "# -------------------------------\n",
    "def extract_context(text, loc, window_size=50):\n",
    "    toks = text.split()\n",
    "    start = max(0, loc - window_size // 2)\n",
    "    end = min(len(toks), loc + window_size // 2)\n",
    "    return \" \".join(toks[start:end])\n",
    "\n",
    "df[\"CONTEXT\"] = [extract_context(t, l) for t, l in zip(df[\"TEXT\"], df[\"LOCATION\"])]\n",
    "texts, labels = df[\"CONTEXT\"].tolist(), df[\"LABEL\"].tolist()\n",
    "\n",
    "# ===============================\n",
    "# 7. Encode labels\n",
    "# ===============================\n",
    "le = LabelEncoder()\n",
    "y_all = le.fit_transform(labels)\n",
    "num_classes = len(le.classes_)\n",
    "print(f\"Classes: {num_classes}\")\n",
    "\n",
    "# ============================================\n",
    "# 8. Tokenization (with caching)\n",
    "# ============================================\n",
    "TOK_PATH = \"tokenized_texts.pkl\"\n",
    "if os.path.exists(TOK_PATH):\n",
    "    with open(TOK_PATH, \"rb\") as f:\n",
    "        tokenized_texts = pickle.load(f)\n",
    "    print(\"Loaded tokenization from cache\")\n",
    "else:\n",
    "    tokenized_texts = []\n",
    "    for doc in nlp.pipe(texts, batch_size=1000):\n",
    "        tokenized_texts.append([t.text for t in doc if not t.is_punct and not t.is_space])\n",
    "    with open(TOK_PATH, \"wb\") as f:\n",
    "        pickle.dump(tokenized_texts, f)\n",
    "    print(\"Tokenization complete and saved\")\n",
    "\n",
    "# ============================================\n",
    "# 9. Build vocabulary & map to indices\n",
    "# ============================================\n",
    "all_toks = [t for doc in tokenized_texts for t in doc]\n",
    "vc = Counter(all_toks)\n",
    "vocab = {w for w, c in vc.items() if c >= 2}\n",
    "\n",
    "word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for w in sorted(vocab):\n",
    "    word2idx[w] = len(word2idx)\n",
    "vocab_size = len(word2idx)\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "\n",
    "def tokens_to_seq(ts):\n",
    "    return [word2idx.get(t, 1) for t in ts]\n",
    "\n",
    "seqs = [tokens_to_seq(doc) for doc in tokenized_texts]\n",
    "\n",
    "# ===========================\n",
    "# 10. Pad sequences\n",
    "# ============================\n",
    "MAX_LEN = 256\n",
    "def pad_seq(s):\n",
    "    return s + [0] * (MAX_LEN - len(s)) if len(s) < MAX_LEN else s[:MAX_LEN]\n",
    "\n",
    "X_all = np.array([pad_seq(s) for s in seqs])\n",
    "y_all = np.array(y_all)\n",
    "\n",
    "# =================================\n",
    "# 11. Train / Validation / Test split\n",
    "# =================================\n",
    "# 10% test, then split remaining into 80% train / 10% val of original\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_all, y_all,\n",
    "    test_size=0.10,\n",
    "    random_state=42,\n",
    "    stratify=y_all\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.1111111111,  # 0.1111 * 0.90 ≈ 0.10 of original\n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")\n",
    "print(f\"Split: {X_train.shape[0]} train / {X_val.shape[0]} val / {X_test.shape[0]} test\")\n",
    "\n",
    "# -----------------------------------\n",
    "# 12. Dataset & DataLoader\n",
    "# -----------------------------------\n",
    "class MedicalAbbrDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X, self.y = X, y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        return torch.tensor(self.X[i]), torch.tensor(self.y[i])\n",
    "\n",
    "BATCH = 128\n",
    "train_dl = DataLoader(\n",
    "    MedicalAbbrDataset(X_train, y_train),\n",
    "    batch_size=BATCH, shuffle=True,\n",
    "    pin_memory=True, num_workers=8, persistent_workers=True\n",
    ")\n",
    "val_dl = DataLoader(\n",
    "    MedicalAbbrDataset(X_val, y_val),\n",
    "    batch_size=BATCH,\n",
    "    pin_memory=True, num_workers=8, persistent_workers=True\n",
    ")\n",
    "test_dl = DataLoader(\n",
    "    MedicalAbbrDataset(X_test, y_test),\n",
    "    batch_size=BATCH,\n",
    "    pin_memory=True, num_workers=8, persistent_workers=True\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# 13. Load GloVe & build embedding matrix\n",
    "# ============================================\n",
    "def load_glove(fp, embedding_dim=100):\n",
    "    emb_index = {}\n",
    "    with open(fp, encoding=\"utf8\") as f:\n",
    "        for ln in f:\n",
    "            parts = ln.split()\n",
    "            word = parts[0]\n",
    "            vec = np.asarray(parts[1:], dtype=\"float32\")\n",
    "            if vec.shape[0] == embedding_dim:\n",
    "                emb_index[word] = vec\n",
    "    return emb_index\n",
    "\n",
    "GLOVE = \"glove.6B.100d.txt\"\n",
    "if not os.path.exists(GLOVE):\n",
    "    raise FileNotFoundError(f\"{GLOVE} not found\")\n",
    "glove = load_glove(GLOVE, 100)\n",
    "print(f\"Loaded {len(glove)} GloVe vectors\")\n",
    "emb_mat = np.zeros((vocab_size, 100), dtype=\"float32\")\n",
    "for w, i in word2idx.items():\n",
    "    emb_mat[i] = glove.get(w, np.random.normal(0, 0.6, 100))\n",
    "\n",
    "# ============================================\n",
    "# 14. Define LSTM model\n",
    "# ============================================\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, n_layers, out_dim,\n",
    "                 dropout=0.3, pretrained=None, freeze_emb=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        if pretrained is not None:\n",
    "            self.embedding.weight.data.copy_(torch.tensor(pretrained))\n",
    "            self.embedding.weight.requires_grad = not freeze_emb\n",
    "        self.lstm = nn.LSTM(\n",
    "            emb_dim, hid_dim, num_layers=n_layers,\n",
    "            batch_first=True, bidirectional=True, dropout=dropout\n",
    "        )\n",
    "        self.fc = nn.Linear(hid_dim * 2, out_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        _, (h_n, _) = self.lstm(emb)\n",
    "        fwd = h_n[-2]\n",
    "        bwd = h_n[-1]\n",
    "        hid = torch.cat((fwd, bwd), dim=1)\n",
    "        return self.fc(self.drop(hid))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMClassifier(vocab_size, 100, 128, 2, num_classes, 0.3,\n",
    "                       pretrained=emb_mat).to(device)\n",
    "print(model)\n",
    "\n",
    "# ============================================\n",
    "# 15. Loss, optimizer, AMP & metrics fn\n",
    "# ============================================\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "def compute_metrics(loader):\n",
    "    model.eval()\n",
    "    total_loss, correct, preds, labels = 0, 0, [], []\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in loader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            out = model(Xb)\n",
    "            loss = criterion(out, yb)\n",
    "            total_loss += loss.item() * Xb.size(0)\n",
    "            pr = out.argmax(1)\n",
    "            correct += (pr == yb).sum().item()\n",
    "            preds.extend(pr.cpu().tolist())\n",
    "            labels.extend(yb.cpu().tolist())\n",
    "    n = len(loader.dataset)\n",
    "    return total_loss / n, correct / n, preds, labels\n",
    "\n",
    "# ============================================\n",
    "# 16. Training loop\n",
    "# ============================================\n",
    "history = {\"train_loss\": [], \"train_acc\": []}\n",
    "for ep in range(1, 11):\n",
    "    model.train()\n",
    "    for Xb, yb in train_dl:\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = model(Xb)\n",
    "            loss = criterion(out, yb)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    tr_loss, tr_acc, _, _ = compute_metrics(train_dl)\n",
    "    val_loss, val_acc, _, _ = compute_metrics(val_dl)\n",
    "    history[\"train_loss\"].append(tr_loss)\n",
    "    history[\"train_acc\"].append(tr_acc)\n",
    "    print(f\"Epoch {ep}/10 | Train Loss={tr_loss:.4f}, Acc={tr_acc:.4f} \"\n",
    "          f\"| Val Loss={val_loss:.4f}, Acc={val_acc:.4f}\")\n",
    "\n",
    "# ============================================\n",
    "# 17. Evaluate on test set\n",
    "# ============================================\n",
    "test_loss, test_acc, test_preds, test_labels = compute_metrics(test_dl)\n",
    "print(f\"Test Loss={test_loss:.4f}, Test Acc={test_acc:.4f}\")\n",
    "\n",
    "# ============================================\n",
    "# 18. Confusion matrix (test)\n",
    "# ============================================\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, cmap=plt.cm.Blues)\n",
    "plt.title(\"Test Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# 19. Classification report (test)\n",
    "# ============================================\n",
    "print(\"Classification Report (test):\")\n",
    "print(classification_report(test_labels, test_preds, target_names=le.classes_, zero_division=0))\n",
    "\n",
    "# ============================================\n",
    "# 20. Inference helper\n",
    "# ============================================\n",
    "def predict_abbreviation(text, loc):\n",
    "    ctx = extract_context(text, loc)\n",
    "    toks = [t.text for t in nlp(ctx) if not t.is_punct and not t.is_space]\n",
    "    seq = [word2idx.get(t, 1) for t in toks]\n",
    "    seq = pad_seq(seq)\n",
    "    inp = torch.tensor(seq).unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(inp)\n",
    "    return le.inverse_transform([out.argmax(1).item()])[0]\n",
    "\n",
    "# Example\n",
    "sample = \"The patient was diagnosed with acute MI and was admitted to the ICU for monitoring.\"\n",
    "print(\"Predicted:\", predict_abbreviation(sample, loc=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45317728-893a-4b54-b0f9-84200d436a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to trained_model_LSTM.pth\n"
     ]
    }
   ],
   "source": [
    "# After training is complete, save the model state dictionary to a file.\n",
    "model_save_path = \"trained_model_LSTM.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00d5d7cc-c703-4824-a84c-9de243639929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Top 30 Classes:\n",
      "\n",
      "                                         precision    recall  f1-score   support\n",
      "\n",
      "                simple sequence repeats       1.00      1.00      1.00        54\n",
      "                              c peptide       0.64      0.43      0.51        54\n",
      "                         verylowdensity       1.00      0.93      0.96        54\n",
      "                   maximal electroshock       0.85      0.94      0.89        54\n",
      "                               approach       0.69      0.87      0.77        54\n",
      "                   toxic shock syndrome       1.00      0.93      0.96        54\n",
      "                               headache       0.79      0.50      0.61        54\n",
      "      diagnostic and statistical manual       1.00      1.00      1.00        54\n",
      "                      actinic keratosis       0.79      0.89      0.83        54\n",
      "                           sleep apnoea       0.41      0.24      0.30        54\n",
      "transurethral resection of the prostate       0.59      0.35      0.44        54\n",
      "                    takayasus arteritis       0.49      0.61      0.55        54\n",
      "                    ipratropium bromide       0.98      0.94      0.96        54\n",
      "                            stab wounds       1.00      0.98      0.99        54\n",
      "             superior cervical ganglion       0.73      0.20      0.32        54\n",
      "                 chemical oxygen demand       0.93      1.00      0.96        54\n",
      "                          sulphonylurea       0.41      0.17      0.24        54\n",
      "         lactate dehydrogenase activity       0.51      0.33      0.40        54\n",
      "                  adenoassociated viral       1.00      1.00      1.00        54\n",
      "                transvaginal sonography       0.93      0.98      0.95        54\n",
      "                       control patients       0.28      0.20      0.23        54\n",
      "                           notforprofit       0.98      0.89      0.93        54\n",
      "                      repetitive firing       0.88      0.98      0.93        54\n",
      "                         oral tolerance       0.87      0.96      0.91        54\n",
      "                health interview survey       0.96      0.98      0.97        54\n",
      "         spontaneous locomotor activity       0.95      1.00      0.97        54\n",
      "              schizosaccharomyces pombe       0.69      0.81      0.75        54\n",
      "           transient receptor potential       0.84      0.96      0.90        54\n",
      "                        figure of merit       0.95      1.00      0.97        54\n",
      "        extracellular matrix components       0.00      0.00      0.00        54\n",
      "\n",
      "                              micro avg       0.82      0.74      0.78      1620\n",
      "                              macro avg       0.77      0.74      0.74      1620\n",
      "                           weighted avg       0.77      0.74      0.74      1620\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Count how many true samples per class\n",
    "counts = Counter(vlabels)\n",
    "# Choose top N most frequent class indices\n",
    "top_n = 30\n",
    "top_classes = [cls for cls, _ in counts.most_common(top_n)]\n",
    "top_names   = [le.classes_[cls] for cls in top_classes]\n",
    "\n",
    "# Generate a pruned classification report\n",
    "report = classification_report(\n",
    "    vlabels,\n",
    "    vpreds,\n",
    "    labels=top_classes,\n",
    "    target_names=top_names,\n",
    "    zero_division=0\n",
    ")\n",
    "print(f\"Classification Report for Top {top_n} Classes:\\n\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fed081a-c2d4-446d-bdc9-b1b74cf5d1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Top 10 Classes:\n",
      "\n",
      "                                   precision    recall  f1-score   support\n",
      "\n",
      "          simple sequence repeats       1.00      1.00      1.00        54\n",
      "                        c peptide       0.64      0.43      0.51        54\n",
      "                   verylowdensity       1.00      0.93      0.96        54\n",
      "             maximal electroshock       0.85      0.94      0.89        54\n",
      "                         approach       0.69      0.87      0.77        54\n",
      "             toxic shock syndrome       1.00      0.93      0.96        54\n",
      "                         headache       0.79      0.50      0.61        54\n",
      "diagnostic and statistical manual       1.00      1.00      1.00        54\n",
      "                actinic keratosis       0.79      0.89      0.83        54\n",
      "                     sleep apnoea       0.41      0.24      0.30        54\n",
      "\n",
      "                        micro avg       0.84      0.77      0.80       540\n",
      "                        macro avg       0.82      0.77      0.79       540\n",
      "                     weighted avg       0.82      0.77      0.79       540\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Count how many true samples per class\n",
    "counts = Counter(vlabels)\n",
    "# Choose top N most frequent class indices\n",
    "top_n = 10\n",
    "top_classes = [cls for cls, _ in counts.most_common(top_n)]\n",
    "top_names   = [le.classes_[cls] for cls in top_classes]\n",
    "\n",
    "# Generate a pruned classification report\n",
    "report = classification_report(\n",
    "    vlabels,\n",
    "    vpreds,\n",
    "    labels=top_classes,\n",
    "    target_names=top_names,\n",
    "    zero_division=0\n",
    ")\n",
    "print(f\"Classification Report for Top {top_n} Classes:\\n\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "717e688d-d586-4887-a9ec-65be5e71a29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full classification report saved to classification_report.txt\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "full_report = classification_report(\n",
    "    vlabels, vpreds, target_names=le.classes_, zero_division=0\n",
    ")\n",
    "\n",
    "with open(\"classification_report_LSTM.txt\", \"w\") as f:\n",
    "    f.write(full_report)\n",
    "\n",
    "print(\"Full classification report saved to classification_report.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f52e076-44ee-45ad-9dc2-5c53ef4737dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For the text: 'The patient was diagnosed with acute MI and was admitted to the ICU for further monitoring.' with abbreviation at position 6, predicted expansion is: meconium ileus\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Step 11: Inference Function\n",
    "# ============================================\n",
    "def predict_abbreviation(model, text, location):\n",
    "    \"\"\"\n",
    "    model: your trained model\n",
    "    text: full sentence string\n",
    "    location: integer token position of the abbreviation\n",
    "    \"\"\"\n",
    "    ctx = extract_context(text, location, window_size=50)\n",
    "    tokens = [t.text for t in nlp(ctx) if not t.is_punct and not t.is_space]\n",
    "    seq    = [ word2idx.get(t,1) for t in tokens ]\n",
    "    seq    = pad_seq(seq)\n",
    "    inp    = torch.tensor(seq, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(inp)\n",
    "    cls = out.argmax(1).item()\n",
    "    return le.inverse_transform([cls])[0]\n",
    "\n",
    "# ============================================\n",
    "# Step 12: Example Inference\n",
    "# ============================================\n",
    "sample_text = \"The patient was diagnosed with acute MI and was admitted to the ICU for further monitoring.\"\n",
    "sample_location = 6\n",
    "pred = predict_abbreviation(model, sample_text, location=6)\n",
    "print(f\"\\nFor the text: '{sample_text}' with abbreviation at position {sample_location}, predicted expansion is: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8fe5c25-1767-4424-9d4f-9edeba3af6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "....../home/khare.aks/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example inference: predicted expansion = CLS1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 9 tests in 0.035s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import torch\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import warnings\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"dropout option adds dropout after all but last recurrent layer\"\n",
    ")\n",
    "\n",
    "import __main__ as main\n",
    "\n",
    "extract_context      = main.extract_context\n",
    "tokens_to_seq        = main.tokens_to_seq\n",
    "pad_seq              = main.pad_seq\n",
    "load_glove           = main.load_glove\n",
    "MedicalAbbrDataset   = main.MedicalAbbrDataset\n",
    "LSTMClassifier       = main.LSTMClassifier\n",
    "compute_metrics      = main.compute_metrics\n",
    "predict_abbreviation = main.predict_abbreviation\n",
    "model                = main.model\n",
    "device               = main.device\n",
    "num_classes          = main.num_classes\n",
    "MAX_LEN              = main.MAX_LEN\n",
    "le                   = main.le\n",
    "\n",
    "class TestMedicalAbbrPipeline(unittest.TestCase):\n",
    "    def test_extract_context_default_window(self):\n",
    "        text = \"one two three four five\"\n",
    "        loc = 2\n",
    "        ctx = extract_context(text, loc)\n",
    "        self.assertEqual(ctx.split(), text.split())\n",
    "\n",
    "    def test_extract_context_small_window(self):\n",
    "        text = \"a b c d e f g h i j\"\n",
    "        loc = 5\n",
    "        ctx = extract_context(text, loc, window_size=4)\n",
    "        self.assertEqual(ctx.split(), [\"d\",\"e\",\"f\",\"g\"])\n",
    "\n",
    "    def test_tokens_to_seq_and_pad_seq(self):\n",
    "        local_w2i = {'<PAD>':0,'<UNK>':1,'hello':2,'world':3}\n",
    "        seq = [local_w2i.get(t,1) for t in ['hello','unknown','world']]\n",
    "        self.assertEqual(seq, [2,1,3])\n",
    "        padded = seq + [0]*(5-len(seq))\n",
    "        self.assertEqual(padded, [2,1,3,0,0])\n",
    "        truncated = seq[:2]\n",
    "        self.assertEqual(truncated, [2,1])\n",
    "\n",
    "    def test_load_glove(self):\n",
    "        content = \"w1 0.1 0.2 0.3\\nw2 0.4 0.5 0.6\\n\"\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            fn = f\"{tmpdir}/glove.txt\"\n",
    "            with open(fn, \"w\") as f: f.write(content)\n",
    "            emb = load_glove(fn, embedding_dim=3)\n",
    "        self.assertAlmostEqual(float(emb['w1'][1]), 0.2, places=6)\n",
    "        self.assertAlmostEqual(float(emb['w2'][2]), 0.6, places=6)\n",
    "\n",
    "    def test_dataset_and_dataloader(self):\n",
    "        X = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "        y = np.array([0,1,0])\n",
    "        ds = MedicalAbbrDataset(X, y)\n",
    "        self.assertEqual(len(ds), 3)\n",
    "        x0, y0 = ds[0]\n",
    "        self.assertIsInstance(x0, torch.Tensor)\n",
    "        self.assertIsInstance(y0, torch.Tensor)\n",
    "        self.assertEqual(y0.item(), 0)\n",
    "        dl = DataLoader(ds, batch_size=2)\n",
    "        xb, yb = next(iter(dl))\n",
    "        self.assertEqual(xb.shape, (2,3))\n",
    "        self.assertEqual(yb.shape, (2,))\n",
    "\n",
    "    def test_lstmclassifier_output_shape(self):\n",
    "        m = LSTMClassifier(vocab_size=10, emb_dim=4, hid_dim=5, n_layers=1, out_dim=3)\n",
    "        data = torch.randint(0, 10, (2, 7))\n",
    "        out = m(data)\n",
    "        self.assertEqual(out.shape, (2,3))\n",
    "\n",
    "    def test_compute_metrics_perfect_accuracy(self):\n",
    "        class DummyDS(Dataset):\n",
    "            def __len__(self): return 5\n",
    "            def __getitem__(self, i):\n",
    "                return torch.zeros((MAX_LEN,), dtype=torch.long), torch.tensor(0)\n",
    "        dl = DataLoader(DummyDS(), batch_size=2)\n",
    "\n",
    "        orig_model = main.model\n",
    "        class DummyModel(nn.Module):\n",
    "            def forward(self, x):\n",
    "                bs = x.size(0)\n",
    "                return torch.zeros((bs, num_classes), device=x.device)\n",
    "        main.model = DummyModel().to(device)\n",
    "\n",
    "        loss, acc, preds, labels = compute_metrics(dl)\n",
    "        self.assertEqual(acc, 1.0)\n",
    "        self.assertEqual(len(preds), len(dl.dataset))\n",
    "        self.assertEqual(len(labels), len(dl.dataset))\n",
    "\n",
    "        main.model = orig_model\n",
    "\n",
    "    def test_predict_abbreviation_with_dummy_model(self):\n",
    "        orig_model = main.model\n",
    "        orig_le    = main.le\n",
    "\n",
    "        class DummyModel(nn.Module):\n",
    "            def forward(self, x):\n",
    "                return torch.tensor([[0.0, 1.0]], device=x.device)\n",
    "        main.model = DummyModel().to(device)\n",
    "\n",
    "        le2 = LabelEncoder()\n",
    "        le2.classes_ = np.array(['CLS0', 'CLS1'])\n",
    "        main.le = le2\n",
    "\n",
    "        pred = predict_abbreviation(main.model, \"anything here\", location=0)\n",
    "        self.assertEqual(pred, 'CLS1')\n",
    "\n",
    "        main.model = orig_model\n",
    "        main.le    = orig_le\n",
    "\n",
    "    def test_inference_example_runs(self):\n",
    "        sample_text     = \"The patient was diagnosed with acute MI and was admitted to the ICU for further monitoring.\"\n",
    "        sample_location = 6\n",
    "        pred = predict_abbreviation(model, sample_text, location=sample_location)\n",
    "        self.assertIn(pred, list(le.classes_))\n",
    "        print(f\"\\nExample inference: predicted expansion = {pred}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be095be9-a5f9-4bd2-a691-a047b6244829",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cs5330_project)",
   "language": "python",
   "name": "cs5330_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
