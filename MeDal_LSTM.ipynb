{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc637e97-52f3-4f0b-ac3d-d16a9ffead9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "After filtering, 3000000 samples remain with 22555 unique labels.\n",
      "Number of classes after filtering: 22555\n",
      "Tokenization complete.\n",
      "Vocabulary size: 489792\n",
      "Training samples: 2400000, Validation samples: 600000\n",
      "Loaded 400000 word vectors from GloVe.\n",
      "LSTMClassifier(\n",
      "  (embedding): Embedding(489792, 100, padding_idx=0)\n",
      "  (lstm): LSTM(100, 128, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (fc): Linear(in_features=256, out_features=22555, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "Epoch 1/10: Train Loss=4.3929, Train Acc=33.28% | Val Loss=1.2773, Val Acc=63.42%\n",
      "Epoch 2/10: Train Loss=1.2849, Train Acc=65.65% | Val Loss=0.9053, Val Acc=72.72%\n",
      "Epoch 3/10: Train Loss=0.9978, Train Acc=72.60% | Val Loss=0.7701, Val Acc=76.47%\n",
      "Epoch 4/10: Train Loss=0.8444, Train Acc=76.42% | Val Loss=0.7057, Val Acc=78.48%\n",
      "Epoch 5/10: Train Loss=0.7352, Train Acc=79.12% | Val Loss=0.6677, Val Acc=79.66%\n",
      "Epoch 6/10: Train Loss=0.6583, Train Acc=81.15% | Val Loss=0.6464, Val Acc=80.66%\n",
      "Epoch 7/10: Train Loss=0.5962, Train Acc=82.79% | Val Loss=0.6449, Val Acc=80.93%\n",
      "Epoch 8/10: Train Loss=0.5439, Train Acc=84.14% | Val Loss=0.6207, Val Acc=81.55%\n",
      "Epoch 9/10: Train Loss=0.5011, Train Acc=85.26% | Val Loss=0.6372, Val Acc=81.57%\n",
      "Epoch 10/10: Train Loss=0.4580, Train Acc=86.39% | Val Loss=0.6594, Val Acc=81.47%\n",
      "\n",
      "For the text: 'The patient was diagnosed with acute MI and was admitted to the ICU for further monitoring.' with abbreviation at position 6, predicted expansion is: myocardial infarct\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "import spacy\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# -------------------------------\n",
    "# Set random seeds for reproducibility\n",
    "# -------------------------------\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# -------------------------------\n",
    "# Load spaCy English model\n",
    "# -------------------------------\n",
    "# Disable parser and NER to speed up tokenization.\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# -------------------------------\n",
    "# Unzip the dataset files if needed\n",
    "# -------------------------------\n",
    "if not os.path.exists(\"train.csv\"):\n",
    "    with zipfile.ZipFile('train.csv.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('.')\n",
    "if not os.path.exists(\"train.csv\"):\n",
    "    with zipfile.ZipFile('archive (3).zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('.')\n",
    "\n",
    "# =======================\n",
    "# Step 1: Load the Dataset\n",
    "# =======================\n",
    "try:\n",
    "    df = pd.read_csv('train.csv')\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'train.csv' not found. Please ensure the dataset is in the working directory.\")\n",
    "    exit()\n",
    "\n",
    "# -------------------------------\n",
    "# Reduce the dataset size for development\n",
    "# -------------------------------\n",
    "max_samples = 3000000  # 3M samples maximum\n",
    "if len(df) > max_samples:\n",
    "    df = df.sample(n=max_samples, random_state=42).reset_index(drop=True)\n",
    "    print(f\"Dataset reduced to {len(df)} samples.\")\n",
    "\n",
    "# ================================\n",
    "# Step 2: Preprocess and Clean Data\n",
    "# ================================\n",
    "df = df.dropna(subset=['TEXT', 'LABEL', 'LOCATION'])\n",
    "df['LOCATION'] = df['LOCATION'].astype(int)\n",
    "\n",
    "# --- Reduce Label Space by Filtering Rare Labels ---\n",
    "min_label_freq = 5\n",
    "label_counts = df['LABEL'].value_counts()\n",
    "valid_labels = label_counts[label_counts >= min_label_freq].index\n",
    "df = df[df['LABEL'].isin(valid_labels)].reset_index(drop=True)\n",
    "print(f\"After filtering, {len(df)} samples remain with {df['LABEL'].nunique()} unique labels.\")\n",
    "\n",
    "# --- Advanced Preprocessing Functions ---\n",
    "def extract_context(text, location, window_size=50):\n",
    "    \"\"\"\n",
    "    Extract a window of tokens around the abbreviation.\n",
    "    Assumes LOCATION is based on simple whitespace tokenization.\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    start = max(0, location - window_size // 2)\n",
    "    end = min(len(tokens), location + window_size // 2)\n",
    "    return \" \".join(tokens[start:end])\n",
    "\n",
    "# Use list comprehension instead of apply for faster context extraction.\n",
    "contexts = [extract_context(text, loc, window_size=50) for text, loc in zip(df['TEXT'], df['LOCATION'])]\n",
    "df['CONTEXT'] = contexts\n",
    "\n",
    "texts = df['CONTEXT'].tolist()\n",
    "labels = df['LABEL'].tolist()\n",
    "\n",
    "# ===============================\n",
    "# Step 3: Encode Labels\n",
    "# ===============================\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Number of classes after filtering: {num_classes}\")\n",
    "\n",
    "# ============================================\n",
    "# Step 4: Tokenization and Vocabulary Construction\n",
    "# ============================================\n",
    "def batch_advanced_tokenize(texts, batch_size=1000):\n",
    "    tokenized_texts = []\n",
    "    for doc in nlp.pipe(texts, batch_size=batch_size):\n",
    "        tokens = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "        tokenized_texts.append(tokens)\n",
    "    return tokenized_texts\n",
    "\n",
    "tokenized_texts = batch_advanced_tokenize(texts, batch_size=1000)\n",
    "print(\"Tokenization complete.\")\n",
    "\n",
    "# Build vocabulary from tokenized texts.\n",
    "all_tokens = [token for tokens in tokenized_texts for token in tokens]\n",
    "vocab_counter = Counter(all_tokens)\n",
    "min_word_freq = 2\n",
    "vocab = {token for token, count in vocab_counter.items() if count >= min_word_freq}\n",
    "\n",
    "# Reserve indices: 0 for padding, 1 for unknown tokens.\n",
    "word_to_index = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for word in sorted(vocab):\n",
    "    word_to_index[word] = len(word_to_index)\n",
    "vocab_size = len(word_to_index)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Convert texts to sequences of indices.\n",
    "def text_to_sequence(tokens):\n",
    "    return [word_to_index.get(token, word_to_index[\"<UNK>\"]) for token in tokens]\n",
    "\n",
    "sequences = [text_to_sequence(tokens) for tokens in tokenized_texts]\n",
    "\n",
    "# ===========================\n",
    "# Step 5: Pad Sequences\n",
    "# ===========================\n",
    "max_len = 256  # Fixed maximum length.\n",
    "def pad_sequence_fn(seq, max_len):\n",
    "    return seq + [0] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len]\n",
    "\n",
    "padded_sequences = [pad_sequence_fn(seq, max_len) for seq in sequences]\n",
    "X = np.array(padded_sequences)\n",
    "y = np.array(labels_encoded)\n",
    "\n",
    "# ==================================\n",
    "# Step 6: Train/Validation Data Split\n",
    "# ==================================\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")\n",
    "\n",
    "# ============================================\n",
    "# Step 7: Create PyTorch Dataset and DataLoader\n",
    "# ============================================\n",
    "class MedicalAbbrDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.long), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = MedicalAbbrDataset(X_train, y_train)\n",
    "val_dataset = MedicalAbbrDataset(X_val, y_val)\n",
    "# Use multiple workers and pin_memory for faster data loading on GPU.\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4, pin_memory=True)\n",
    "\n",
    "# ============================================\n",
    "# Step 8: Load Pre-trained GloVe Embeddings and Build Embedding Matrix\n",
    "# ============================================\n",
    "def load_glove_embeddings(filepath, embedding_dim):\n",
    "    embeddings_index = {}\n",
    "    with open(filepath, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            if vector.shape[0] == embedding_dim:\n",
    "                embeddings_index[word] = vector\n",
    "    return embeddings_index\n",
    "\n",
    "embedding_dim = 100\n",
    "glove_path = \"glove.6B.100d.txt\"\n",
    "if not os.path.exists(glove_path):\n",
    "    raise FileNotFoundError(f\"{glove_path} not found. Please download it and place it in the working directory.\")\n",
    "\n",
    "glove_embeddings = load_glove_embeddings(glove_path, embedding_dim)\n",
    "print(f\"Loaded {len(glove_embeddings)} word vectors from GloVe.\")\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim), dtype=np.float32)\n",
    "for word, idx in word_to_index.items():\n",
    "    if word in glove_embeddings:\n",
    "        embedding_matrix[idx] = glove_embeddings[word]\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "\n",
    "# ============================================\n",
    "# Step 9: Define the LSTM-only Model (No Attention)\n",
    "# ============================================\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, output_dim, dropout=0.3,\n",
    "                 pretrained_embeddings=None, freeze_embeddings=False):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(torch.tensor(pretrained_embeddings))\n",
    "            self.embedding.weight.requires_grad = not freeze_embeddings\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, \n",
    "                            batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (h_n, _) = self.lstm(embedded)\n",
    "        forward_h = h_n[-2, :, :]\n",
    "        backward_h = h_n[-1, :, :]\n",
    "        hidden = torch.cat((forward_h, backward_h), dim=1)\n",
    "        hidden = self.dropout(hidden)\n",
    "        logits = self.fc(hidden)\n",
    "        return logits\n",
    "\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "output_dim = num_classes\n",
    "dropout = 0.3\n",
    "\n",
    "model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, num_layers, output_dim, dropout,\n",
    "                       pretrained_embeddings=embedding_matrix, freeze_embeddings=False)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "# ============================================\n",
    "# Step 10: Define Loss, Optimizer, and Training Loop\n",
    "# ============================================\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss, epoch_correct = 0, 0\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * inputs.size(0)\n",
    "        epoch_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "    return epoch_loss / len(loader.dataset), epoch_correct / len(loader.dataset)\n",
    "\n",
    "def evaluate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss, epoch_correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_loss += loss.item() * inputs.size(0)\n",
    "            epoch_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "    return epoch_loss / len(loader.dataset), epoch_correct / len(loader.dataset)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = evaluate_epoch(model, val_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss={train_loss:.4f}, Train Acc={train_acc*100:.2f}% | Val Loss={val_loss:.4f}, Val Acc={val_acc*100:.2f}%\")\n",
    "\n",
    "# ============================================\n",
    "# Step 11: Inference Function\n",
    "# ============================================\n",
    "def predict_abbreviation(model, text, location, word_to_index, max_len, device, label_encoder):\n",
    "    context = extract_context(text, location, window_size=50)\n",
    "    tokens = [token.text for token in nlp(context) if not token.is_punct and not token.is_space]\n",
    "    seq = [word_to_index.get(token, word_to_index[\"<UNK>\"]) for token in tokens]\n",
    "    seq = pad_sequence_fn(seq, max_len)\n",
    "    input_tensor = torch.tensor(seq, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)\n",
    "    pred_class = logits.argmax(dim=1).item()\n",
    "    pred_label = label_encoder.inverse_transform([pred_class])[0]\n",
    "    return pred_label\n",
    "\n",
    "# ============================================\n",
    "# Step 12: Example Inference\n",
    "# ============================================\n",
    "sample_text = \"The patient was diagnosed with acute MI and was admitted to the ICU for further monitoring.\"\n",
    "sample_location = 6\n",
    "predicted_expansion = predict_abbreviation(model, sample_text, sample_location, word_to_index, max_len, device, label_encoder)\n",
    "print(f\"\\nFor the text: '{sample_text}' with abbreviation at position {sample_location}, predicted expansion is: {predicted_expansion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aa0b46d-9a7a-4939-8964-568d3069a185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to trained_model_LSTM.pth\n"
     ]
    }
   ],
   "source": [
    "# After training is complete, save the model state dictionary to a file.\n",
    "model_save_path = \"trained_model_LSTM.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "845d624b-d9e5-4695-bb0e-084b7d3ff671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For the text: 'The patient was diagnosed with acute MI and was admitted to the ICU for further monitoring.' with abbreviation at position 6, predicted expansion is: myocardial infarct\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Step 12: Example Inference\n",
    "# ============================================\n",
    "sample_text = \"The patient was diagnosed with acute MI and was admitted to the ICU for further monitoring.\"\n",
    "sample_location = 6\n",
    "predicted_expansion = predict_abbreviation(model, sample_text, sample_location, word_to_index, max_len, device, label_encoder)\n",
    "print(f\"\\nFor the text: '{sample_text}' with abbreviation at position {sample_location}, predicted expansion is: {predicted_expansion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d66d05a3-9a47-426c-954c-5622eaec40cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1:\n",
      "Text: 'The patient underwent CABG and was later transferred to the CCU for recovery.'\n",
      "Abbreviation token index: 3\n",
      "Predicted Expansion: coronary artery bypass grafting\n",
      "\n",
      "Test 2:\n",
      "Text: 'Signs of COPD were evident in the chest X-ray, prompting further investigation.'\n",
      "Abbreviation token index: 2\n",
      "Predicted Expansion: relative risk\n",
      "\n",
      "Test 3:\n",
      "Text: 'After the MRI, the scan showed no significant abnormalities.'\n",
      "Abbreviation token index: 1\n",
      "Predicted Expansion: magnetic resonance imaging scan\n",
      "\n",
      "Test 4:\n",
      "Text: 'The treatment plan included a dose of IV antibiotics to control the infection.'\n",
      "Abbreviation token index: 4\n",
      "Predicted Expansion: expanded programme on immunization\n",
      "\n",
      "Test 5:\n",
      "Text: 'He was admitted to the ER after experiencing severe chest pain.'\n",
      "Abbreviation token index: 5\n",
      "Predicted Expansion: emergency room\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Step 12: Extended Inference Tests\n",
    "# ============================================\n",
    "\n",
    "# Define a list of test cases with sample texts and the token index of the abbreviation\n",
    "test_cases = [\n",
    "    {\n",
    "        \"text\": \"The patient underwent CABG and was later transferred to the CCU for recovery.\",\n",
    "        \"location\": 3  # Adjust this value to indicate the abbreviation's token index\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Signs of COPD were evident in the chest X-ray, prompting further investigation.\",\n",
    "        \"location\": 2  # Adjust this as needed\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"After the MRI, the scan showed no significant abnormalities.\",\n",
    "        \"location\": 1  # Adjust the token index accordingly\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"The treatment plan included a dose of IV antibiotics to control the infection.\",\n",
    "        \"location\": 4  # Update if the abbreviation is elsewhere in the text\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"He was admitted to the ER after experiencing severe chest pain.\",\n",
    "        \"location\": 5  # Adjust this value as necessary\n",
    "    }\n",
    "]\n",
    "\n",
    "# Loop through the test cases and print out the predicted expansions\n",
    "for idx, test in enumerate(test_cases, start=1):\n",
    "    pred_expansion = predict_abbreviation(model, test[\"text\"], test[\"location\"],\n",
    "                                          word_to_index, max_len, device, label_encoder)\n",
    "    print(f\"Test {idx}:\")\n",
    "    print(f\"Text: '{test['text']}'\")\n",
    "    print(f\"Abbreviation token index: {test['location']}\")\n",
    "    print(f\"Predicted Expansion: {pred_expansion}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f285365d-8e8f-4c7a-b72d-c2f9c6a49b28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cs5330_project)",
   "language": "python",
   "name": "cs5330_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
