{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fd98cf-a6f0-45e9-ab31-14a78432169a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "Dataset reduced to 900000 samples.\n",
      "After filtering, 892980 samples remain with 19920 unique labels.\n",
      "Number of classes after filtering: 19920\n",
      "Tokenization complete.\n",
      "Vocabulary size: 232241\n",
      "Training samples: 714384, Validation samples: 178596\n",
      "Loaded 400000 word vectors from GloVe.\n",
      "LSTMClassifier(\n",
      "  (embedding): Embedding(232241, 100, padding_idx=0)\n",
      "  (lstm): LSTM(100, 128, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (fc): Linear(in_features=256, out_features=19920, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "Epoch 1/5: Train Loss=8.7647, Train Acc=0.64% | Val Loss=6.2100, Val Acc=10.02%\n",
      "Epoch 2/5: Train Loss=3.5599, Train Acc=35.48% | Val Loss=1.7365, Val Acc=54.71%\n",
      "Epoch 3/5: Train Loss=1.7297, Train Acc=56.28% | Val Loss=1.2766, Val Acc=62.60%\n",
      "Epoch 4/5: Train Loss=1.3238, Train Acc=63.62% | Val Loss=1.0877, Val Acc=66.96%\n",
      "Epoch 5/5: Train Loss=1.1046, Train Acc=68.64% | Val Loss=1.0313, Val Acc=69.30%\n",
      "\n",
      "For the text: 'The patient was diagnosed with acute MI and was admitted to the ICU for further monitoring.' with abbreviation at position 6, predicted expansion is: meconium ileus\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "import spacy\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# -------------------------------\n",
    "# Set random seeds for reproducibility\n",
    "# -------------------------------\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# -------------------------------\n",
    "# Load spaCy English model\n",
    "# -------------------------------\n",
    "# Disable parser and NER to speed up tokenization.\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# -------------------------------\n",
    "# Unzip the dataset files if needed\n",
    "# -------------------------------\n",
    "if not os.path.exists(\"train.csv\"):\n",
    "    with zipfile.ZipFile('train.csv.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('.')\n",
    "if not os.path.exists(\"train.csv\"):\n",
    "    with zipfile.ZipFile('archive (3).zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('.')\n",
    "\n",
    "# =======================\n",
    "# Step 1: Load the Dataset\n",
    "# =======================\n",
    "try:\n",
    "    df = pd.read_csv('train.csv')\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'train.csv' not found. Please ensure the dataset is in the working directory.\")\n",
    "    exit()\n",
    "\n",
    "# -------------------------------\n",
    "# Reduce the dataset size for development\n",
    "# -------------------------------\n",
    "max_samples = 900000  # reduce to 300K samples\n",
    "if len(df) > max_samples:\n",
    "    df = df.sample(n=max_samples, random_state=42).reset_index(drop=True)\n",
    "    print(f\"Dataset reduced to {len(df)} samples.\")\n",
    "\n",
    "# ================================\n",
    "# Step 2: Preprocess and Clean Data\n",
    "# ================================\n",
    "# Use 'TEXT', 'LABEL', and 'LOCATION' columns.\n",
    "df = df.dropna(subset=['TEXT', 'LABEL', 'LOCATION'])\n",
    "df['LOCATION'] = df['LOCATION'].astype(int)\n",
    "\n",
    "# --- Reduce Label Space by Filtering Rare Labels ---\n",
    "min_label_freq = 5  # drop labels occurring fewer than 5 times\n",
    "label_counts = df['LABEL'].value_counts()\n",
    "valid_labels = label_counts[label_counts >= min_label_freq].index\n",
    "df = df[df['LABEL'].isin(valid_labels)].reset_index(drop=True)\n",
    "print(f\"After filtering, {len(df)} samples remain with {df['LABEL'].nunique()} unique labels.\")\n",
    "\n",
    "# --- Advanced Preprocessing Functions ---\n",
    "def extract_context(text, location, window_size=50):\n",
    "    \"\"\"\n",
    "    Extract a window of tokens around the abbreviation.\n",
    "    Assumes LOCATION is based on simple whitespace tokenization.\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    start = max(0, location - window_size // 2)\n",
    "    end = min(len(tokens), location + window_size // 2)\n",
    "    return \" \".join(tokens[start:end])\n",
    "\n",
    "# Create a new column 'CONTEXT' using the LOCATION info.\n",
    "df['CONTEXT'] = df.apply(lambda row: extract_context(row['TEXT'], row['LOCATION'], window_size=50), axis=1)\n",
    "\n",
    "# Use the context column for training.\n",
    "texts = df['CONTEXT'].tolist()\n",
    "labels = df['LABEL'].tolist()\n",
    "\n",
    "# ===============================\n",
    "# Step 3: Encode Labels\n",
    "# ===============================\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Number of classes after filtering: {num_classes}\")\n",
    "\n",
    "# ============================================\n",
    "# Step 4: Tokenization and Vocabulary Construction (with Batch Processing)\n",
    "# ============================================\n",
    "def batch_advanced_tokenize(texts, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Use spaCy's nlp.pipe to tokenize texts in batches.\n",
    "    \"\"\"\n",
    "    tokenized_texts = []\n",
    "    for doc in nlp.pipe(texts, batch_size=batch_size):\n",
    "        tokens = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "        tokenized_texts.append(tokens)\n",
    "    return tokenized_texts\n",
    "\n",
    "# Tokenize all texts in batches.\n",
    "tokenized_texts = batch_advanced_tokenize(texts, batch_size=1000)\n",
    "print(\"Tokenization complete.\")\n",
    "\n",
    "# Build vocabulary from tokenized texts.\n",
    "all_tokens = [token for tokens in tokenized_texts for token in tokens]\n",
    "vocab_counter = Counter(all_tokens)\n",
    "min_word_freq = 2  # ignore words that appear less than 2 times\n",
    "vocab = {token for token, count in vocab_counter.items() if count >= min_word_freq}\n",
    "\n",
    "# Reserve indices: 0 for padding, 1 for unknown tokens.\n",
    "word_to_index = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for word in sorted(vocab):\n",
    "    word_to_index[word] = len(word_to_index)\n",
    "vocab_size = len(word_to_index)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Convert texts to sequences of indices.\n",
    "def text_to_sequence(tokens):\n",
    "    return [word_to_index.get(token, word_to_index[\"<UNK>\"]) for token in tokens]\n",
    "\n",
    "sequences = [text_to_sequence(tokens) for tokens in tokenized_texts]\n",
    "\n",
    "# ===========================\n",
    "# Step 5: Pad Sequences\n",
    "# ===========================\n",
    "max_len = 256  # Fixed maximum length for input sequences.\n",
    "def pad_sequence_fn(seq, max_len):\n",
    "    if len(seq) < max_len:\n",
    "        return seq + [0] * (max_len - len(seq))\n",
    "    else:\n",
    "        return seq[:max_len]\n",
    "\n",
    "padded_sequences = [pad_sequence_fn(seq, max_len) for seq in sequences]\n",
    "X = np.array(padded_sequences)\n",
    "y = np.array(labels_encoded)\n",
    "\n",
    "# ==================================\n",
    "# Step 6: Train/Validation Data Split\n",
    "# ==================================\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")\n",
    "\n",
    "# ============================================\n",
    "# Step 7: Create PyTorch Dataset and DataLoader\n",
    "# ============================================\n",
    "class MedicalAbbrDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.long), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = MedicalAbbrDataset(X_train, y_train)\n",
    "val_dataset = MedicalAbbrDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# ============================================\n",
    "# Step 8: Load Pre-trained GloVe Embeddings and Build Embedding Matrix\n",
    "# ============================================\n",
    "def load_glove_embeddings(filepath, embedding_dim):\n",
    "    embeddings_index = {}\n",
    "    with open(filepath, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            if vector.shape[0] == embedding_dim:\n",
    "                embeddings_index[word] = vector\n",
    "    return embeddings_index\n",
    "\n",
    "embedding_dim = 100 \n",
    "glove_path = \"glove.6B.100d.txt\"  # Update the path if needed\n",
    "\n",
    "if not os.path.exists(glove_path):\n",
    "    raise FileNotFoundError(f\"{glove_path} not found. Please download it and place it in the working directory.\")\n",
    "\n",
    "glove_embeddings = load_glove_embeddings(glove_path, embedding_dim)\n",
    "print(f\"Loaded {len(glove_embeddings)} word vectors from GloVe.\")\n",
    "\n",
    "# Build the embedding matrix.\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim), dtype=np.float32)\n",
    "for word, idx in word_to_index.items():\n",
    "    if word in glove_embeddings:\n",
    "        embedding_matrix[idx] = glove_embeddings[word]\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "\n",
    "# ============================================\n",
    "# Step 9: Define the LSTM-only Model Using Pre-trained GloVe Embeddings\n",
    "# ============================================\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, output_dim, dropout=0.3,\n",
    "                 pretrained_embeddings=None, freeze_embeddings=False):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(torch.tensor(pretrained_embeddings))\n",
    "            self.embedding.weight.requires_grad = not freeze_embeddings\n",
    "        # Bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, \n",
    "                            batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        # Concatenate final forward and backward hidden states\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_len, embedding_dim]\n",
    "        lstm_out, (h_n, _) = self.lstm(embedded)\n",
    "        # h_n shape: (num_layers*2, batch_size, hidden_dim)\n",
    "        forward_h = h_n[-2, :, :]\n",
    "        backward_h = h_n[-1, :, :]\n",
    "        hidden = torch.cat((forward_h, backward_h), dim=1)\n",
    "        hidden = self.dropout(hidden)\n",
    "        logits = self.fc(hidden)\n",
    "        return logits\n",
    "\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "output_dim = num_classes\n",
    "dropout = 0.3\n",
    "\n",
    "model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, num_layers, output_dim, dropout,\n",
    "                       pretrained_embeddings=embedding_matrix, freeze_embeddings=False)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "# ============================================\n",
    "# Step 10: Define Loss, Optimizer, and Training Loop\n",
    "# ============================================\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss, epoch_correct = 0, 0\n",
    "    for inputs, labels in loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * inputs.size(0)\n",
    "        epoch_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "    return epoch_loss / len(loader.dataset), epoch_correct / len(loader.dataset)\n",
    "\n",
    "def evaluate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss, epoch_correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_loss += loss.item() * inputs.size(0)\n",
    "            epoch_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "    return epoch_loss / len(loader.dataset), epoch_correct / len(loader.dataset)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = evaluate_epoch(model, val_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss={train_loss:.4f}, Train Acc={train_acc*100:.2f}% | Val Loss={val_loss:.4f}, Val Acc={val_acc*100:.2f}%\")\n",
    "\n",
    "# ============================================\n",
    "# Step 11: Inference Function\n",
    "# ============================================\n",
    "def predict_abbreviation(model, text, location, word_to_index, max_len, device, label_encoder):\n",
    "    # Extract context using the same procedure as during training.\n",
    "    context = extract_context(text, location, window_size=50)\n",
    "    # Use advanced tokenization (one text at a time is acceptable for inference)\n",
    "    tokens = [token.text for token in nlp(context) if not token.is_punct and not token.is_space]\n",
    "    seq = [word_to_index.get(token, word_to_index[\"<UNK>\"]) for token in tokens]\n",
    "    seq = pad_sequence_fn(seq, max_len)\n",
    "    input_tensor = torch.tensor(seq, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)\n",
    "    pred_class = logits.argmax(dim=1).item()\n",
    "    pred_label = label_encoder.inverse_transform([pred_class])[0]\n",
    "    return pred_label\n",
    "\n",
    "# ============================================\n",
    "# Step 12: Example Inference\n",
    "# ============================================\n",
    "sample_text = \"The patient was diagnosed with acute MI and was admitted to the ICU for further monitoring.\"\n",
    "# Assume the abbreviation appears near token index 6 (adjust as needed)\n",
    "sample_location = 6\n",
    "predicted_expansion = predict_abbreviation(model, sample_text, sample_location, word_to_index, max_len, device, label_encoder)\n",
    "print(f\"\\nFor the text: '{sample_text}' with abbreviation at position {sample_location}, predicted expansion is: {predicted_expansion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46360d14-d81b-40bc-b45b-9b840867b31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy && python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe68097-d0ac-43d2-9b6d-8a198678001c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a21f261d-fc75-4009-8a43-22b18ab0d811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: typing_extensions in ./.local/lib/python3.8/site-packages (4.12.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019e8301-5e6c-4474-9462-1135a35e5b84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cs5330_project)",
   "language": "python",
   "name": "cs5330_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
