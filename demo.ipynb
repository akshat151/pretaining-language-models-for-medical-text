{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.data.medal import MeDALSubset\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import yaml\n",
    "from src.models.trainer import ModelTrainer\n",
    "from src.vectorizer.trainable import TrainableEmbedding\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from tqdm import tqdm\n",
    "from src.utils import save_embeddings_to_file\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MeDAL dataset initialized with name: MeDAL\n",
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.10), please consider upgrading to the latest version (0.3.11).\n",
      "Dataset downloaded to: /Users/prashanthjaganathan/.cache/kagglehub/datasets/xhlulu/medal-emnlp/versions/4\n",
      "Dataset moved to: /Users/prashanthjaganathan/Desktop/CS6120 - NLP/pretaining-language-models-for-medical-text/dataset\n",
      "Total number of classes: 22555\n"
     ]
    }
   ],
   "source": [
    "medal_dataset = MeDALSubset('MeDAL')\n",
    "data, train_data, val_data, test_data = medal_dataset.load_dataset()\n",
    "class_to_idx = medal_dataset.class_to_idx\n",
    "del data, medal_dataset, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def load_config(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "config = load_config('config/config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_train = medal_dataset.preprocess(['train', 'valid'])\n",
    "# NOTE: Pre-processed for 503 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# train_df, val_df = processed_train\n",
    "\n",
    "# Save to CSV\n",
    "# NOTE: commented out v sensitive code, files contain huge corpus of preprocessed data\n",
    "# DO NOT OVERWRITE THE FILES\n",
    "# train_df.to_csv(\"dataset/medal/preprocessed_subset/train.csv\", index=False)\n",
    "# val_df.to_csv(\"dataset/medal/preprocessed_subset/valid.csv\", index=False)\n",
    "\n",
    "# print(\"CSV files saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and load pre-processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n"
     ]
    }
   ],
   "source": [
    "# preprocessed_train = pd.read_csv('dataset/medal/preprocessed_subset/train.csv')\n",
    "preprocessed_val = pd.read_csv('dataset/medal/preprocessed_subset/valid.csv')\n",
    "# medal_dataset.train_data = preprocessed_train\n",
    "medal_dataset.val_data = preprocessed_val\n",
    "print(len(medal_dataset.val_data))\n",
    "\n",
    "# del preprocessed_train\n",
    "del preprocessed_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bbb4983d1334554a92606fabf02bc26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=125000), Label(value='0 / 125000')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# val_tokens = medal_dataset.tokenize('nltk', splits=['valid'])\n",
    "# file_name = \"dataset/medal/nltk_tokenized_subset/valid.parquet\"\n",
    "# val_tokens.to_frame().to_parquet(file_name)\n",
    "# print('Parquet file saved successfully!')\n",
    "\n",
    "# type(val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Docs: 100%|██████████| 3000000/3000000 [00:19<00:00, 155347.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in train corpus: 3000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_tokens = pd.read_parquet(\n",
    "    \"dataset/medal/nltk_tokenized_subset/train.parquet\", \n",
    "    engine=\"pyarrow\"\n",
    "    ).squeeze()\n",
    "\n",
    "# to make it as a list[list[str]]\n",
    "tokenized_train_corpus = [doc.tolist() for doc in tqdm(train_tokens, 'Docs', len(train_tokens))] \n",
    "print(f'Number of documents in train corpus: {len(tokenized_train_corpus)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Docs: 100%|██████████| 1000000/1000000 [00:03<00:00, 264271.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in val corpus: 1000000\n"
     ]
    }
   ],
   "source": [
    "val_tokens = pd.read_parquet(\"dataset/medal/nltk_tokenized_subset/valid.parquet\", engine=\"pyarrow\").squeeze()\n",
    "\n",
    "# to make it as a list[list[str]]\n",
    "tokenized_val_corpus = [doc.tolist() for doc in tqdm(val_tokens, 'Docs', len(val_tokens))] \n",
    "print(f'Number of documents in val corpus: {len(tokenized_val_corpus)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Word2Vec model on the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = TrainableEmbedding(\n",
    "        tokenized_corpus=tokenized_train_corpus,\n",
    "        algorithm=\"word2vec\",\n",
    "        vector_size=100,\n",
    "        window=5,\n",
    "        min_count=2\n",
    "    )\n",
    "embeddings = embedding_model.embed(tokenized_train_corpus)\n",
    "print(f'Embedding dimensions: {len(embeddings[0][0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training FastText model on the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = TrainableEmbedding(\n",
    "        tokenized_corpus=tokenized_train_corpus,\n",
    "        algorithm=\"fasttext\",\n",
    "        vector_size=100,\n",
    "        window=7,\n",
    "        min_count=2\n",
    "    )\n",
    "embeddings = embedding_model.embed(tokenized_train_corpus)\n",
    "print(f'Embeddings Dimensions: {len(embeddings[0][0])}')\n",
    "save_embeddings_to_file(embeddings, \"embeddings/fasttext_val_embeddings.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing the embedding to Truncated Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = TrainableEmbedding(\n",
    "        tokenized_corpus=train_tokens.tolist(),\n",
    "        algorithm=\"tfidf\",\n",
    "        vector_size=100,\n",
    "        window=5,\n",
    "        min_count=2\n",
    "    )\n",
    "embeddings = embedding_model.embed(train_tokens.tolist())\n",
    "print(f'Embedding dimensions: {embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: bio wordvec model is like 12GB and unable to load it in the memeory and build embeddings\n",
    "\n",
    "train_embeddings = medal_dataset.embed(\n",
    "    'bio_wordvec',\n",
    "    splits=['train'],\n",
    "    tokenized_data = train_tokens,\n",
    "    model_path = 'trained_models/embeddings/pretrained/bio_wordvec.bin'\n",
    ")\n",
    "len(train_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying not to use bio bert as it involves trasformer models and our architecture is limited to using \n",
    "LSTM + Self Attention, therefore, looking for static embedding models only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = medal_dataset.embed(\n",
    "    'bio_bert', \n",
    "    splits=['train'],\n",
    "    model_name='dmis-lab/biobert-base-cased-v1.1'\n",
    "    )\n",
    "len(train_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_corpus = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Model Training\n",
    "\n",
    "First, let's create the dataloader with embeddings as features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class LazyEmbeddingDataset(Dataset):\n",
    "    def __init__(self, file_path, trainable_embed_model, labels, class_to_idx, max_seq_len=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tokenized_corpus (list[list[str]]): List of tokenized documents.\n",
    "            embedding_model: Pre-trained embedding model that implements .embed().\n",
    "                The embed() method should accept a list of tokenized docs and return\n",
    "                a list of sequences of word embeddings.\n",
    "            labels (list): Labels corresponding to each document.\n",
    "            class_to_idx (dict): Mapping from class label to integer index.\n",
    "            max_seq_len (int, optional): If provided, pad/truncate each document\n",
    "                so that the sequence length equals max_seq_len.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        table = pq.read_table(self.file_path)\n",
    "        self.tokenized_corpus = table['TEXT']\n",
    "\n",
    "        \n",
    "        self.trainable_embed_model = trainable_embed_model\n",
    "        self.labels = labels\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "            # Get the tokens for the current document\n",
    "            tokens = self.tokenized_corpus[idx].as_py()  # Convert PyArrow StringScalar to string\n",
    "            # Compute the embeddings for this document on the fly.\n",
    "            embedding_seq = self.trainable_embed_model.embed([tokens])[0]  # shape: (seq_len, embedding_dim)\n",
    "\n",
    "            # Convert the embeddings into a numpy array\n",
    "            embedding_seq = np.array(embedding_seq, dtype=np.float32)\n",
    "            seq_len = len(embedding_seq)\n",
    "            embedding_dim = len(embedding_seq[0])\n",
    "\n",
    "            # Create a mask for the sequence\n",
    "            mask = np.ones(seq_len, dtype=np.float32)  # 1 indicates a valid token\n",
    "            if self.max_seq_len is not None:\n",
    "                if seq_len < self.max_seq_len:\n",
    "                    # Pad with zeros\n",
    "                    pad = np.zeros((self.max_seq_len - seq_len, embedding_dim), dtype=np.float32)\n",
    "                    embedding_seq = np.vstack([embedding_seq, pad])\n",
    "                    mask = np.concatenate([mask, np.zeros(self.max_seq_len - seq_len, dtype=np.float32)])\n",
    "                else:\n",
    "                    # Truncate if too long\n",
    "                    embedding_seq = embedding_seq[:self.max_seq_len]\n",
    "                    mask = mask[:self.max_seq_len]\n",
    "\n",
    "            # Convert label to index\n",
    "            label = self.labels[idx]\n",
    "            label_idx = self.class_to_idx[label]\n",
    "\n",
    "            # Convert to PyTorch tensors (after converting to NumPy arrays for speed)\n",
    "            return (torch.tensor(embedding_seq, dtype=torch.float32),\n",
    "                    torch.tensor(mask, dtype=torch.float32),  # Return the mask\n",
    "                    torch.tensor(label_idx, dtype=torch.int64))\n",
    "\n",
    "\n",
    "\n",
    "def create_lazy_dataloader(file_path, trainable_embed_model, labels, class_to_idx, batch_size, max_seq_len=None):\n",
    "    dataset = LazyEmbeddingDataset(file_path, trainable_embed_model, labels, class_to_idx, max_seq_len)\n",
    "    indices = range(1000000)\n",
    "    # Create a subset of the dataset using the sampled indices\n",
    "    sampled_dataset = torch.utils.data.Subset(dataset, indices)\n",
    "    return DataLoader(sampled_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_embed_model = TrainableEmbedding(\n",
    "        tokenized_corpus=tokenized_train_corpus,\n",
    "        algorithm=\"word2vec\", # use fasttext to better handle OOV words\n",
    "        vector_size=100,\n",
    "        window=5,\n",
    "        min_count=0\n",
    "    )\n",
    "\n",
    "max_seq_len = config['datasets']['medal']['max_sequence_length']\n",
    "batch_size = config['training']['hyperparameters']['batch_size']\n",
    "\n",
    "trainloader = create_lazy_dataloader(\n",
    "    'dataset/medal/nltk_tokenized_subset/train.parquet', \n",
    "    trainable_embed_model, \n",
    "    train_data['LABEL'],\n",
    "    class_to_idx, \n",
    "    batch_size=batch_size,\n",
    "    max_seq_len=max_seq_len\n",
    "    )\n",
    "\n",
    "\n",
    "valloader = create_lazy_dataloader(\n",
    "    'dataset/medal/nltk_tokenized_subset/valid.parquet', \n",
    "    trainable_embed_model, \n",
    "    val_data['LABEL'],\n",
    "    class_to_idx, \n",
    "    batch_size=batch_size,\n",
    "    max_seq_len=max_seq_len\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- lstm_and_self_attention --------\n",
      "{'lstm_units': 3, 'lstm_hidden_dim': 50, 'dropout': 0.1, 'num_classes': 22555, 'embedding_dim': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 320/15625 [05:33<4:41:36,  1.10s/it]"
     ]
    }
   ],
   "source": [
    "# Use the new dataloaders\n",
    "model_trainer = ModelTrainer()\n",
    "train_results = model_trainer.train(\n",
    "    trainloader, \n",
    "    valloader, \n",
    "    dataset='medal', \n",
    "    embedding_dim=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.plot_results(train_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIMIC III Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
