embedding_models:
  bio_wordvec: 
    model_path: 'trained_models/embeddings/pretrained/bio_wordvec_200.bin'
    doc_vector_technique: 'average' # Options: [average, max, sum, normal (l2 norm)]
    embedding_dim: 200

datasets:
  medal:
    name: "medal"
    data_path: "dataset/medal/pretrain_subset"
    preprocessed_data_path: "dataset/medal/preprocessed_subset"      
    num_classes: 22555
    tokenizer: "Tokenizer_1"
    use_embeddings: true
    embedding_path: "/path/to/embedding_file"
    loss_function: "cross_entropy"
    max_sequence_length: 954 # maxseq

model_names: ['lstm_and_self_attention']

models:
  lstm_and_self_attention:
    hyperparameters: 
      lstm_units: 64
      lstm_hidden_dim: 500
      dropout: 0.5
    base_params:

    metrics:
      - "accuracy"
      - "precision"
      - "recall"
    
# Common training settings
training:
  hyperparameters:
    batch_size: 32
    learning_rate: 0.001

  optimizer: 'adam'
  weight_decay: 0.5

  epochs: 3
  shuffle: true
  random_seed: 42

